{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb81083f-b033-4586-9c91-73e9de902eb0",
   "metadata": {},
   "source": [
    "<strong> Installing packages\n",
    "<li><strong>Iangchain</li>\n",
    "<li><strong>OpenAi</li>\n",
    "<li><strong>tqdm: libray to show the progress of an action (downloading, training...)</li>\n",
    "<li><strong>iq: lightweight and flexible JSON processor</li>\n",
    "<li><strong>Unstructured: A library that prepares raw documents for downstream ML tasks</li>\n",
    "<li><strong>pypdf: A pure-python PDF libray capable of splitting, merging, cropping, and transforming PDF files</li>\n",
    "<li><strong>tiktoken: a fast open-source tokenizer by OpenAI</li>\n",
    "<br>https://github.com/tpn/pdfs/blob/master/The%20Elements%20of%20Statistical%20Learning%20-%20Data%20Mining%2C%20Inference%20and%20Prediction%20-%202nd%20Edition%20(ESLII_print4).pdf\n",
    "<br>https://hastie.su.domains/ElemStatLearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f1602-d1f3-49a8-9ae8-2355e48239e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c18350-932d-4ebd-ae42-91ebbded8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unstructured pypdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f188b1f-c652-4e85-acee-11e04a901546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69924ca9-8e35-459b-9ffc-f9cac55c8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570666c-8aae-4327-b950-5730cb9d4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loader that loads data from JSON.\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "\n",
    "class JSONLoader(BaseLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: Union[str, Path],\n",
    "        content_key: Optional[str] = None,\n",
    "        ):\n",
    "        self.file_path = Path(file_path).resolve()\n",
    "        self._content_key = content_key\n",
    "        \n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load and return documents from the JSON file.\"\"\"\n",
    "\n",
    "        docs=[]\n",
    "        # Load JSON file\n",
    "        with open(self.file_path) as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            # Iterate through 'pages'\n",
    "            for page in data['pages']:\n",
    "                parenturl = page['parenturl']\n",
    "                pagetitle = page['pagetitle']\n",
    "                indexeddate = page['indexeddate']\n",
    "                snippets = page['snippets']\n",
    "\n",
    "                # Process snippets for each page\n",
    "                for snippet in snippets:\n",
    "                    index = snippet['index']\n",
    "                    childurl = snippet['childurl']\n",
    "                    text = snippet['text']\n",
    "                    metadata = dict(\n",
    "                        source=childurl,\n",
    "                        title=pagetitle)\n",
    "\n",
    "                    docs.append(Document(page_content=text, metadata=metadata))\n",
    "        return docs\n",
    "\n",
    "file_path='data/data.json'\n",
    "loader = JSONLoader(file_path=file_path)\n",
    "data = loader.load()\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0530b588-a5d0-47b9-93d1-790171b171ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import(\n",
    "    UnstructuredCSVLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    PythonLoader,\n",
    "    PyPDFLoader,\n",
    "    JSONLoader\n",
    ")\n",
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9582799d-329e-4c8a-87d6-dbc4247ee10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read CSV Files\n",
    "countries = pd.read_csv(\"/Users/xiexi/LangChain/data/csv_data/countries.csv\")\n",
    "cities = pd.read_csv(\"/Users/xiexi/LangChain/data/csv_data/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158beaad-c6f5-4032-a672-60b84800562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e07d955-e0e0-4c47-a410-7a4f8ea26645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.73 s\n",
      "Wall time: 6.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read Parquet Files\n",
    "daily_weather = pd.read_parquet(\"/Users/xiexi/LangChain/data/csv_data/daily_weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e11c1daa-8666-411c-8571-540975f23353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27635763"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daily_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dff65551-57c6-4acc-a423-3ed912f38239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>avg_temp_c</th>\n",
       "      <th>min_temp_c</th>\n",
       "      <th>max_temp_c</th>\n",
       "      <th>precipitation_mm</th>\n",
       "      <th>snow_depth_mm</th>\n",
       "      <th>avg_wind_dir_deg</th>\n",
       "      <th>avg_wind_speed_kmh</th>\n",
       "      <th>peak_wind_gust_kmh</th>\n",
       "      <th>avg_sea_level_pres_hpa</th>\n",
       "      <th>sunshine_total_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41515</td>\n",
       "      <td>Asadabad</td>\n",
       "      <td>1957-07-01</td>\n",
       "      <td>Summer</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41515</td>\n",
       "      <td>Asadabad</td>\n",
       "      <td>1957-07-02</td>\n",
       "      <td>Summer</td>\n",
       "      <td>22.8</td>\n",
       "      <td>18.9</td>\n",
       "      <td>32.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41515</td>\n",
       "      <td>Asadabad</td>\n",
       "      <td>1957-07-03</td>\n",
       "      <td>Summer</td>\n",
       "      <td>24.3</td>\n",
       "      <td>16.7</td>\n",
       "      <td>35.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41515</td>\n",
       "      <td>Asadabad</td>\n",
       "      <td>1957-07-04</td>\n",
       "      <td>Summer</td>\n",
       "      <td>26.6</td>\n",
       "      <td>16.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41515</td>\n",
       "      <td>Asadabad</td>\n",
       "      <td>1957-07-05</td>\n",
       "      <td>Summer</td>\n",
       "      <td>30.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id city_name       date  season  avg_temp_c  min_temp_c  max_temp_c  \\\n",
       "0      41515  Asadabad 1957-07-01  Summer        27.0        21.1        35.6   \n",
       "1      41515  Asadabad 1957-07-02  Summer        22.8        18.9        32.2   \n",
       "2      41515  Asadabad 1957-07-03  Summer        24.3        16.7        35.6   \n",
       "3      41515  Asadabad 1957-07-04  Summer        26.6        16.1        37.8   \n",
       "4      41515  Asadabad 1957-07-05  Summer        30.8        20.0        41.7   \n",
       "\n",
       "   precipitation_mm  snow_depth_mm  avg_wind_dir_deg  avg_wind_speed_kmh  \\\n",
       "0               0.0            NaN               NaN                 NaN   \n",
       "1               0.0            NaN               NaN                 NaN   \n",
       "2               1.0            NaN               NaN                 NaN   \n",
       "3               4.1            NaN               NaN                 NaN   \n",
       "4               0.0            NaN               NaN                 NaN   \n",
       "\n",
       "   peak_wind_gust_kmh  avg_sea_level_pres_hpa  sunshine_total_min  \n",
       "0                 NaN                     NaN                 NaN  \n",
       "1                 NaN                     NaN                 NaN  \n",
       "2                 NaN                     NaN                 NaN  \n",
       "3                 NaN                     NaN                 NaN  \n",
       "4                 NaN                     NaN                 NaN  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52a8c37d-3b3a-4359-80d8-0d9cf0d51bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_id', 'city_name', 'date', 'season', 'avg_temp_c', 'min_temp_c',\n",
       "       'max_temp_c', 'precipitation_mm', 'snow_depth_mm', 'avg_wind_dir_deg',\n",
       "       'avg_wind_speed_kmh', 'peak_wind_gust_kmh', 'avg_sea_level_pres_hpa',\n",
       "       'sunshine_total_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c13e304-3307-44be-9273-3692838a6d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2b5fab5-90f5-4c8d-9537-645a17b742ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74597f76-c9af-4016-bb95-555dd9709570",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c312f-59a1-45fe-9699-0cd96ed6db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0bff2-73fc-4b9b-8905-dcceedb18d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8ffab-67d7-4ddf-bd50-b7dda6652663",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14838911-acb5-4291-b7a0-b3718addbef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Asadabad\n",
       "1        Asadabad\n",
       "2        Asadabad\n",
       "3        Asadabad\n",
       "4        Asadabad\n",
       "           ...   \n",
       "24216    Masvingo\n",
       "24217    Masvingo\n",
       "24218    Masvingo\n",
       "24219    Masvingo\n",
       "24220    Masvingo\n",
       "Name: city_name, Length: 27635763, dtype: category\n",
       "Categories (1234, object): ['Aalborg', 'Abakan', 'Abha', 'Abidjan', ..., 'Ōita', 'Ōsaka', 'Şanlıurfa', 'Šibenik']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather.city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29c83caf-1eb6-4281-beb9-e598efdd781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather.to_csv(\"/Users/xiexi/LangChain/data/csv_data/weather_data.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb038610-bbe2-480f-8d4f-70e30fd66020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiexi\\AppData\\Local\\Temp\\ipykernel_41112\\396818087.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  weather_data = pd.read_csv(\"/Users/xiexi/LangChain/data/csv_data/weather_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27635763"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data = pd.read_csv(\"/Users/xiexi/LangChain/data/csv_data/weather_data.csv\")\n",
    "len(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c69ed-bedf-4b2d-84da-6d36e023b6ff",
   "metadata": {},
   "source": [
    "<strong>Loading CSV file using CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0323d1da-bbf4-4ce1-b0b8-b0eae8f34d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "file_path = \"/Users/xiexi/LangChain/data/csv_data/countries.csv\"\n",
    "csv_loader= CSVLoader(file_path=file_path, encoding=\"utf-8\")\n",
    "csv_countries= csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3bb1220d-deb3-406d-a8f2-dce0f1c39521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'country: Afghanistan\\nnative_name: افغانستان\\niso2: AF\\niso3: AFG\\npopulation: 26023100.0\\narea: 652230.0\\ncapital: Kabul\\ncapital_lat: 34.526011\\ncapital_lng: 69.177684\\nregion: Southern and Central Asia\\ncontinent: Asia'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_countries[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836634d-92bc-4fc1-bb33-30352e786451",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/xiexi/LangChain/data/csv_data/weather_data.csv\"\n",
    "csv_loader= CSVLoader(file_path=file_path, encoding=\"utf-8\")\n",
    "weather_data= csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44053ca4-e16f-4edf-9233-1a9ddef74151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'station_id: 41515\\ncity_name: Asadabad\\ndate: 1957-07-01\\nseason: Summer\\navg_temp_c: 27.0\\nmin_temp_c: 21.1\\nmax_temp_c: 35.6\\nprecipitation_mm: 0.0\\nsnow_depth_mm: \\navg_wind_dir_deg: \\navg_wind_speed_kmh: \\npeak_wind_gust_kmh: \\navg_sea_level_pres_hpa: \\nsunshine_total_min: '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f955c2-06d9-428f-9c04-78009a09e967",
   "metadata": {},
   "source": [
    "<strong>Load PDF via by PDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0352741e-8090-4e1e-9b92-ce3fc362ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/Users/xiexi/LangChain/data/mixed_data/ESLII_print12.pdf\"\n",
    "sl_loader = PyPDFLoader(file_path=file_path)\n",
    "sl_data= sl_loader.load_and_split()\n",
    "sl_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93dab52c-7bbc-4cda-b153-dc945a9b00d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ec1d36e-c98a-41dc-9325-223427dac733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import(\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1969340b-5841-4b87-988b-024378c0016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split [\"\\n\\n\"]\n",
    "splitter1 = CharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "splitter2 = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38068ab6-e8f5-4da3-bf9c-e5c58ed137ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_data1 = sl_loader.load_and_split(text_splitter=splitter1)\n",
    "sl_data2 = sl_loader.load_and_split(text_splitter=splitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d1b5529-7173-42ae-93a4-1ea780f5a5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adcaa1cd-e098-4890-86f8-54a32e9a4469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2252"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "027e64f1-69af-4d40-91e5-33b27e2ea1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2164"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data1[600].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84205171-5a8b-4b9d-9459-a44c2c5e8b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data2[600].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103fdb3-9a7f-476f-8318-aec533a950a3",
   "metadata": {},
   "source": [
    "<strong>pip3 install pdfminer.six<br>\n",
    "<strong>pip3 install  unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b156be2-c03b-45f6-8462-581b47b75df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac31ac-cec4-45d4-9c27-35374ec658b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d6b33-fec3-4f62-a456-015e5e12fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197a694-779d-41c8-8acd-9212dbb2b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04625e0-c248-4c29-982b-5654ae70cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da138e5-472e-4f36-b0f9-4991fa2f97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('./data/mixed_data', glob='**/*.pdf')\n",
    "#folder_path=\"/Users/xiexi/LangChain/data/mixed_data/\"\n",
    "mixed = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c412e2-ca6e-4a9a-a5b0-2b050aa94c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "\n",
    "elements = partition(\"./data/mixed_data/ESLII_print12.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e6c19-a8fc-4c2d-86dd-c5a11213eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e842cc51-0de6-484c-b8ac-1be19b6f3fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [05:14<00:00, 104.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "folder_path=\"/Users/xiexi/LangChain/data/mixed_data/\"\n",
    "mixed_loader = DirectoryLoader(\n",
    "    path=folder_path,\n",
    "    use_multithreading=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "mixed_data = mixed_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbe43b6-fbcd-4b56-b157-706dda0294bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mixed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139cd2dd-801f-4c0d-9879-85599574c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Speech Recognition and Graph Transformer Network I\\n\\n\\\\gdef \\\\sam #1 {\\\\mathrm{softargmax}(#1)}\\n\\n\\\\gdef \\\\vect #1 {\\\\boldsymbol{#1}}\\n\\n\\\\gdef \\\\matr #1 {\\\\boldsymbol{#1}}\\n\\n\\\\gdef \\\\E  {\\\\mathbb{E}}\\n\\n\\\\gdef \\\\V  {\\\\mathbb{V}}\\n\\n\\\\gdef \\\\R {\\\\mathbb{R}}\\n\\n\\\\gdef \\\\N {\\\\mathbb{N}}\\n\\n\\\\gdef \\\\relu #1 {\\\\texttt{ReLU}(#1)}\\n\\n\\\\gdef \\\\D {\\\\,\\\\mathrm{d}}\\n\\n\\\\gdef \\\\deriv #1 #2 {\\\\frac{\\\\D #1}{\\\\D #2}}\\n\\n\\\\gdef \\\\pd #1 #2 {\\\\frac{\\\\partial #1}{\\\\partial #2}}\\n\\n\\\\gdef \\\\set #1 {\\\\left\\\\lbrace #1 \\\\right\\\\rbrace} \\n    \\n    % My colours\\n\\n\\\\gdef \\\\aqua #1 {\\\\textcolor{8dd3c7}{#1}}\\n\\n\\\\gdef \\\\yellow #1 {\\\\textcolor{ffffb3}{#1}}\\n\\n\\\\gdef \\\\lavender #1 {\\\\textcolor{bebada}{#1}}\\n\\n\\\\gdef \\\\red #1 {\\\\textcolor{fb8072}{#1}}\\n\\n\\\\gdef \\\\blue #1 {\\\\textcolor{80b1d3}{#1}}\\n\\n\\\\gdef \\\\orange #1 {\\\\textcolor{fdb462}{#1}}\\n\\n\\\\gdef \\\\green #1 {\\\\textcolor{b3de69}{#1}}\\n\\n\\\\gdef \\\\pink #1 {\\\\textcolor{fccde5}{#1}}\\n\\n\\\\gdef \\\\vgrey #1 {\\\\textcolor{d9d9d9}{#1}}\\n\\n\\\\gdef \\\\violet #1 {\\\\textcolor{bc80bd}{#1}}\\n\\n\\\\gdef \\\\unka #1 {\\\\textcolor{ccebc5}{#1}}\\n\\n\\\\gdef \\\\unkb #1 {\\\\textcolor{ffed6f}{#1}} \\n\\n    % Vectors\\n\\n\\\\gdef \\\\vx {\\\\pink{\\\\vect{x }}}\\n\\n\\\\gdef \\\\vy {\\\\blue{\\\\vect{y }}}\\n\\n\\\\gdef \\\\vb {\\\\vect{b}}\\n\\n\\\\gdef \\\\vz {\\\\orange{\\\\vect{z }}}\\n\\n\\\\gdef \\\\vtheta {\\\\vect{\\\\theta }}\\n\\n\\\\gdef \\\\vh {\\\\green{\\\\vect{h }}}\\n\\n\\\\gdef \\\\vq {\\\\aqua{\\\\vect{q }}}\\n\\n\\\\gdef \\\\vk {\\\\yellow{\\\\vect{k }}}\\n\\n\\\\gdef \\\\vv {\\\\green{\\\\vect{v }}}\\n\\n\\\\gdef \\\\vytilde {\\\\violet{\\\\tilde{\\\\vect{y}}}}\\n\\n\\\\gdef \\\\vyhat {\\\\red{\\\\hat{\\\\vect{y}}}}\\n\\n\\\\gdef \\\\vycheck {\\\\blue{\\\\check{\\\\vect{y}}}}\\n\\n\\\\gdef \\\\vzcheck {\\\\blue{\\\\check{\\\\vect{z}}}}\\n\\n\\\\gdef \\\\vztilde {\\\\green{\\\\tilde{\\\\vect{z}}}}\\n\\n\\\\gdef \\\\vmu {\\\\green{\\\\vect{\\\\mu}}}\\n\\n\\\\gdef \\\\vu {\\\\orange{\\\\vect{u}}} \\n    \\n    % Matrices\\n\\n\\\\gdef \\\\mW {\\\\matr{W}}\\n\\n\\\\gdef \\\\mA {\\\\matr{A}}\\n\\n\\\\gdef \\\\mX {\\\\pink{\\\\matr{X}}}\\n\\n\\\\gdef \\\\mY {\\\\blue{\\\\matr{Y}}}\\n\\n\\\\gdef \\\\mQ {\\\\aqua{\\\\matr{Q }}}\\n\\n\\\\gdef \\\\mK {\\\\yellow{\\\\matr{K }}}\\n\\n\\\\gdef \\\\mV {\\\\lavender{\\\\matr{V }}}\\n\\n\\\\gdef \\\\mH {\\\\green{\\\\matr{H }}} \\n\\n    % Coloured math\\n\\n\\\\gdef \\\\cx {\\\\pink{x}}\\n\\n\\\\gdef \\\\ctheta {\\\\orange{\\\\theta}}\\n\\n\\\\gdef \\\\cz {\\\\orange{z}}\\n\\n\\\\gdef \\\\Enc {\\\\lavender{\\\\text{Enc}}}\\n\\n\\\\gdef \\\\Dec {\\\\aqua{\\\\text{Dec}}}\\n\\nModern Speech Recognition\\n\\nThis section is a high level introduction to speech recognition and modern speech recognition specifically why it’s become so good, but what are some of the problems still.\\n\\nAutomatic speech recognition has greatly improved since 2012\\n    \\n      Machine performance can be as good or better than human level performance\\n\\nSpeech recognition still struggles in\\n    \\n      conversational speech\\n      multiple speakers\\n      lots of background noise\\n      the accent of the speakers\\n      certain features not well represented in the training data\\n\\nPre 2012 speech recognition systems consisted of lots of many hand engineered components\\n    \\n      larger dataset is not useful so datasets remain small\\n      combining modules only at inference time instead of learning them together allowed for errors to cascade\\n      researchers hard to know how to improve complex systems\\n\\nPost 2012 speech recognition systems improvements\\n    \\n      replaced a lot of the traditional components\\n      add more data\\n      above two together work in a virtuous cycle\\n\\nThe CTC Loss\\n\\nGiven some input speech utterance X\\\\mXX, which consists of TTT frames of audio. We desire to produce a transcription Y\\\\mYY and we’ll think of our transcription as consisting of the letters of a sentence, so y1y_1y1\\u200b is the first letter yUy_UyU\\u200b is the last letter.\\n\\nX=[x1,...,xT],\\xa0Y=[y1,...,yU]\\\\mX=[x_1,...,x_T],\\\\ \\\\mY=[y_1,...,y_U]X=[x1\\u200b,...,xT\\u200b],\\xa0Y=[y1\\u200b,...,yU\\u200b]\\n\\nCompute conditional probability(the score) to evaluate transcription, we want to maximize the probability.\\n\\nlog\\u2061P(Y∣X;θ)\\\\log{P(\\\\mY \\\\mid \\\\mX;\\\\theta)}logP(Y∣X;θ)\\n\\nExample 1\\n\\nX=[x1,x2,x3],\\xa0Y=[c,a,t]\\\\mX=[x_1, x_2, x_3],\\\\ \\\\mY=[c,a,t]X=[x1\\u200b,x2\\u200b,x3\\u200b],\\xa0Y=[c,a,t]\\n\\nX\\\\mXX has three frames, Y\\\\mYY has three letters, the number of inputs matches the number of outputs, it’s easy to compute the probability by one to one mapping.\\n\\nlog\\u2061P(c∣x1)+log\\u2061P(a∣x2)+log\\u2061P(t∣x3)\\\\log{P(c \\\\mid x_1)} + \\\\log{P(a \\\\mid x_2)} + \\\\log{P(t \\\\mid x_3)}logP(c∣x1\\u200b)+logP(a∣x2\\u200b)+logP(t∣x3\\u200b)\\n\\nExample 2', metadata={'source': '\\\\Users\\\\xiexi\\\\LangChain\\\\data\\\\mixed_data\\\\Speech Recognition and Graph Transformer Network I · Deep Learning.html'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e7e732-9698-4da8-b48c-dd1e064752be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Suppose $\\\\mX=[x_1,x_2,x_3,x_4]$, $\\\\mY=[c,a,t]$, the forward variable $\\\\alpha_2^c$ represents the score of all possible alignments of length two up to the first two frames that ends in $c$ in the first output of the transcription. There's only one possible alignment for that $x_1\\\\rightarrow c$, $x_2\\\\rightarrow c$. This is simple to compute.\\n\\n$$\\\\alpha_2^c=\\\\log{P(c \\\\mid x_1)}+\\\\log{P(c \\\\mid x_2)}$$\\n\\nSimilarly, $\\\\alpha_2^a$ has only one possibility.\\n\\n$$\\\\alpha_2^a=\\\\log{P(c \\\\mid x_1)}+\\\\log{P(a \\\\mid x_2)}$$\\n\\nFor $\\\\alpha_3^a$, there are two possible alignments\\n\\n$A_1$: $x_1\\\\rightarrow c$, $x_2\\\\rightarrow c$, $x_3\\\\rightarrow a$\\n\\n$A_2$: $x_1\\\\rightarrow c$, $x_2\\\\rightarrow a$, $x_3\\\\rightarrow a$\\n\\n$$\\n\\\\alpha_3^a=\\\\text{actual-softmax}[\\\\log{P(A_1)}, \\\\log{P(A_2)}] \\\\\\n\\\\log{P(A_1)}=\\\\log{P(c \\\\mid x_1)}+\\\\log{P(c \\\\mid x_2)}+\\\\log{P(a \\\\mid x_3)} \\\\\\n\\\\log{P(A_2)}=\\\\log{P(c \\\\mid x_1)}+\\\\log{P(a \\\\mid x_2)}+\\\\log{P(a \\\\mid x_3)}\\n$$\\n\\nThis is the naive approach to compute $\\\\alpha_3^a$.\\n\\nUsing this forward variable, we seek to model the probability distribution $P(\\\\mY \\\\mid \\\\mX) = \\\\sum_{a \\\\in A} P(a)$, where $A$ is the set of all possible alignments from $\\\\mY$ to $\\\\mX$.  This decomposes as\\n\\n$$P(\\\\mY \\\\mid \\\\mX) = \\\\sum_{a \\\\in A}  \\\\prod_{t=1}^T P(a_t \\\\mid \\\\mX)$$\\n\\nwhere $P(a_t \\\\mid \\\\mX)$ are the output logits of a system such as an RNN. That is, to compute the likelihood of the transcript $\\\\mY$ we must marginalize over an intractably large number of alignments.  We may do this with a recursive decomposition of the forward variable.  The below presentation is inspired by https://distill.pub/2017/ctc/, which is an excellent introduction to the algorithm.\\n\\nFirst, we permit an alignment to contain the empty output $\\\\epsilon$ in order to account for the fact that audio sequences are longer than their corresponding transcripts.  We also collapse repetitions, so that ${a, \\\\epsilon, a, a, \\\\epsilon, a}$ corresponds to the sequence $aaa$.  We will also define $\\\\alpha$ using an alternative transcript $Z$, which is equal to $\\\\mY$ but is interspersed with $\\\\epsilon$.  That is, $Z = {\\\\epsilon, y_1, \\\\epsilon, y_2, ..., y_n, \\\\epsilon }$.\\n\\nNow, suppose $y_i = y_{i+1}$, so that $Z$ contains a subsequence $y_i, \\\\epsilon, y_{i+1}$, and suppose $y_{i+1}$ occurs at psosition $s$ in $Z$.  Then the alignment for $\\\\alpha_{s}^t$ can be arrived at by one of two ways: either the prediction at time $t-1$ can be $y_{i+1}$ (in which case the repetition is collapsed) or else the prediction at time $t-1$ can be epsilon.  So, we may decompose:\\n\\n$$\\\\alpha_s^t = (\\\\alpha_{s, t-1} + \\\\alpha_{s-1, t-1}) P(z_s \\\\mid \\\\mX)$$\\n\\nwhere the elements of the sum represent the two possible prefixes to the alignment.  If, on the other hand, we have $y_i \\\\ne y_{i+1}$ then there is the additional third possibility that the prediction at time $t-1$ is equal to $y_i$.  So, we have the decomposition\\n\\n$$\\\\alpha_s^t = (\\\\alpha_{s, t-1} + \\\\alpha_{s-1, t-1} + \\\\alpha{s-2, t-1}) P(z_s \\\\mid \\\\mX)$$\\n\\nBy computing $\\\\alpha_{\\\\vert Z\\\\vert}^{T}$, we may effectively marginalize over all possible alignments between the transcript $\\\\mY$ and the audio $\\\\mX$, allowing efficient training and inference.  This is called Connectionist Temporal Classification, or CTC.\", metadata={'source': '\\\\Users\\\\xiexi\\\\LangChain\\\\data\\\\mixed_data\\\\Modern Speech Recognition.md'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_data[1]"
   ]
  },
  {
   "attachments": {
    "88c404da-d472-46aa-90d3-483de0d41627.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "78f3afda-194c-40a5-b5bb-b817b036298e",
   "metadata": {},
   "source": [
    "<strong>BASIC SUMMARY\n",
    "\n",
    "![image.png](attachment:88c404da-d472-46aa-90d3-483de0d41627.png)\n",
    "\n",
    "The “Stuff” chain\r\n",
    "With LangChain, it is not difficult to summarize text of any length. To summarize text with an LLM, there are a few strategies.\r\n",
    "\r\n",
    "If the whole text fits in the context window, then you can simply feed the raw data and get the result. LangChain refers to that strategy as the “stuff“ chain typng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82df9773-16d2-4e70-96ce-002f3fdbcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]='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' #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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da72b664-bc61-44a9-a363-bf37eef98266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of LLM-powered autonomous agents, with a focus on the components of planning, memory, and tool use. It includes case studies and proof-of-concept examples, as well as challenges and references. The author emphasizes the potential of LLMs in creating powerful problem-solving agents, while also highlighting limitations such as finite context length and reliability of natural language interface.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c610001a-8e4e-4a9c-b0c7-2edf5841f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.18 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.0.19)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\xiexi\\anaconda3\\envs\\ml\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86d2df5f-af70-48eb-b3fb-467075254324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The book \"The Elements of Statistical Learning\" is a comprehensive resource that covers various topics in statistics, data mining, and machine learning. Written by professors Trevor Hastie, Robert Tibshirani, and Jerome Friedman, it offers a conceptual framework for understanding these areas and includes many examples and color graphics. The second edition includes new topics such as graphical models, random forests, and ensemble methods. The authors are well-known researchers in the field of statistics and have contributed to the development of statistical modeling software and various data-mining tools.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff'\n",
    "    \n",
    ")\n",
    "chain.run(sl_data[:2])"
   ]
  },
  {
   "attachments": {
    "b379e3fe-0f7f-4a03-a9bf-372f1ed9149c.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ac4d8618-8b43-4c17-b1ff-81e3037b65a3",
   "metadata": {},
   "source": [
    "<strong>Unfortunately for the Stuff chain, it is going to break if the data is too large because the number of tokens sent to the LLM is larger than the context window:\n",
    "![image.png](attachment:b379e3fe-0f7f-4a03-a9bf-372f1ed9149c.png)\n",
    "\n",
    "\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd748447-8dda-4896-a3b1-1f4c322e0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"vi\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "–William Edwards Deming (1900-1993)1\n",
      "We have been gratiﬁed by the popularity of the ﬁrst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning ﬁeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existi ng\n",
      "chapters. Because many readers are familiar with the layout of the ﬁrst\n",
      "edition, we have tried to change it as little as possible. Her e is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Deming and R obert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for th is quote,\n",
      "and ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"viii Preface to the Second Edition\n",
      "Chapter What’s new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generaliza tions\n",
      "of the lasso\n",
      "4.Linear Methods for Classiﬁcation Lasso path for logistic re gression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cr oss-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oﬀ to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiﬁer\n",
      "13.Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,\n",
      "we tended to favor red/greencontrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange/bluecontrast.\n",
      "•We have changed the name of Chapter 6 from “Kernel Methods” to\n",
      "“Kernel Smoothing Methods”, to avoid confusion with the mac hine-\n",
      "learningkernelmethodthatisdiscussedinthecontextofsu pportvec-\n",
      "tor machines (Chapter 12) and more generally in Chapters 5 an d 14.\n",
      "•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of\n",
      "conditional error rates (conditional on the training set) a nd uncondi-\n",
      "tional rates. We have ﬁxed this in the new edition.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Preface to the Second Edition ix\n",
      "•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-\n",
      "ters are probably best read in that order.\n",
      "•In Chapter 17, we have not attempted a comprehensive treatme nt\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we ha ve\n",
      "speciﬁcally omitted coverage of directed graphical models .\n",
      "•Chapter 18 explores the “ p≫N” problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many ar eas, in-\n",
      "cluding genomic and proteomic studies, and document classi ﬁcation.\n",
      "We thank the many readers who have found the (too numerous) er rors in\n",
      "the ﬁrst edition. We apologize for those and have done our bes t to avoid er-\n",
      "rorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry\n",
      "Wasserman for comments on some of the new chapters, and many S tanford\n",
      "graduate and post-doctoral students who oﬀered comments, i n particular\n",
      "Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us th rough this\n",
      "new edition. RT dedicates this edition to the memory of Anna M cPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"x Preface to the Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "–Rutherford D. Roger\n",
      "The ﬁeld of Statistics is constantly challenged by the probl ems that science\n",
      "andindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\n",
      "from agricultural and industrial experiments and were rela tively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challen ges in the\n",
      "areas of data storage, organization and searching have led t o the new ﬁeld\n",
      "of “data mining”; statistical and computational problems i n biology and\n",
      "medicine have created “bioinformatics.” Vast amounts of da ta are being\n",
      "generated in many ﬁelds, and the statistician’s job is to mak e sense of it\n",
      "all: to extract important patterns and trends, and understa nd “what the\n",
      "data says.” We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and t he goal is to\n",
      "describe the associations and patterns among a set of input m easures.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the importa nt new\n",
      "ideas in learning, and explain them in a statistical framewo rk. While some\n",
      "mathematical details are needed, we emphasize the methods a nd their con-\n",
      "ceptual underpinnings rather than their theoretical prope rties. As a result,\n",
      "we hope that this book will appeal not just to statisticians b ut also to\n",
      "researchers and practitioners in a wide variety of ﬁelds.\n",
      "Just as we have learned a great deal from researchers outside of the ﬁeld\n",
      "of statistics, our statistical viewpoint may help others to better understand\n",
      "diﬀerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "–Andreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo B reiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner\n",
      "Stuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computat ional\n",
      "problems, and maintained an excellent computing environme nt. Shin-Ho\n",
      "Bang helped in the production of a number of the ﬁgures. Lee Wi lkinson\n",
      "gavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya\n",
      "Gupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manusc ript and\n",
      "oﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb\n",
      "production team at Springer. Trevor Hastie would like to tha nk the statis-\n",
      "tics department at the University of Cape Town for their hosp itality during\n",
      "the ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for\n",
      "their support of this work. Finally, we would like to thank ou r families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "–Ian Hacking\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xv\n",
      "4 Linear Methods for Classiﬁcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example:SouthAfricanHeartDisease(Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization–Maximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector Classiﬁer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene Classiﬁcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p≫N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p≫N. . . . . 659\n",
      "18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein Classiﬁcation . . . . . . . . . . . . . 668\n",
      "18.5.2 Classiﬁcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person’s bloo d.\n",
      "•Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statis tics, data\n",
      "mining and artiﬁcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The book \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of concepts in data mining, machine learning, and bioinformatics. It covers topics such as supervised and unsupervised learning, neural networks, support vector machines, and boosting. The second edition includes additional topics like graphical models, random forests, and ensemble methods. The authors are professors of statistics at Stanford University and are well-known researchers in the field.\n",
      "\n",
      "The page is dedicated to the parents and family members of the individuals mentioned, including Valerie and Patrick Hastie, Vera and Sami Tibshirani, and Florence and Harry Friedman. The families mentioned include Samantha, Timothy, and Lynda, Charlie, Ryan, Julie, and Cheryl, and Melanie, Dora, Monika, and Ildiko.\n",
      "\n",
      "\"vi\" is a text editor commonly used in Unix-like operating systems.\n",
      "\n",
      "The second edition of \"The Elements of Statistical Learning\" has been updated with four new chapters and some revisions to existing chapters. The layout has remained largely unchanged to accommodate familiar readers. The preface includes a quote often attributed to William Edwards Deming, but the true source of the quote is uncertain.\n",
      "\n",
      "The second edition of the book introduces new chapters and expands on existing topics. Changes have been made to improve readability for colorblind readers. The name of Chapter 6 has been changed to avoid confusion. The discussion of error-rate estimation in Chapter 7 has been clarified.\n",
      "\n",
      "The preface to the second edition of a book suggests that Chapters 15 and 16 should be read after Chapter 10. Chapter 17 covers undirected graphical models and excludes coverage of directed models. Chapter 18 explores the problem of learning in high-dimensional feature spaces. The authors thank readers for finding errors in the first edition and acknowledge individuals for their contributions to the new edition. The book is dedicated to the memory of Anna McPhee.\n",
      "\n",
      "The \"x Preface to the Second Edition\" provides an introduction to the second edition of a certain book or document.\n",
      "\n",
      "The preface discusses the challenges faced by the field of Statistics due to the increasing complexity and size of data. It mentions the emergence of data mining and bioinformatics as new fields to handle these challenges. The preface also highlights the importance of learning from data and the role of computation in statistical sciences. It categorizes learning problems as either supervised or unsupervised, with the goal of prediction or description respectively.\n",
      "\n",
      "The book aims to explain important new ideas in learning using a statistical framework. It emphasizes the methods and conceptual underpinnings rather than theoretical properties. The authors hope that the book will appeal to statisticians, researchers, and practitioners in various fields. They acknowledge the contribution of many people to the completion of the book and express gratitude to their families and parents for their support. The authors also acknowledge the influence of statisticians in changing the ways we reason, experiment, and form opinions.\n",
      "\n",
      "This is page xiii of a book that provides an introduction and overview of supervised learning. It covers topics such as variable types and terminology, prediction approaches like least squares and nearest neighbors, statistical decision theory, local methods in high dimensions, and structured regression models.\n",
      "\n",
      "This section discusses classes of restricted estimators, including roughness penalty and Bayesian methods, kernel methods and local regression, and basis functions and dictionary methods. It also covers model selection and the bias-variance tradeoff. The next section focuses on linear methods for regression, such as linear regression models and least squares, subset selection, shrinkage methods, methods using derived input directions, multiple outcome shrinkage and selection, and the Lasso and related path algorithms. The section concludes with a discussion on computational considerations.\n",
      "\n",
      "The summary provides a list of contents for chapters 4 and 5 of a book on linear methods for classification. Chapter 4 covers topics such as linear regression, linear discriminant analysis, logistic regression, and separating hyperplanes. Chapter 5 discusses basis expansions and regularization techniques, including piecewise polynomials, splines, filtering and feature extraction, smoothing splines, and wavelet smoothing. The appendix includes computational considerations for splines.\n",
      "\n",
      "This section of the book discusses kernel smoothing methods, specifically one-dimensional kernel smoothers and local regression techniques. It also covers the selection of kernel width, structured local regression models, kernel density estimation and classification, radial basis functions and kernels, and mixture models for density estimation and classification. Additionally, it explores model assessment and selection, including bias, variance, and model complexity, as well as cross-validation and bootstrap methods. The section concludes with a discussion on model inference and averaging.\n",
      "\n",
      "This section of the book covers various statistical methods including the Bootstrap and Maximum Likelihood Methods, Bayesian Methods, the EM Algorithm, MCMC for Sampling from the Posterior, Bagging, Model Averaging and Stacking, and Stochastic Search. It also discusses Additive Models, Trees, and Related Methods such as Generalized Additive Models, Tree-Based Methods, PRIM (Bump Hunting), MARS (Multivariate Adaptive Regression Splines), Hierarchical Mixtures of Experts, and Missing Data. Lastly, it covers Boosting Methods.\n",
      "\n",
      "This section of the book discusses boosting, a technique for fitting additive models. It covers topics such as exponential loss and AdaBoost, loss functions and robustness, \"off-the-shelf\" procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation of results, and includes several illustrations. The next section of the book focuses on neural networks, including projection pursuit regression, fitting neural networks, training issues, and examples.\n",
      "\n",
      "The section \"Support Vector Machines and Flexible Discriminants\" discusses the support vector classifier and support vector machines, as well as their application in classification and regression problems. It also covers topics such as kernels, function estimation, and the curse of dimensionality. The section \"Generalizing Linear Discriminant Analysis\" introduces flexible discriminant analysis and penalized discriminant analysis. The section \"Prototype Methods and Nearest-Neighbors\" explores methods such as K-means clustering, learning vector quantization, and Gaussian mixtures. It also discusses k-nearest neighbor classifiers and adaptive nearest-neighbor methods. Computational considerations are also addressed in this section.\n",
      "\n",
      "This section of the book covers unsupervised learning techniques, including association rules, cluster analysis, self-organizing maps, principal components, non-negative matrix factorization, independent component analysis, multidimensional scaling, nonlinear dimension reduction, and the Google PageRank algorithm. It provides an introduction to each topic and includes examples and practical issues.\n",
      "\n",
      "This section of the book covers topics such as random forests, ensemble learning, undirected graphical models, and high-dimensional problems. It provides an introduction to each topic and discusses various details and analysis related to them. Exercises are also included for further practice.\n",
      "\n",
      "This section of the document discusses various linear classifiers and their applications in high-dimensional data analysis. Topics covered include diagonal linear discriminant analysis, nearest shrunken centroids, linear classifiers with quadratic regularization, linear classifiers with L1 regularization, classification when features are unavailable, supervised principal components for high-dimensional regression, feature assessment and the multiple-testing problem. The section concludes with bibliographic notes, exercises, and references.\n",
      "\n",
      "This excerpt discusses the importance of statistical learning in various fields and provides examples of learning problems. It highlights that learning from data involves predicting outcomes based on features and having a training set of data to observe the outcome and features.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"The book \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of concepts in data mining, machine learning, and bioinformatics. It covers topics such as supervised and unsupervised learning, neural networks, support vector machines, and boosting. The second edition includes additional topics like graphical models, random forests, and ensemble methods. The authors are professors of statistics at Stanford University and are well-known researchers in the field. The book aims to explain important new ideas in learning using a statistical framework and emphasizes methods and conceptual underpinnings. It is dedicated to the memory of Anna McPhee and thanks readers and contributors for their support. The preface discusses the challenges faced by the field of statistics and the importance of learning from data. It categorizes learning problems as supervised or unsupervised and highlights the role of computation in statistical sciences. The book also covers linear methods for regression and classification, kernel smoothing methods, statistical methods such as the bootstrap and maximum likelihood methods, and discusses boosting, support vector machines, and flexible discriminants. It also covers unsupervised learning techniques and topics such as random forests, ensemble learning, undirected graphical models, and high-dimensional problems. The book provides examples of learning problems and highlights the importance of statistical learning in various fields.\"'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(sl_data[:20]) #chain.run(sl_data[:200])error for out of limited . Limit 60000, Requested 107442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6affb2e-89eb-4a5b-a4f7-fa7cd3db8525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_id                      category\n",
       "city_name                       category\n",
       "date                      datetime64[ns]\n",
       "season                          category\n",
       "avg_temp_c                       float64\n",
       "min_temp_c                       float64\n",
       "max_temp_c                       float64\n",
       "precipitation_mm                 float64\n",
       "snow_depth_mm                    float64\n",
       "avg_wind_dir_deg                 float64\n",
       "avg_wind_speed_kmh               float64\n",
       "peak_wind_gust_kmh               float64\n",
       "avg_sea_level_pres_hpa           float64\n",
       "sunshine_total_min               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5e2b8-793c-4b0a-bc16-db96219939ec",
   "metadata": {},
   "source": [
    "<strong>Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e45e95b3-6d2d-4733-a28a-15ac1ec79ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'), llm=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_proxy='')), document_variable_name='text')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63ae04ec-75ba-48ef-bb98-44b354a44172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d94711e-a78b-49a5-aeaa-9a6ef2e3f6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47069d5e-3299-4777-aa2c-12879dfb01a4",
   "metadata": {},
   "source": [
    "<strong>Summary in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "098eac5e-ac89-4a0a-9643-5eef8cec3cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'《Springer统计学系列》是一本描述统计学领域中重要概念的书籍，涵盖了数据挖掘、机器学习和生物信息学等新兴领域。该书强调概念而非数学，提供了许多示例和图形。新版增加了图形模型、随机森林、集成方法等内容。作者是斯坦福大学的统计学教授，他们在该领域有着显著的研究成果。书籍的范围广泛，适用于统计学家和对科学或工业中的数据挖掘感兴趣的人。'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Write a concise summary of the following in chinese:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY IN CHINESE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt   \n",
    ")\n",
    "\n",
    "chain.run(sl_data[:2])"
   ]
  },
  {
   "attachments": {
    "313e2561-6623-4a4c-9513-dd19b6d3acc7.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "10ec2978-407e-44a7-ae34-45c551ff619c",
   "metadata": {},
   "source": [
    "<strong>\n",
    "The “Map-reduce“ chain\r\n",
    "When we want to summarize a lot of data, we can use the Map-reduce strategy. We break down the data into multiple chunks, summarize each chunk, and summarize the concatenated summaries in a final \"combine\" step\n",
    "\n",
    "![image.png](attachment:313e2561-6623-4a4c-9513-dd19b6d3acc7.png):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "899cca42-e835-40c2-8736-0d0bdb5289db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The Elements of Statistical Learning\" is a comprehensive book on statistics, data mining, and machine learning. Written by renowned professors, it covers a wide range of topics including neural networks and support vector machines. The second edition includes new chapters and updates to existing ones. The preface discusses the challenges in the field of statistics and the importance of extracting patterns from data. The book aims to introduce new ideas in learning and focuses on methods and conceptual underpinnings. It covers supervised and unsupervised learning techniques, linear methods for regression and classification, kernel smoothing methods, model assessment and selection, and boosting and neural networks. It also discusses support vector machines, flexible discriminants, and unsupervised learning techniques such as association rules and cluster analysis. The book emphasizes the practical application of statistical learning in various fields.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a89895-265f-41c2-9f16-9688ba55bdf3",
   "metadata": {},
   "source": [
    "<strong>Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6abc25c2-4769-4710-a946-7e176519c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446928c3-200a-440e-85d4-04a1e1b897f0",
   "metadata": {},
   "source": [
    "<strong>And a prompt for the combine step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33f99600-da73-4b94-85eb-48c06e94b2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.combine_document_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1b2f6cd-8e87-4133-938d-895a6a3772a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "vi\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "–William Edwards Deming (1900-1993)1\n",
      "We have been gratiﬁed by the popularity of the ﬁrst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning ﬁeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existi ng\n",
      "chapters. Because many readers are familiar with the layout of the ﬁrst\n",
      "edition, we have tried to change it as little as possible. Her e is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Deming and R obert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for th is quote,\n",
      "and ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "viii Preface to the Second Edition\n",
      "Chapter What’s new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generaliza tions\n",
      "of the lasso\n",
      "4.Linear Methods for Classiﬁcation Lasso path for logistic re gression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cr oss-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oﬀ to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiﬁer\n",
      "13.Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,\n",
      "we tended to favor red/greencontrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange/bluecontrast.\n",
      "•We have changed the name of Chapter 6 from “Kernel Methods” to\n",
      "“Kernel Smoothing Methods”, to avoid confusion with the mac hine-\n",
      "learningkernelmethodthatisdiscussedinthecontextofsu pportvec-\n",
      "tor machines (Chapter 12) and more generally in Chapters 5 an d 14.\n",
      "•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of\n",
      "conditional error rates (conditional on the training set) a nd uncondi-\n",
      "tional rates. We have ﬁxed this in the new edition.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Preface to the Second Edition ix\n",
      "•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-\n",
      "ters are probably best read in that order.\n",
      "•In Chapter 17, we have not attempted a comprehensive treatme nt\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we ha ve\n",
      "speciﬁcally omitted coverage of directed graphical models .\n",
      "•Chapter 18 explores the “ p≫N” problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many ar eas, in-\n",
      "cluding genomic and proteomic studies, and document classi ﬁcation.\n",
      "We thank the many readers who have found the (too numerous) er rors in\n",
      "the ﬁrst edition. We apologize for those and have done our bes t to avoid er-\n",
      "rorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry\n",
      "Wasserman for comments on some of the new chapters, and many S tanford\n",
      "graduate and post-doctoral students who oﬀered comments, i n particular\n",
      "Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us th rough this\n",
      "new edition. RT dedicates this edition to the memory of Anna M cPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "x Preface to the Second Edition\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "–Rutherford D. Roger\n",
      "The ﬁeld of Statistics is constantly challenged by the probl ems that science\n",
      "andindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\n",
      "from agricultural and industrial experiments and were rela tively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challen ges in the\n",
      "areas of data storage, organization and searching have led t o the new ﬁeld\n",
      "of “data mining”; statistical and computational problems i n biology and\n",
      "medicine have created “bioinformatics.” Vast amounts of da ta are being\n",
      "generated in many ﬁelds, and the statistician’s job is to mak e sense of it\n",
      "all: to extract important patterns and trends, and understa nd “what the\n",
      "data says.” We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and t he goal is to\n",
      "describe the associations and patterns among a set of input m easures.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the importa nt new\n",
      "ideas in learning, and explain them in a statistical framewo rk. While some\n",
      "mathematical details are needed, we emphasize the methods a nd their con-\n",
      "ceptual underpinnings rather than their theoretical prope rties. As a result,\n",
      "we hope that this book will appeal not just to statisticians b ut also to\n",
      "researchers and practitioners in a wide variety of ﬁelds.\n",
      "Just as we have learned a great deal from researchers outside of the ﬁeld\n",
      "of statistics, our statistical viewpoint may help others to better understand\n",
      "diﬀerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "–Andreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo B reiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner\n",
      "Stuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computat ional\n",
      "problems, and maintained an excellent computing environme nt. Shin-Ho\n",
      "Bang helped in the production of a number of the ﬁgures. Lee Wi lkinson\n",
      "gavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya\n",
      "Gupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manusc ript and\n",
      "oﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb\n",
      "production team at Springer. Trevor Hastie would like to tha nk the statis-\n",
      "tics department at the University of Cape Town for their hosp itality during\n",
      "the ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for\n",
      "their support of this work. Finally, we would like to thank ou r families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "–Ian Hacking\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xv\n",
      "4 Linear Methods for Classiﬁcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example:SouthAfricanHeartDisease(Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization–Maximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector Classiﬁer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene Classiﬁcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p≫N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p≫N. . . . . 659\n",
      "18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein Classiﬁcation . . . . . . . . . . . . . 668\n",
      "18.5.2 Classiﬁcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person’s bloo d.\n",
      "•Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statis tics, data\n",
      "mining and artiﬁcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of summaries:\n",
      "\n",
      "Based on the list of documents provided, the main themes can be identified as follows:\n",
      "\n",
      "1. Springer Series in Statistics: This document appears to be a series of books or publications related to statistics.\n",
      "\n",
      "2. The Elements of Statistical Learning: This document is a book that discusses data mining, inference, and prediction in the context of statistical learning. It covers various topics such as neural networks, support vector machines, classification trees, boosting, graphical models, random forests, ensemble methods, and more.\n",
      "\n",
      "3. Trevor Hastie, Robert Tibshirani, Jerome Friedman: This document provides information about the authors of \"The Elements of Statistical Learning.\" They are professors of statistics at Stanford University and have made significant contributions to the field.\n",
      "\n",
      "Overall, the main themes of these documents revolve around statistics, data mining, machine learning, and their applications in various fields such as medicine, biology, finance, and marketing.\n",
      "\n",
      "The main themes identified in this list of documents are family and gratitude.\n",
      "\n",
      "Unfortunately, without any context or specific information about the content or nature of the documents listed, it is impossible to determine the main themes. Please provide more details or specific information about the documents for a more accurate analysis.\n",
      "\n",
      "The main themes in the given list of documents are:\n",
      "\n",
      "1. The popularity and success of the first edition of \"The Elements of Statistical Learning.\"\n",
      "2. The motivation to update the book with a second edition due to the fast pace of research in the statistical learning field.\n",
      "3. The addition of four new chapters and updates to existing chapters in the second edition.\n",
      "4. The attempt to maintain the familiar layout of the first edition while making minimal changes.\n",
      "5. The inclusion of a quote by William Edwards Deming, emphasizing the importance of data in decision making.\n",
      "\n",
      "Based on the list of documents provided, the main themes can be identified as follows:\n",
      "\n",
      "1. Introduction\n",
      "2. Supervised Learning\n",
      "3. Linear Methods for Regression and Classification\n",
      "4. Basis Expansions and Regularization\n",
      "5. Kernel Smoothing Methods\n",
      "6. Model Assessment and Selection\n",
      "7. Model Inference and Averaging\n",
      "8. Additive Models, Trees, and Related Methods\n",
      "9. Boosting and Additive Trees\n",
      "10. Neural Networks\n",
      "11. Support Vector Machines and Flexible Discriminants\n",
      "12. Prototype Methods and Nearest-Neighbors\n",
      "13. Unsupervised Learning\n",
      "14. Random Forests\n",
      "15. Ensemble Learning\n",
      "16. Undirected Graphical Models\n",
      "17. High-Dimensional Problems\n",
      "\n",
      "Some additional notes and changes made in the second edition of the documents are also mentioned.\n",
      "\n",
      "Based on the list of documents provided, the main themes can be identified as follows:\n",
      "\n",
      "1. Chapter sequencing and order: The preface mentions that Chapters 15 and 16 follow naturally from Chapter 10, suggesting that there is a specific order in which the chapters should be read.\n",
      "\n",
      "2. Graphical models: Chapter 17 discusses graphical models, specifically undirected models, and new methods for their estimation. Directed graphical models are specifically mentioned to be omitted due to lack of space.\n",
      "\n",
      "3. \"p≫N\" problem: Chapter 18 explores the \"p≫N\" problem, which refers to learning in high-dimensional feature spaces. This problem is relevant in various areas such as genomic and proteomic studies and document classification.\n",
      "\n",
      "4. Acknowledgments and errors: The preface acknowledges the readers who found errors in the first edition and mentions that efforts have been made to avoid errors in the new edition. It also thanks individuals who provided comments on the new chapters and acknowledges the dedication of the edition to the memory of Anna McPhee.\n",
      "\n",
      "Overall, the main themes in these documents relate to the sequencing of chapters, the discussion of graphical models (specifically undirected models), the exploration of the \"p≫N\" problem, and acknowledgments and corrections in the second edition.\n",
      "\n",
      "It is not possible to identify the main themes based on the given information. The list only includes one document titled \"x Preface to the Second Edition.\" More information or additional documents would be required to identify the main themes.\n",
      "\n",
      "1. The challenges and advancements in the field of Statistics due to the explosion of information and complexity.\n",
      "2. The emergence of new fields such as data mining and bioinformatics to address the challenges in data storage, organization, and analysis.\n",
      "3. The role of computation and the contributions of researchers from other fields such as computer science and engineering in advancing the statistical sciences.\n",
      "4. The distinction between supervised and unsupervised learning, with a focus on predicting outcomes and describing patterns in input measures.\n",
      "\n",
      "Based on this list of documents, the main themes can be identified as follows:\n",
      "\n",
      "1. Learning and new ideas: The preface mentions that the book aims to bring together important new ideas in learning and explain them in a statistical framework. The authors emphasize the methods and their conceptual underpinnings.\n",
      "\n",
      "2. Statistical viewpoint: The authors mention that their statistical viewpoint may help others understand different aspects of learning.\n",
      "\n",
      "3. Acknowledgments: The authors acknowledge the contributions of many people to the conception and completion of the book. They mention specific individuals who have greatly influenced their careers and provided advice and help on computational problems.\n",
      "\n",
      "4. Interpretation: The quote by Andreas Buja highlights the importance of interpretation in enabling others to think fruitfully about an idea.\n",
      "\n",
      "5. Support and gratitude: The authors express gratitude towards their families, parents, and organizations that have supported their work. They also thank the statistics department at the University of Cape Town and NSF and NIH for their support.\n",
      "\n",
      "6. Impact of statisticians: The quote by Ian Hacking mentions how statisticians have changed the ways we reason, experiment, and form opinions, without necessarily discovering new facts or technical developments.\n",
      "\n",
      "From the list of documents provided, the main themes that can be identified are:\n",
      "\n",
      "1. Introduction\n",
      "2. Overview of Supervised Learning\n",
      "3. Variable Types and Terminology\n",
      "4. Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors\n",
      "5. Statistical Decision Theory\n",
      "6. Local Methods in High Dimensions\n",
      "7. Statistical Models, Supervised Learning, and Function Approximation\n",
      "8. Structured Regression Models\n",
      "\n",
      "Based on the list of documents provided, the main themes appear to be:\n",
      "1. Classes of Restricted Estimators\n",
      "2. Model Selection and the Bias-Variance Tradeoff\n",
      "3. Linear Methods for Regression\n",
      "4. Subset Selection\n",
      "5. Shrinkage Methods\n",
      "6. Methods Using Derived Input Directions\n",
      "7. Multiple Outcome Shrinkage and Selection\n",
      "8. More on the Lasso and Related Path Algorithms\n",
      "9. Computational Considerations\n",
      "\n",
      "Based on the list of documents, the main themes are:\n",
      "\n",
      "1. Linear Methods for Classification:\n",
      "   - Linear Regression of an Indicator Matrix\n",
      "   - Linear Discriminant Analysis\n",
      "   - Regularized Discriminant Analysis\n",
      "   - Reduced-Rank Linear Discriminant Analysis\n",
      "   - Logistic Regression\n",
      "   - Fitting Logistic Regression Models\n",
      "   - Example: South African Heart Disease\n",
      "   - Quadratic Approximations and Inference\n",
      "   - L1 Regularized Logistic Regression\n",
      "   - Logistic Regression or LDA?\n",
      "   - Separating Hyperplanes\n",
      "   - Rosenblatt’s Perceptron Learning Algorithm\n",
      "   - Optimal Separating Hyperplanes\n",
      "\n",
      "2. Basis Expansions and Regularization:\n",
      "   - Piecewise Polynomials and Splines\n",
      "   - Natural Cubic Splines\n",
      "   - Example: South African Heart Disease (Continued)\n",
      "   - Example: Phoneme Recognition\n",
      "   - Filtering and Feature Extraction\n",
      "   - Smoothing Splines\n",
      "   - Degrees of Freedom and Smoother Matrices\n",
      "   - Automatic Selection of the Smoothing Parameters\n",
      "   - Fixing the Degrees of Freedom\n",
      "   - The Bias-Variance Tradeoff\n",
      "   - Nonparametric Logistic Regression\n",
      "   - Multidimensional Splines\n",
      "   - Regularization and Reproducing Kernel Hilbert Spaces\n",
      "   - Spaces of Functions Generated by Kernels\n",
      "   - Examples of RKHS\n",
      "   - Wavelet Smoothing\n",
      "   - Wavelet Bases and the Wavelet Transform\n",
      "   - Adaptive Wavelet Filtering\n",
      "\n",
      "3. Miscellaneous:\n",
      "   - Bibliographic Notes\n",
      "   - Exercises\n",
      "   - Appendix: Computational Considerations for Splines\n",
      "   - Appendix: B-splines\n",
      "   - Appendix: Computations for Smoothing Splines\n",
      "\n",
      "Based on this list of documents, the main themes are:\n",
      "\n",
      "1. Kernel Smoothing Methods (Chapter 6): This section discusses various methods and techniques related to kernel smoothing, including one-dimensional kernel smoothers, local regression, selecting the width of the kernel, structured local regression models, kernel density estimation and classification, radial basis functions and kernels, mixture models for density estimation and classification, and computational considerations.\n",
      "\n",
      "2. Model Assessment and Selection (Chapter 7): This section focuses on model assessment and selection techniques, such as bias, variance, and model complexity, the bias-variance decomposition, optimism of the training error rate, estimates of in-sample prediction error, the effective number of parameters, the Bayesian approach and BIC, minimum description length, Vapnik-Chervonenkis dimension, cross-validation, bootstrap methods, and conditional or expected test error.\n",
      "\n",
      "3. Model Inference and Averaging (Chapter 8): This section discusses model inference and averaging techniques, including various concepts and methods related to model inference and averaging. However, the specific details are not mentioned in the provided text.\n",
      "\n",
      "Based on the list of documents, the main themes identified are:\n",
      "\n",
      "1. Bootstrap and Maximum Likelihood Methods\n",
      "2. Bayesian Methods\n",
      "3. EM Algorithm\n",
      "4. MCMC for Sampling from the Posterior\n",
      "5. Bagging\n",
      "6. Model Averaging and Stacking\n",
      "7. Stochastic Search: Bumping\n",
      "8. Additive Models, Trees, and Related Methods\n",
      "9. Generalized Additive Models\n",
      "10. Tree-Based Methods\n",
      "11. PRIM: Bump Hunting\n",
      "12. MARS: Multivariate Adaptive Regression Splines\n",
      "13. Hierarchical Mixtures of Experts\n",
      "14. Missing Data\n",
      "15. Computational Considerations\n",
      "16. Boosting Methods\n",
      "\n",
      "Based on this list of documents, the main themes seem to be boosting, gradient boosting, and neural networks. Other related themes include additive modeling, exponential loss, robustness, regularization, interpretation, and computational considerations.\n",
      "\n",
      "Based on the list of documents, the main themes are:\n",
      "\n",
      "1. Support Vector Machines and Flexible Discriminants\n",
      "2. Prototype Methods and Nearest-Neighbors\n",
      "\n",
      "Based on the list of documents, the main themes can be identified as follows:\n",
      "\n",
      "1. Unsupervised Learning\n",
      "2. Association Rules and Market Basket Analysis\n",
      "3. Cluster Analysis\n",
      "4. Self-Organizing Maps\n",
      "5. Principal Components, Curves, and Surfaces\n",
      "6. Non-negative Matrix Factorization\n",
      "7. Independent Component Analysis and Exploratory Projection Pursuit\n",
      "8. Multidimensional Scaling\n",
      "9. Nonlinear Dimension Reduction and Local Multidimensional Scaling\n",
      "10. The Google PageRank Algorithm\n",
      "\n",
      "Based on the list of documents, the main themes are:\n",
      "\n",
      "1. Random Forests (Chapter 15)\n",
      "2. Ensemble Learning (Chapter 16)\n",
      "3. Undirected Graphical Models (Chapter 17)\n",
      "4. High-Dimensional Problems (Chapter 18)\n",
      "\n",
      "Based on the list of documents, the main themes appear to be:\n",
      "\n",
      "1. Linear Discriminant Analysis and Nearest Shrunken Centroids\n",
      "2. Linear Classifiers with Quadratic Regularization\n",
      "   - Regularized Discriminant Analysis\n",
      "   - Logistic Regression with Quadratic Regularization\n",
      "   - The Support Vector Classifier\n",
      "   - Feature Selection\n",
      "   - Computational Shortcuts When p≫N\n",
      "3. Linear Classifiers with L1 Regularization\n",
      "   - Application of Lasso to Protein Mass Spectroscopy\n",
      "   - The Fused Lasso for Functional Data\n",
      "4. Classification When Features are Unavailable\n",
      "   - Example: String Kernels and Protein Classification\n",
      "   - Classification and Other Models Using Inner-Product Kernels and Pairwise Distances\n",
      "   - Example: Abstracts Classification\n",
      "5. High-Dimensional Regression: Supervised Principal Components\n",
      "   - Connection to Latent-Variable Modeling\n",
      "   - Relationship with Partial Least Squares\n",
      "   - Pre-Conditioning for Feature Selection\n",
      "6. Feature Assessment and the Multiple-Testing Problem\n",
      "   - The False Discovery Rate\n",
      "   - Asymmetric Cutpoints and the SAM Procedure\n",
      "   - A Bayesian Interpretation of the FDR\n",
      "7. Bibliographic Notes\n",
      "8. Exercises\n",
      "9. References\n",
      "10. Author Index\n",
      "11. Index\n",
      "\n",
      "The main themes identified in the set of documents are:\n",
      "1. Statistical learning and its importance in various fields.\n",
      "2. Examples of learning problems, including predicting outcomes and identifying patterns.\n",
      "3. The intersection of statistical learning with disciplines such as statistics, data mining, artificial intelligence, and engineering.\n",
      "4. Learning from data and the process of predicting outcomes based on features and training data.\n",
      "\n",
      "Take these and distill it into a final, consolidated list of the main themes. \n",
      "Return that list as a comma separated list. \n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Statistical learning, data mining, machine learning, applications in various fields'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_template = \"\"\"The following is a set of documents\n",
    "\n",
    "{text}\n",
    "\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "combine_template = \"\"\"The following is a set of summaries:\n",
    "\n",
    "{text}\n",
    "\n",
    "Take these and distill it into a final, consolidated list of the main themes. \n",
    "Return that list as a comma separated list. \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "combine_prompt = PromptTemplate.from_template(combine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3e0d26a-d498-44ab-8542-81dd403345e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"vi\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "–William Edwards Deming (1900-1993)1\n",
      "We have been gratiﬁed by the popularity of the ﬁrst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning ﬁeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existi ng\n",
      "chapters. Because many readers are familiar with the layout of the ﬁrst\n",
      "edition, we have tried to change it as little as possible. Her e is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Deming and R obert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for th is quote,\n",
      "and ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"viii Preface to the Second Edition\n",
      "Chapter What’s new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generaliza tions\n",
      "of the lasso\n",
      "4.Linear Methods for Classiﬁcation Lasso path for logistic re gression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cr oss-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oﬀ to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiﬁer\n",
      "13.Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,\n",
      "we tended to favor red/greencontrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange/bluecontrast.\n",
      "•We have changed the name of Chapter 6 from “Kernel Methods” to\n",
      "“Kernel Smoothing Methods”, to avoid confusion with the mac hine-\n",
      "learningkernelmethodthatisdiscussedinthecontextofsu pportvec-\n",
      "tor machines (Chapter 12) and more generally in Chapters 5 an d 14.\n",
      "•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of\n",
      "conditional error rates (conditional on the training set) a nd uncondi-\n",
      "tional rates. We have ﬁxed this in the new edition.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Preface to the Second Edition ix\n",
      "•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-\n",
      "ters are probably best read in that order.\n",
      "•In Chapter 17, we have not attempted a comprehensive treatme nt\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we ha ve\n",
      "speciﬁcally omitted coverage of directed graphical models .\n",
      "•Chapter 18 explores the “ p≫N” problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many ar eas, in-\n",
      "cluding genomic and proteomic studies, and document classi ﬁcation.\n",
      "We thank the many readers who have found the (too numerous) er rors in\n",
      "the ﬁrst edition. We apologize for those and have done our bes t to avoid er-\n",
      "rorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry\n",
      "Wasserman for comments on some of the new chapters, and many S tanford\n",
      "graduate and post-doctoral students who oﬀered comments, i n particular\n",
      "Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us th rough this\n",
      "new edition. RT dedicates this edition to the memory of Anna M cPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"x Preface to the Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "–Rutherford D. Roger\n",
      "The ﬁeld of Statistics is constantly challenged by the probl ems that science\n",
      "andindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame\n",
      "from agricultural and industrial experiments and were rela tively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challen ges in the\n",
      "areas of data storage, organization and searching have led t o the new ﬁeld\n",
      "of “data mining”; statistical and computational problems i n biology and\n",
      "medicine have created “bioinformatics.” Vast amounts of da ta are being\n",
      "generated in many ﬁelds, and the statistician’s job is to mak e sense of it\n",
      "all: to extract important patterns and trends, and understa nd “what the\n",
      "data says.” We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising\n",
      "that much of this new development has been done by researcher s in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly catego rized as\n",
      "eithersupervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and t he goal is to\n",
      "describe the associations and patterns among a set of input m easures.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the importa nt new\n",
      "ideas in learning, and explain them in a statistical framewo rk. While some\n",
      "mathematical details are needed, we emphasize the methods a nd their con-\n",
      "ceptual underpinnings rather than their theoretical prope rties. As a result,\n",
      "we hope that this book will appeal not just to statisticians b ut also to\n",
      "researchers and practitioners in a wide variety of ﬁelds.\n",
      "Just as we have learned a great deal from researchers outside of the ﬁeld\n",
      "of statistics, our statistical viewpoint may help others to better understand\n",
      "diﬀerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "–Andreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo B reiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner\n",
      "Stuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computat ional\n",
      "problems, and maintained an excellent computing environme nt. Shin-Ho\n",
      "Bang helped in the production of a number of the ﬁgures. Lee Wi lkinson\n",
      "gavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya\n",
      "Gupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manusc ript and\n",
      "oﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb\n",
      "production team at Springer. Trevor Hastie would like to tha nk the statis-\n",
      "tics department at the University of Cape Town for their hosp itality during\n",
      "the ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for\n",
      "their support of this work. Finally, we would like to thank ou r families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "–Ian Hacking\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xv\n",
      "4 Linear Methods for Classiﬁcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example:SouthAfricanHeartDisease(Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization–Maximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector Classiﬁer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene Classiﬁcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p≫N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p≫N. . . . . 659\n",
      "18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein Classiﬁcation . . . . . . . . . . . . . 668\n",
      "18.5.2 Classiﬁcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person’s bloo d.\n",
      "•Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statis tics, data\n",
      "mining and artiﬁcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2 1. Introduction\n",
      "TABLE 1.1. Average percentage of words or characters in an email messag e\n",
      "equal to the indicated word or character. We have chosen the w ords and characters\n",
      "showing the largest diﬀerence between spamandemail.\n",
      "george you your hp free hpl ! our re edu remove\n",
      "spam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\n",
      "email 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\n",
      "measurements for a set of objects (such as people). Using thi s data we build\n",
      "a prediction model, or learner, which will enable us to predict the outcome\n",
      "for new unseen objects. A good learner is one that accurately predicts such\n",
      "an outcome.\n",
      "The examples above describe what is called the supervised learning prob-\n",
      "lem. It is called “supervised” because of the presence of the outcome vari-\n",
      "able to guide the learning process. In the unsupervised learning problem ,\n",
      "we observe only the features and have no measurements of the o utcome.\n",
      "Our task is rather to describe how the data are organized or cl ustered. We\n",
      "devote most of this book to supervised learning; the unsuper vised problem\n",
      "is less developed in the literature, and is the focus of Chapt er 14.\n",
      "Here are some examples of real learning problems that are dis cussed in\n",
      "this book.\n",
      "Example 1: Email Spam\n",
      "The data for this example consists of information from 4601 e mail mes-\n",
      "sages, in a study to try to predict whether the email was junk e mail, or\n",
      "“spam.” The objective was to design an automatic spam detect or that\n",
      "could ﬁlter out spam before clogging the users’ mailboxes. F or all 4601\n",
      "email messages, the true outcome (email type) emailorspamis available,\n",
      "along with the relative frequencies of 57 of the most commonl y occurring\n",
      "words and punctuation marks in the email message. This is a su pervised\n",
      "learning problem, with the outcome the class variable email/spam. It is also\n",
      "called aclassiﬁcation problem.\n",
      "Table 1.1 lists the words and characters showing the largest average\n",
      "diﬀerence between spamandemail.\n",
      "Our learning method has to decide which features to use and ho w: for\n",
      "example, we might use a rule such as\n",
      "if (%george<0.6) & (%you>1.5) then spam\n",
      "elseemail.\n",
      "Another form of a rule might be:\n",
      "if (0.2·%you−0.3·%george)>0 then spam\n",
      "elseemail.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"1. Introduction 3\n",
      "lpsa−1 1 2 3 4\n",
      "ooo ooo ooo oo o ooooo o oooo oooooooooo ooooooooooooooo oo ooooooooooooo oo oooo ooooooooooooooooooooooooooooo\n",
      "oo o ooo ooo oo o oooooooooooooooooooo ooooo o ooooooooo oo ooooooo oooooo oo oooo ooooooooooooooooooooooooooooo40 50 60 70 80\n",
      "oo o ooo ooo oo o oooooo o ooooooooooooo ooooooooooooooo o o o oooooo oo oooo oo oooo oooooooooooooo ooooooooooooooo\n",
      "oo o ooo o o o oo ooooo o oo o o o ooooo ooo oo o oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\n",
      "oo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\n",
      "oo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo o oo oooo ooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\n",
      "oo o ooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo oo oooo ooooo o ooooooooo o ooooooooooooo\n",
      "0 1 2 3 4 5oo o ooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o o oo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "lcavol\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo o\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "oo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "oo o o\n",
      "oo o o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo o o\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooo o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooolweight\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "oo o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo o\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "2.5 3.5 4.5oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo40 50 60 70 80ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "age\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o o oooooo oo o oo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o o oooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o o oooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "oo o ooooo\n",
      "o ooo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o oo o o ooo\n",
      "o o oo\n",
      "o o ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "o o o oo ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "o o o o o ooo\n",
      "o ooo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o oo\n",
      "olbph\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "−1 0 1 2o o o o o ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\n",
      "o o o o o o oo\n",
      "o o o o o o o o o o o o o oo\n",
      "oo\n",
      "o o o o o oo\n",
      "oo oo o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "ooo o\n",
      "oooooo o\n",
      "o oo o o o o o o o o o o o oo o oo o o o o o o o o o o o o o o o o o o oo\n",
      "o o o o o ooo\n",
      "o o o o o o o o o o o o o oo\n",
      "oo\n",
      "o o oo o oo\n",
      "oo o o o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "ooo o o o o\n",
      "o o o oo o o o o o o o o o o o oo o o oo o oo o oo o o o o oo o o ooo\n",
      "oo oo o o oo\n",
      "o o o o o o o o oo o o o oo\n",
      "oo\n",
      "oo oo o oo\n",
      "oo o o o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "ooo o o oo\n",
      "o o o o o o o o o oo o o o o o o oo o o o o o o o oo o o o o o o o o o oo\n",
      "o o o o o o oo\n",
      "o o o o o o o o o oo o o oo\n",
      "oo\n",
      "o o o o ooo\n",
      "oo o o o\n",
      "o oo\n",
      "oo oo\n",
      "o oo\n",
      "ooo o\n",
      "oo oo o o o\n",
      "o o o o o o o oo o o oo o o o oo o oo oo o o o o o o o o o oo o oo oo\n",
      "o o o oo o oo\n",
      "o o oo o ooo oo o o o oo\n",
      "oo\n",
      "o o o oo oo\n",
      "oo o o o\n",
      "ooo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o o o o o\n",
      "svi\n",
      "o o o o o o o o o o o o o o o o o o o o o oo oo o o o o oo o o o o o ooo\n",
      "o o o o o o oo\n",
      "o o o o o o oo o o o o o oo\n",
      "oo\n",
      "o o o o o oo\n",
      "oo o o o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o oo o o\n",
      "o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo\n",
      "o oo o o o oo\n",
      "o o o o o o o o o o o o o oo\n",
      "oo\n",
      "o o o o o oo\n",
      "oo oo o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o o o o o\n",
      "o o o o o o o o o o o o o o o o o o o o o o o oo o o o oo o o o o oo o oo\n",
      "o oo oo o oo\n",
      "o o o o o o o o o o o o o oo\n",
      "oo\n",
      "o o oo ooo\n",
      "oo oo o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o o o oo\n",
      "oo o ooo o o o oo oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o oo o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o oo o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o o o o o o o oo oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o o o o o oo o o oo\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"oo\n",
      "ooo\n",
      "o ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "lcp\n",
      "o o o o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "−1 0 1 2 3o o o o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo6.0 7.0 8.0 9.0ooo\n",
      "ooo o o o oo oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo o o oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "ooo o\n",
      "oooooo o\n",
      "o oo\n",
      "o o o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo ooo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo o o oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "oo o o\n",
      "ooo o o o o\n",
      "o oo\n",
      "oo o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o ooo o\n",
      "o o o oo ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo o o oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "oo o o\n",
      "ooo o o oo\n",
      "o oo\n",
      "o o o o o o oo oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o ooo o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo o o oo\n",
      "o o o o o oo o oo\n",
      "o o\n",
      "ooo o\n",
      "oo oo o o o\n",
      "o oo\n",
      "o o o o oo o o oo o o\n",
      "oo\n",
      "o o ooo\n",
      "oo\n",
      "o oo o o\n",
      "o o o oo ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "ooo o oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo o ooo\n",
      "o o ooo o o o oo\n",
      "o o\n",
      "oo o o\n",
      "oo o o o o o\n",
      "o oo\n",
      "o o o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o ooo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo oo oo\n",
      "o o o o oo o o oo\n",
      "o o\n",
      "oo o o\n",
      "oo o o o o o\n",
      "o oo\n",
      "o o o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo oo o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo oo oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "oo o o\n",
      "oo o oo o ogleason\n",
      "o oo\n",
      "o o o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo o o oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "oo o o\n",
      "oo o o o oo\n",
      "0 1 2 3 4 5ooo\n",
      "ooo o o o oo oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "2.5 3.5 4.5o oo\n",
      "oo o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o oo ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o oo oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "−1 0 1 2o oo\n",
      "o o o o oo o o oo\n",
      "o ooo\n",
      "o o ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o oo ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "−1 0 1 2 3o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "0 20 60 100\n",
      "0 20 60 100pgg45\n",
      "FIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row s hows\n",
      "the response against each of the predictors in turn. Two of th e predictors, sviand\n",
      "gleason, are categorical.\n",
      "For this problem not all errors are equal; we want to avoid ﬁlt ering out\n",
      "good email, while letting spam get through is not desirable b ut less serious\n",
      "in its consequences. We discuss a number of diﬀerent methods for tackling\n",
      "this learning problem in the book.\n",
      "Example 2: Prostate Cancer\n",
      "The data for this example, displayed in Figure 1.11, come from a study\n",
      "by Stamey et al. (1989) that examined the correlation betwee n the level of\n",
      "1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"by Stamey et al. (1989) that examined the correlation betwee n the level of\n",
      "1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had\n",
      "a value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is\n",
      "44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4 1. Introduction\n",
      "FIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\n",
      "prostate speciﬁc antigen (PSA) and a number of clinical meas ures, in 97\n",
      "men who were about to receive a radical prostatectomy.\n",
      "The goal is to predict the log of PSA ( lpsa) from a number of measure-\n",
      "ments including log cancer volume ( lcavol), log prostate weight lweight,\n",
      "age, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\n",
      "vasionsvi, log of capsular penetration lcp, Gleason score gleason, and\n",
      "percent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\n",
      "of the variables. Some correlations with lpsaare evident, but a good pre-\n",
      "dictive model is diﬃcult to construct by eye.\n",
      "This is a supervised learning problem, known as a regression problem ,\n",
      "because the outcome measurement is quantitative.\n",
      "Example 3: Handwritten Digit Recognition\n",
      "The data from this example come from the handwritten ZIP code s on\n",
      "envelopes from U.S. postal mail. Each image is a segment from a ﬁve digit\n",
      "ZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\n",
      "maps, with each pixel ranging in intensity from 0 to 255. Some sample\n",
      "images are shown in Figure 1.2.\n",
      "The images have been normalized to have approximately the sa me size\n",
      "and orientation. The task is to predict, from the 16 ×16 matrix of pixel\n",
      "intensities, the identity of each image (0 ,1,...,9) quickly and accurately. If\n",
      "it is accurate enough, the resulting algorithm would be used as part of an\n",
      "automatic sorting procedure for envelopes. This is a classi ﬁcation problem\n",
      "for which the error rate needs to be kept very low to avoid misd irection of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"1. Introduction 5\n",
      "mail. In order to achieve this low error rate, some objects ca n be assigned\n",
      "to a “don’t know” category, and sorted instead by hand.\n",
      "Example 4: DNA Expression Microarrays\n",
      "DNA stands for deoxyribonucleic acid, and is the basic mater ial that makes\n",
      "up human chromosomes. DNA microarrays measure the expressi on of a\n",
      "gene in a cell by measuring the amount of mRNA (messenger ribo nucleic\n",
      "acid) present for that gene. Microarrays are considered a br eakthrough\n",
      "technology in biology, facilitating the quantitative stud y of thousands of\n",
      "genes simultaneously from a single sample of cells.\n",
      "Here is how a DNA microarray works. The nucleotide sequences for a few\n",
      "thousand genes are printed on a glass slide. A target sample a nd a reference\n",
      "sample are labeled with red and green dyes, and each are hybri dized with\n",
      "the DNA on the slide. Through ﬂuoroscopy, the log (red/green ) intensities\n",
      "of RNA hybridizing at each site is measured. The result is a fe w thousand\n",
      "numbers, typically ranging from say −6to6, measuring the expressionlevel\n",
      "of each gene in the target relative to the reference sample. P ositive values\n",
      "indicate higher expression in the target versus the referen ce, and vice versa\n",
      "for negative values.\n",
      "A gene expression dataset collects together the expression values from a\n",
      "series of DNA microarray experiments, with each column repr esenting an\n",
      "experiment.Therearethereforeseveralthousandrowsrepr esentingindivid-\n",
      "ual genes, and tens of columns representing samples: in the p articular ex-\n",
      "ample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\n",
      "although for clarity only a random sample of 100 rows are show n. The ﬁg-\n",
      "ure displays the data set as a heat map, ranging from green (ne gative) to\n",
      "red (positive). The samples are 64 cancer tumors from diﬀere nt patients.\n",
      "The challenge here is to understand how the genes and samples are or-\n",
      "ganized. Typical questions include the following:\n",
      "(a) which samples are most similar to each other, in terms of t heir expres-\n",
      "sion proﬁles across genes?\n",
      "(b) which genes are most similar to each other, in terms of the ir expression\n",
      "proﬁles across samples?\n",
      "(c) do certain genes show very high (or low) expression for ce rtain cancer\n",
      "samples?\n",
      "We could view this task as a regression problem, with two cate gorical\n",
      "predictor variables—genes and samples—with the response var iable being\n",
      "the level of expression. However, it is probably more useful to view it as\n",
      "unsupervised learning problem. For example, for question (a) above, we\n",
      "think of the samples as points in 6830–dimensional space, wh ich we want\n",
      "toclustertogether in some way.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"6 1. Introduction\n",
      "SID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\n",
      "RENAL\n",
      "MELANOMAMELANOMA\n",
      "MCF7D-repro\n",
      "COLONCOLON\n",
      "K562B-repro\n",
      "COLON\n",
      "NSCLC\n",
      "LEUKEMIA\n",
      "RENAL\n",
      "MELANOMA\n",
      "BREAST\n",
      "CNSCNS\n",
      "RENAL\n",
      "MCF7A-repro\n",
      "NSCLC\n",
      "K562A-repro\n",
      "COLON\n",
      "CNS\n",
      "NSCLCNSCLC\n",
      "LEUKEMIA\n",
      "CNS\n",
      "OVARIAN\n",
      "BREAST\n",
      "LEUKEMIA\n",
      "MELANOMAMELANOMA\n",
      "OVARIANOVARIAN\n",
      "NSCLC\n",
      "RENAL\n",
      "BREAST\n",
      "MELANOMA\n",
      "OVARIANOVARIAN\n",
      "NSCLC\n",
      "RENAL\n",
      "BREAST\n",
      "MELANOMA\n",
      "LEUKEMIA\n",
      "COLON\n",
      "BREAST\n",
      "LEUKEMIA\n",
      "COLON\n",
      "CNS\n",
      "MELANOMA\n",
      "NSCLC\n",
      "PROSTATE\n",
      "NSCLC\n",
      "RENALRENAL\n",
      "NSCLC\n",
      "RENAL\n",
      "LEUKEMIA\n",
      "OVARIAN\n",
      "PROSTATE\n",
      "COLON\n",
      "BREAST\n",
      "RENAL\n",
      "UNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\n",
      "and64samples (columns), for the human tumor data. Only a random sample\n",
      "of100rows are shown. The display is a heat map, ranging from bright g reen\n",
      "(negative, under expressed) to bright red (positive, over e xpressed). Missing values\n",
      "are gray. The rows and columns are displayed in a randomly chosen order.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"1. Introduction 7\n",
      "Who Should Read this Book\n",
      "This book is designed for researchers and students in a broad variety of\n",
      "ﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\n",
      "expect that the reader will have had at least one elementary c ourse in\n",
      "statistics, covering basic topics including linear regres sion.\n",
      "We have not attempted to write a comprehensive catalog of lea rning\n",
      "methods, but rather to describe some of the most important te chniques.\n",
      "Equally notable, we describe the underlying concepts and co nsiderations\n",
      "by which a researcher can judge a learning method. We have tri ed to write\n",
      "this book in an intuitive fashion, emphasizing concepts rat her than math-\n",
      "ematical details.\n",
      "Asstatisticians,ourexpositionwillnaturallyreﬂectour backgroundsand\n",
      "areas of expertise. However in the past eight years we have be en attending\n",
      "conferences in neural networks, data mining and machine lea rning, and our\n",
      "thinking has been heavily inﬂuenced by these exciting ﬁelds . This inﬂuence\n",
      "is evident in our current research, and in this book.\n",
      "How This Book is Organized\n",
      "Our view is that one must understand simple methods before tr ying to\n",
      "grasp more complex ones. Hence, after giving an overview of t he supervis-\n",
      "ing learning problem in Chapter 2 , we discuss linear methods for regression\n",
      "and classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,\n",
      "wavelets and regularization/penalization methods for a si ngle predictor,\n",
      "whileChapter 6 covers kernel methods and local regression. Both of these\n",
      "sets of methods are important building blocks for high-dime nsional learn-\n",
      "ing techniques. Model assessment and selection is the topic ofChapter 7 ,\n",
      "covering the concepts of bias and variance, overﬁtting and m ethods such as\n",
      "cross-validation for choosing models. Chapter 8 discusses model inference\n",
      "and averaging, including an overview of maximum likelihood , Bayesian in-\n",
      "ference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\n",
      "A related procedure called boosting is the focus of Chapter 10 .\n",
      "InChapters 9–13 we describe a series of structured methods for su-\n",
      "pervised learning, with Chapters 9 and 11 covering regression and Chap-\n",
      "ters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\n",
      "unsupervised learning. Two recently proposed techniques, random forests\n",
      "and ensemble learning, are discussed in Chapters 15 and 16 . We describe\n",
      "undirected graphical models in Chapter 17 and ﬁnally we study high-\n",
      "dimensional problems in Chapter 18 .\n",
      "At the end of each chapter we discuss computational considerations im-\n",
      "portant for data mining applications, including how the com putations scale\n",
      "with the number of observations and predictors. Each chapte r ends with\n",
      "Bibliographic Notes giving background references for the material.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"8 1. Introduction\n",
      "We recommend that Chapters 1–4 be ﬁrst read in sequence. Chap ter 7\n",
      "should also be considered mandatory, as it covers central co ncepts that\n",
      "pertain to all learning methods. With this in mind, the rest o f the book\n",
      "can be read sequentially, or sampled, depending on the reade r’s interest.\n",
      "The symbol\n",
      " indicates a technically diﬃcult section, one that can\n",
      "be skipped without interrupting the ﬂow of the discussion.\n",
      "Book Website\n",
      "The website for this book is located at\n",
      "http://www-stat.stanford.edu/ElemStatLearn\n",
      "It contains a number of resources, including many of the data sets used in\n",
      "this book.\n",
      "Note for Instructors\n",
      "We have successively used the ﬁrst edition of this book as the basis for a\n",
      "two-quartercourse,andwiththeadditionalmaterialsinth issecondedition,\n",
      "itcouldevenbeusedforathree-quartersequence.Exercise sareprovidedat\n",
      "the end of each chapter. It is important for students to have a ccess to good\n",
      "software tools for these topics. We used the R and S-PLUS prog ramming\n",
      "languages in our courses.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 9\n",
      "Printer: Opaque this\n",
      "2\n",
      "Overview of Supervised Learning\n",
      "2.1 Introduction\n",
      "The ﬁrst three examples described in Chapter 1 have several c omponents\n",
      "in common. For each there is a set of variables that might be de noted as\n",
      "inputs, which are measured or preset. These have some inﬂuence on on e or\n",
      "moreoutputs. For each example the goal is to use the inputs to predict the\n",
      "values of the outputs. This exercise is called supervised learning .\n",
      "We have used the more modern language of machine learning. In the\n",
      "statistical literature the inputs are often called the predictors , a term we\n",
      "will use interchangeably with inputs, and more classically theindependent\n",
      "variables .Inthepatternrecognitionliteraturetheterm featuresispreferred,\n",
      "which we use as well. The outputs are called the responses , or classically\n",
      "thedependent variables .\n",
      "2.2 Variable Types and Terminology\n",
      "The outputs vary in nature among the examples. In the glucose prediction\n",
      "example, the output is a quantitative measurement, where some measure-\n",
      "ments are bigger than others, and measurements close in valu e are close\n",
      "in nature. In the famous Iris discrimination example due to R . A. Fisher,\n",
      "the output is qualitative (species of Iris) and assumes values in a ﬁnite set\n",
      "G={Virginica ,SetosaandVersicolor}. In the handwritten digit example\n",
      "the output is one of 10 diﬀerent digit classes:G={0,1,...,9}. In both of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"10 2. Overview of Supervised Learning\n",
      "these there is no explicit ordering in the classes, and in fac t often descrip-\n",
      "tive labels rather than numbers are used to denote the classe s. Qualitative\n",
      "variables are also referred to as categorical ordiscretevariables as well as\n",
      "factors.\n",
      "For both types of outputs it makes sense to think of using the i nputs to\n",
      "predict the output. Given some speciﬁc atmospheric measure ments today\n",
      "and yesterday, we want to predict the ozone level tomorrow. G iven the\n",
      "grayscale values for the pixels of the digitized image of the handwritten\n",
      "digit, we want to predict its class label.\n",
      "This distinction in output type has led to a naming conventio n for the\n",
      "prediction tasks: regression when we predict quantitative outputs, and clas-\n",
      "siﬁcation when we predict qualitative outputs. We will see that these t wo\n",
      "tasks have a lot in common, and in particular both can be viewe d as a task\n",
      "in function approximation.\n",
      "Inputs also vary in measurement type; we can have some of each of qual-\n",
      "itative and quantitative input variables. These have also l ed to distinctions\n",
      "in the types of methods that are used for prediction: some met hods are\n",
      "deﬁned most naturally for quantitative inputs, some most na turally for\n",
      "qualitative and some for both.\n",
      "A third variable type is ordered categorical , such as small, medium and\n",
      "large, where there is an ordering between the values, but no metric notion\n",
      "is appropriate (the diﬀerence between medium and small need not be the\n",
      "same as that between large and medium). These are discussed f urther in\n",
      "Chapter 4.\n",
      "Qualitative variables are typically represented numerica lly by codes. The\n",
      "easiest case is when there are only two classes or categories , such as “suc-\n",
      "cess” or “failure,” “survived” or “died.” These are often re presented by a\n",
      "single binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will\n",
      "become apparent, such numeric codes are sometimes referred to astargets.\n",
      "When there are more than two categories, several alternativ es are available.\n",
      "The most useful and commonly used coding is via dummy variables . Here a\n",
      "K-level qualitative variable is represented by a vector of Kbinary variables\n",
      "or bits, only one of which is “on” at a time. Although more comp act coding\n",
      "schemes are possible, dummy variables are symmetric in the l evels of the\n",
      "factor.\n",
      "We will typically denote an input variable by the symbol X. IfXis\n",
      "a vector, its components can be accessed by subscripts Xj. Quantitative\n",
      "outputs will be denoted by Y, and qualitative outputs by G(for group).\n",
      "We use uppercase letters such as X,YorGwhen referring to the generic\n",
      "aspects of a variable. Observed values are written in lowerc ase; hence the\n",
      "ith observed value of Xis written as xi(wherexiis again a scalar or\n",
      "vector). Matrices are represented by bold uppercase letter s; for example, a\n",
      "set ofNinputp-vectorsxi, i= 1,...,Nwould be represented by the N×p\n",
      "matrixX. In general, vectors will not be bold, except when they have N\n",
      "components; this convention distinguishes a p-vector of inputs xifor the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.3 Least Squares and Nearest Neighbors 11\n",
      "ith observation from the N-vectorxjconsisting of all the observations on\n",
      "variableXj. Since all vectors are assumed to be column vectors, the ith\n",
      "row ofXisxT\n",
      "i, the vector transpose of xi.\n",
      "For the moment we can loosely state the learning task as follo ws: given\n",
      "the value of an input vector X, make a good prediction of the output Y,\n",
      "denoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\n",
      "ˆY; likewise for categorical outputs, ˆGshould take values in the same set G\n",
      "associated with G.\n",
      "For a two-class G, one approach is to denote the binary coded target\n",
      "asY, and then treat it as a quantitative output. The predictions ˆYwill\n",
      "typically lie in [0 ,1], and we can assign to ˆGthe class label according to\n",
      "whether ˆy >0.5. This approach generalizes to K-level qualitative outputs\n",
      "as well.\n",
      "We need data to construct prediction rules, often a lot of it. We thus\n",
      "suppose we have available a set of measurements ( xi,yi) or (xi,gi), i=\n",
      "1,...,N,knownasthe training data ,withwhichtoconstructourprediction\n",
      "rule.\n",
      "2.3 Two Simple Approaches to Prediction: Least\n",
      "Squares and Nearest Neighbors\n",
      "In this section we develop two simple but powerful predictio n methods: the\n",
      "linear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\n",
      "Thelinearmodelmakeshugeassumptionsaboutstructureand yieldsstable\n",
      "but possibly inaccurate predictions. The method of k-nearest neighbors\n",
      "makes very mild structural assumptions: its predictions ar e often accurate\n",
      "but can be unstable.\n",
      "2.3.1 Linear Models and Least Squares\n",
      "The linear model has been a mainstay of statistics for the pas t 30 years\n",
      "and remains one of our most important tools. Given a vector of inputs\n",
      "XT= (X1,X2,...,X p), we predict the output Yvia the model\n",
      "ˆY=ˆβ0+p∑\n",
      "j=1Xjˆβj. (2.1)\n",
      "The term ˆβ0is the intercept, also known as the biasin machine learning.\n",
      "Often it is convenient to include the constant variable 1 in X, include ˆβ0in\n",
      "the vector of coeﬃcients ˆβ, and then write the linear model in vector form\n",
      "as an inner product\n",
      "ˆY=XTˆβ, (2.2)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"12 2. Overview of Supervised Learning\n",
      "whereXTdenotes vector or matrix transpose ( Xbeing a column vector).\n",
      "Here we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\n",
      "aK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\n",
      "(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\n",
      "If the constant is included in X, then the hyperplane includes the origin\n",
      "and is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point\n",
      "(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\n",
      "Viewed as a function over the p-dimensional input space, f(X) =XTβ\n",
      "is linear, and the gradient f′(X) =βis a vector in input space that points\n",
      "in the steepest uphill direction.\n",
      "How do we ﬁt the linear model to a set of training data? There ar e\n",
      "many diﬀerent methods, but by far the most popular is the meth od of\n",
      "least squares . In this approach, we pick the coeﬃcients βto minimize the\n",
      "residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−xT\n",
      "iβ)2. (2.3)\n",
      "RSS(β) is a quadratic function of the parameters, and hence its min imum\n",
      "always exists, but may not be unique. The solution is easiest to characterize\n",
      "in matrix notation. We can write\n",
      "RSS(β) = (y−Xβ)T(y−Xβ), (2.4)\n",
      "whereXis anN×pmatrix with each row an input vector, and yis an\n",
      "N-vector of the outputs in the training set. Diﬀerentiating w .r.t.βwe get\n",
      "thenormal equations\n",
      "XT(y−Xβ) = 0. (2.5)\n",
      "IfXTXis nonsingular, then the unique solution is given by\n",
      "ˆβ= (XTX)−1XTy, (2.6)\n",
      "and the ﬁtted value at the ith inputxiis ˆyi= ˆy(xi) =xT\n",
      "iˆβ. At an arbi-\n",
      "trary input x0the prediction is ˆ y(x0) =xT\n",
      "0ˆβ. The entire ﬁtted surface is\n",
      "characterized by the pparameters ˆβ. Intuitively, it seems that we do not\n",
      "need a very large data set to ﬁt such a model.\n",
      "Let’s look at an example of the linear model in a classiﬁcatio n context.\n",
      "Figure 2.1 shows a scatterplot of training data on a pair of in putsX1and\n",
      "X2. The data are simulated, and for the present the simulation m odel is\n",
      "not important. The output class variable Ghas the values BLUEorORANGE,\n",
      "and is represented as such in the scatterplot. There are 100 p oints in each\n",
      "of the two classes. The linear regression model was ﬁt to thes e data, with\n",
      "the response Ycoded as 0 for BLUEand 1 for ORANGE. The ﬁtted values ˆY\n",
      "are converted to a ﬁtted class variable ˆGaccording to the rule\n",
      "ˆG={\n",
      "ORANGE ifˆY >0.5,\n",
      "BLUE ifˆY≤0.5.(2.7)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.3 Least Squares and Nearest Neighbors 13\n",
      "Linear Regression of 0/1 Response\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\".. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.1. A classiﬁcation example in two dimensions. The classes are code d\n",
      "as a binary variable ( BLUE= 0,ORANGE= 1), and then ﬁt by linear regression.\n",
      "The line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\n",
      "denotes that part of input space classiﬁed as ORANGE, while the blue region is\n",
      "classiﬁed as BLUE.\n",
      "The set of points in IR2classiﬁed as ORANGEcorresponds to{x:xTˆβ >0.5},\n",
      "indicated in Figure 2.1, and the two predicted classes are se parated by the\n",
      "decision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\n",
      "that for these data there are several misclassiﬁcations on b oth sides of the\n",
      "decisionboundary.Perhapsourlinearmodelistoorigid—or aresucherrors\n",
      "unavoidable? Remember that these are errors on the training data itself,\n",
      "and we have not said where the constructed data came from. Con sider the\n",
      "two possible scenarios:\n",
      "Scenario 1: The training data in each class were generated from bivariat e\n",
      "Gaussian distributions with uncorrelated components and d iﬀerent\n",
      "means.\n",
      "Scenario 2: Thetrainingdataineachclasscamefromamixtureof10low-\n",
      "variance Gaussian distributions, with individual means th emselves\n",
      "distributed as Gaussian.\n",
      "A mixture of Gaussians is best described in terms of the gener ative\n",
      "model. One ﬁrst generates a discrete variable that determin es which of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"14 2. Overview of Supervised Learning\n",
      "the component Gaussians to use, and then generates an observ ation from\n",
      "the chosen density. In the case of one Gaussian per class, we w ill see in\n",
      "Chapter 4 that a linear decision boundary is the best one can d o, and that\n",
      "our estimate is almost optimal. The region of overlap is inev itable, and\n",
      "future data to be predicted will be plagued by this overlap as well.\n",
      "In the case of mixtures of tightly clustered Gaussians the st ory is dif-\n",
      "ferent. A linear decision boundary is unlikely to be optimal , and in fact is\n",
      "not. The optimal decision boundary is nonlinear and disjoin t, and as such\n",
      "will be much more diﬃcult to obtain.\n",
      "We now look at another classiﬁcation and regression procedu re that is\n",
      "in some sense at the opposite end of the spectrum to the linear model, and\n",
      "far better suited to the second scenario.\n",
      "2.3.2 Nearest-Neighbor Methods\n",
      "Nearest-neighbor methods use those observations in the tra ining setTclos-\n",
      "est in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\n",
      "forˆYis deﬁned as follows:\n",
      "ˆY(x) =1\n",
      "k∑\n",
      "xi∈Nk(x)yi, (2.8)\n",
      "whereNk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\n",
      "the training sample. Closeness implies a metric, which for t he moment we\n",
      "assume is Euclidean distance. So, in words, we ﬁnd the kobservations with\n",
      "xiclosest toxin input space, and average their responses.\n",
      "In Figure 2.2 we use the same training data as in Figure 2.1, an d use\n",
      "15-nearest-neighbor averaging of the binary coded respons e as the method\n",
      "of ﬁtting. Thus ˆYis the proportion of ORANGE’s in the neighborhood, and\n",
      "so assigning class ORANGEtoˆGifˆY >0.5 amounts to a majority vote in\n",
      "the neighborhood. The colored regions indicate all those po ints in input\n",
      "space classiﬁed as BLUEorORANGEby such a rule, in this case found by\n",
      "evaluating the procedure on a ﬁne grid in input space. We see t hat the\n",
      "decision boundaries that separate the BLUEfrom the ORANGEregions are far\n",
      "more irregular, and respond to local clusters where one clas s dominates.\n",
      "Figure 2.3 shows the results for 1-nearest-neighbor classi ﬁcation: ˆYis\n",
      "assigned the value yℓof the closest point xℓtoxin the training data. In\n",
      "this case the regions of classiﬁcation can be computed relat ively easily, and\n",
      "correspond to a Voronoi tessellation of the training data. Each point xi\n",
      "has an associated tile bounding the region for which it is the closest input\n",
      "point. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\n",
      "more irregular than before.\n",
      "The method of k-nearest-neighbor averaging is deﬁned in exactly the\n",
      "same way for regression of a quantitative output Y, althoughk= 1 would\n",
      "be an unlikely choice.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.3 Least Squares and Nearest Neighbors 15\n",
      "15-Nearest Neighbor Classifier\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\".. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\n",
      "ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1)and\n",
      "then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted clas s is hence\n",
      "chosen by majority vote amongst the 15-nearest neighbors.\n",
      "In Figure 2.2 we see that far fewer training observations are misclassiﬁed\n",
      "than in Figure 2.1. This should not give us too much comfort, t hough, since\n",
      "in Figure 2.3 noneof the training data are misclassiﬁed. A little thought\n",
      "suggests that for k-nearest-neighbor ﬁts, the error on the training data\n",
      "should be approximately an increasing function of k, and will always be 0\n",
      "fork= 1. An independent test set would give us a more satisfactory means\n",
      "for comparing the diﬀerent methods.\n",
      "It appears that k-nearest-neighbor ﬁts have a single parameter, the num-\n",
      "ber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\n",
      "though this is the case, we will see that the eﬀective number of parameters\n",
      "ofk-nearest neighbors is N/kand is generally bigger than p, and decreases\n",
      "with increasing k. To get an idea of why, note that if the neighborhoods\n",
      "were nonoverlapping, there would be N/kneighborhoods and we would ﬁt\n",
      "one parameter (a mean) in each neighborhood.\n",
      "It is also clear that we cannot use sum-of-squared errors on t he training\n",
      "set as a criterion for picking k, since we would always pick k= 1! It would\n",
      "seem thatk-nearest-neighbor methods would be more appropriate for th e\n",
      "mixture Scenario 2 described above, while for Gaussian data the decision\n",
      "boundaries of k-nearest neighbors would be unnecessarily noisy.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"16 2. Overview of Supervised Learning\n",
      "1−Nearest Neighbor Classifier\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\n",
      "ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1), and\n",
      "then predicted by 1-nearest-neighbor classiﬁcation.\n",
      "2.3.3 From Least Squares to Nearest Neighbors\n",
      "The linear decision boundary from least squares is very smoo th, and ap-\n",
      "parently stable to ﬁt. It does appear to rely heavily on the as sumption\n",
      "that a linear decision boundary is appropriate. In language we will develop\n",
      "shortly, it has low variance and potentially high bias.\n",
      "On the other hand, the k-nearest-neighbor procedures do not appear to\n",
      "relyonanystringentassumptionsabouttheunderlyingdata ,andcanadapt\n",
      "to any situation. However, any particular subregion of the d ecision bound-\n",
      "ary depends on a handful of input points and their particular positions,\n",
      "and is thus wiggly and unstable—high variance and low bias.\n",
      "Each method has its own situations for which it works best; in particular\n",
      "linear regression is more appropriate for Scenario 1 above, while nearest\n",
      "neighbors are more suitable for Scenario 2. The time has come to expose\n",
      "the oracle! The data in fact were simulated from a model somew here be-\n",
      "tween the two, but closer to Scenario 2. First we generated 10 meansmk\n",
      "from a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\n",
      "BLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\n",
      "ORANGE. Then for each class we generated 100 observations as follow s: for\n",
      "each observation, we picked an mkat random with probability 1 /10, and\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.3 Least Squares and Nearest Neighbors 17\n",
      "Degrees of Freedom − N/kTest Error\n",
      "0.10 0.15 0.20 0.25 0.30\n",
      "  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\n",
      "Train\n",
      "Test\n",
      "Bayesk −  Number of Nearest Neighbors\n",
      "Linear\n",
      "FIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fig -\n",
      "ures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\n",
      "sample of size 10,000. The orange curves are test and the blue are training er-\n",
      "ror fork-nearest-neighbor classiﬁcation. The results for linear regr ession are the\n",
      "bigger orange and blue squares at three degrees of freedom. Th e purple line is the\n",
      "optimal Bayes error rate.\n",
      "then generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\n",
      "ters for each class. Figure 2.4 shows the results of classify ing 10,000 new\n",
      "observations generated from the model. We compare the resul ts for least\n",
      "squares and those for k-nearest neighbors for a range of values of k.\n",
      "A large subset of the most popular techniques in use today are variants of\n",
      "these two simple procedures. In fact 1-nearest-neighbor, t he simplest of all,\n",
      "captures a large percentage of the market for low-dimension al problems.\n",
      "The following list describes some ways in which these simple procedures\n",
      "have been enhanced:\n",
      "•Kernel methods use weights that decrease smoothly to zero wi th dis-\n",
      "tancefromthetargetpoint,ratherthantheeﬀective0 /1weightsused\n",
      "byk-nearest neighbors.\n",
      "•In high-dimensional spaces the distance kernels are modiﬁe d to em-\n",
      "phasize some variable more than others.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"18 2. Overview of Supervised Learning\n",
      "•Local regression ﬁts linear models by locally weighted leas t squares,\n",
      "rather than ﬁtting constants locally.\n",
      "•Linear models ﬁt to a basis expansion of the original inputs a llow\n",
      "arbitrarily complex models.\n",
      "•Projection pursuit and neural network models consist of sum s of non-\n",
      "linearly transformed linear models.\n",
      "2.4 Statistical Decision Theory\n",
      "In this section we develop a small amount of theory that provi des a frame-\n",
      "work for developing models such as those discussed informal ly so far. We\n",
      "ﬁrst consider the case of a quantitative output, and place ou rselves in the\n",
      "world of random variables and probability spaces. Let X∈IRpdenote a\n",
      "real valued random input vector, and Y∈IR a real valued random out-\n",
      "put variable, with joint distribution Pr( X,Y). We seek a function f(X)\n",
      "for predicting Ygiven values of the input X. This theory requires a loss\n",
      "functionL(Y,f(X)) for penalizing errors in prediction, and by far the most\n",
      "common and convenient is squared error loss :L(Y,f(X)) = (Y−f(X))2.\n",
      "This leads us to a criterion for choosing f,\n",
      "EPE(f) = E(Y−f(X))2(2.9)\n",
      "=∫\n",
      "[y−f(x)]2Pr(dx,dy), (2.10)\n",
      "the expected (squared) prediction error . By conditioning1onX, we can\n",
      "write EPE as\n",
      "EPE(f) = EXEY|X(\n",
      "[Y−f(X)]2|X)\n",
      "(2.11)\n",
      "and we see that it suﬃces to minimize EPE pointwise:\n",
      "f(x) = argmincEY|X(\n",
      "[Y−c]2|X=x)\n",
      ". (2.12)\n",
      "The solution is\n",
      "f(x) = E(Y|X=x), (2.13)\n",
      "the conditional expectation, also known as the regression function. Thus\n",
      "the best prediction of Yat any point X=xis the conditional mean, when\n",
      "best is measured by average squared error.\n",
      "The nearest-neighbor methods attempt to directly implemen t this recipe\n",
      "using the training data. At each point x, we might ask for the average of all\n",
      "1Conditioning here amounts to factoring the joint density Pr( X,Y) = Pr(Y|X)Pr(X)\n",
      "where Pr( Y|X) = Pr(Y,X)/Pr(X), and splitting up the bivariate integral accordingly.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.4 Statistical Decision Theory 19\n",
      "thoseyis with input xi=x. Since there is typically at most one observation\n",
      "at any point x, we settle for\n",
      "ˆf(x) = Ave(yi|xi∈Nk(x)), (2.14)\n",
      "where “Ave” denotes average, and Nk(x) is the neighborhood containing\n",
      "thekpoints inTclosest tox. Two approximations are happening here:\n",
      "•expectation is approximated by averaging over sample data;\n",
      "•conditioning at a point is relaxed to conditioning on some re gion\n",
      "“close” to the target point.\n",
      "For large training sample size N, the points in the neighborhood are likely\n",
      "to be close to x, and askgets large the average will get more stable.\n",
      "In fact, under mild regularity conditions on the joint proba bility distri-\n",
      "bution Pr(X,Y), one can show that as N,k→∞such thatk/N→0,\n",
      "ˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\n",
      "we have a universal approximator? We often do not have very la rge sam-\n",
      "ples. If the linear or some more structured model is appropri ate, then we\n",
      "can usually get a more stable estimate than k-nearest neighbors, although\n",
      "such knowledge has to be learned from the data as well. There a re other\n",
      "problems though, sometimes disastrous. In Section 2.5 we se e that as the\n",
      "dimensionpgets large, so does the metric size of the k-nearest neighbor-\n",
      "hood. So settling for nearest neighborhood as a surrogate fo r conditioning\n",
      "will fail us miserably. The convergence above still holds, b ut therateof\n",
      "convergence decreases as the dimension increases.\n",
      "How does linear regressionﬁt intothis framework? Thesimpl est explana-\n",
      "tion is that one assumes that the regression function f(x) is approximately\n",
      "linear in its arguments:\n",
      "f(x)≈xTβ. (2.15)\n",
      "Thisisamodel-basedapproach—wespecifyamodelfortheregr essionfunc-\n",
      "tion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\n",
      "we can solve for βtheoretically:\n",
      "β= [E(XXT)]−1E(XY). (2.16)\n",
      "Note we have notconditioned on X; rather we have used our knowledge\n",
      "of the functional relationship to poolover values of X. The least squares\n",
      "solution (2.6) amounts to replacing the expectation in (2.1 6) by averages\n",
      "over the training data.\n",
      "So bothk-nearest neighbors and least squares end up approximating\n",
      "conditional expectations by averages. But they diﬀer drama tically in terms\n",
      "of model assumptions:\n",
      "•Least squares assumes f(x) is well approximated by a globally linear\n",
      "function.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"20 2. Overview of Supervised Learning\n",
      "•k-nearest neighbors assumes f(x) is well approximated by a locally\n",
      "constant function.\n",
      "Although the latter seems more palatable, we have already se en that we\n",
      "may pay a price for this ﬂexibility.\n",
      "Many of the more modern techniques described in this book are model\n",
      "based, although far more ﬂexible than the rigid linear model . For example,\n",
      "additive models assume that\n",
      "f(X) =p∑\n",
      "j=1fj(Xj). (2.17)\n",
      "Thisretainstheadditivity ofthelinearmodel,buteachcoo rdinatefunction\n",
      "fjis arbitrary. It turns out that the optimal estimate for the a dditive model\n",
      "uses techniques such as k-nearest neighbors to approximate univariate con-\n",
      "ditional expectations simultaneously for each of the coordinate functions.\n",
      "Thus the problems of estimating a conditional expectation i n high dimen-\n",
      "sionsaresweptawayinthiscasebyimposingsome(oftenunre alistic)model\n",
      "assumptions, in this case additivity.\n",
      "Are we happy with the criterion (2.11)? What happens if we rep lace the\n",
      "L2loss function with the L1: E|Y−f(X)|? The solution in this case is the\n",
      "conditional median,\n",
      "ˆf(x) = median( Y|X=x), (2.18)\n",
      "which is a diﬀerent measure of location, and its estimates ar e more robust\n",
      "than those for the conditional mean. L1criteria have discontinuities in\n",
      "their derivatives, which have hindered their widespread us e. Other more\n",
      "resistant loss functions will be mentioned in later chapter s, but squared\n",
      "error is analytically convenient and the most popular.\n",
      "What do we do when the output is a categorical variable G? The same\n",
      "paradigm works here, except we need a diﬀerent loss function for penalizing\n",
      "prediction errors. An estimate ˆGwill assume values in G, the set of possible\n",
      "classes. Our loss function can be represented by a K×KmatrixL, where\n",
      "K= card(G).Lwill be zero on the diagonal and nonnegative elsewhere,\n",
      "whereL(k,ℓ) is the price paid for classifying an observation belonging to\n",
      "classGkasGℓ. Most often we use the zero–one loss function, where all\n",
      "misclassiﬁcations are charged a single unit. The expected p rediction error\n",
      "is\n",
      "EPE = E[L(G,ˆG(X))], (2.19)\n",
      "where again the expectation is taken with respect to the join t distribution\n",
      "Pr(G,X). Again we condition, and can write EPE as\n",
      "EPE = E XK∑\n",
      "k=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.4 Statistical Decision Theory 21\n",
      "Bayes Optimal Classifier\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\". . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.5. The optimal Bayes decision boundary for the simulation examp le\n",
      "of Figures 2.1, 2.2 and 2.3. Since the generating density is k nown for each class,\n",
      "this boundary can be calculated exactly (Exercise 2.2).\n",
      "and again it suﬃces to minimize EPE pointwise:\n",
      "ˆG(x) = argming∈GK∑\n",
      "k=1L(Gk,g)Pr(Gk|X=x). (2.21)\n",
      "With the 0–1 loss function this simpliﬁes to\n",
      "ˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\n",
      "or simply\n",
      "ˆG(x) =Gkif Pr(Gk|X=x) = max\n",
      "g∈GPr(g|X=x).(2.23)\n",
      "This reasonable solution is known as the Bayes classiﬁer , and says that\n",
      "we classify to the most probable class, using the conditiona l (discrete) dis-\n",
      "tribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\n",
      "for our simulation example. The error rate of the Bayes class iﬁer is called\n",
      "theBayes rate .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"22 2. Overview of Supervised Learning\n",
      "Again we see that the k-nearest neighbor classiﬁer directly approximates\n",
      "this solution—a majority vote in a nearest neighborhood amou nts to ex-\n",
      "actly this, except that conditional probability at a point i s relaxed to con-\n",
      "ditional probability within a neighborhood of a point, and p robabilities are\n",
      "estimated by training-sample proportions.\n",
      "Suppose for a two-class problem we had taken the dummy-varia ble ap-\n",
      "proach and coded Gvia a binary Y, followed by squared error loss estima-\n",
      "tion. Then ˆf(X) = E(Y|X) = Pr(G=G1|X) ifG1corresponded to Y= 1.\n",
      "Likewise for a K-class problem, E( Yk|X) = Pr(G=Gk|X). This shows\n",
      "that our dummy-variable regression procedure, followed by classiﬁcation to\n",
      "the largest ﬁtted value, is another way of representing the B ayes classiﬁer.\n",
      "Although this theory is exact, in practice problems can occu r, depending\n",
      "on the regression model used. For example, when linear regre ssion is used,\n",
      "ˆf(X) need not be positive, and we might be suspicious about using it as\n",
      "an estimate of a probability. We will discuss a variety of app roaches to\n",
      "modeling Pr( G|X) in Chapter 4.\n",
      "2.5 Local Methods in High Dimensions\n",
      "We have examined two learning techniques for prediction so f ar: the stable\n",
      "but biased linear model and the less stable but apparently le ss biased class\n",
      "ofk-nearest-neighbor estimates. It would seem that with a reas onably large\n",
      "set of training data, we could always approximate the theore tically optimal\n",
      "conditional expectation by k-nearest-neighbor averaging, since we should\n",
      "be able to ﬁnd a fairly large neighborhood of observations cl ose to any x\n",
      "and average them. This approach and our intuition breaks dow n in high\n",
      "dimensions, and the phenomenon is commonly referred to as th ecurse\n",
      "of dimensionality (Bellman, 1961). There are many manifestations of this\n",
      "problem, and we will examine a few here.\n",
      "Considerthenearest-neighborprocedureforinputsunifor mlydistributed\n",
      "in ap-dimensional unit hypercube, as in Figure 2.6. Suppose we se nd out a\n",
      "hypercubical neighborhood about a target point to capture a fractionrof\n",
      "the observations. Since this corresponds to a fraction rof the unit volume,\n",
      "theexpectededgelengthwillbe ep(r) =r1/p.Intendimensions e10(0.01) =\n",
      "0.63 ande10(0.1) = 0.80, while the entire range for each input is only 1 .0.\n",
      "So to capture 1% or 10% of the data to form a local average, we mu st cover\n",
      "63% or 80% of the range of each input variable. Such neighborh oods are no\n",
      "longer “local.” Reducing rdramatically does not help much either, since\n",
      "the fewer observations we average, the higher is the varianc e of our ﬁt.\n",
      "Another consequence of the sparse sampling in high dimensio ns is that\n",
      "all sample points are close to an edge of the sample. Consider Ndata points\n",
      "uniformly distributed in a p-dimensional unit ball centered at the origin.\n",
      "Suppose we consider a nearest-neighbor estimate at the orig in. The median\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.5 Local Methods in High Dimensions 23\n",
      "1\n",
      "10Unit Cube\n",
      "Fraction of VolumeDistance\n",
      "0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\n",
      "Neighborhood\n",
      "FIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\n",
      "neighborhood for uniform data in a unit cube. The ﬁgure on the right shows the\n",
      "side-length of the subcube needed to capture a fraction rof the volume of the data,\n",
      "for diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range\n",
      "of each coordinate to capture 10%of the data.\n",
      "distance from the origin to the closest data point is given by the expression\n",
      "d(p,N) =(\n",
      "1−1\n",
      "21/N)1/p\n",
      "(2.24)\n",
      "(Exercise 2.3). A more complicated expression exists for th e mean distance\n",
      "to the closest point. For N= 500,p= 10 ,d(p,N)≈0.52, more than\n",
      "halfwaytotheboundary.Hencemostdatapointsarecloserto theboundary\n",
      "of the sample space than to any other data point. The reason th at this\n",
      "presents a problem is that prediction is much more diﬃcult ne ar the edges\n",
      "of the training sample. One must extrapolate from neighbori ng sample\n",
      "points rather than interpolate between them.\n",
      "Another manifestation of the curse is that the sampling dens ity is pro-\n",
      "portional to N1/p, wherepis the dimension of the input space and Nis the\n",
      "sample size. Thus, if N1= 100 represents a dense sample for a single input\n",
      "problem, then N10= 10010is the sample size required for the same sam-\n",
      "pling density with 10 inputs. Thus in high dimensions all fea sible training\n",
      "samples sparsely populate the input space.\n",
      "Let us construct another uniform example. Suppose we have 10 00 train-\n",
      "ing examples xigenerated uniformly on [ −1,1]p. Assume that the true\n",
      "relationship between XandYis\n",
      "Y=f(X) =e−8||X||2,\n",
      "without any measurement error. We use the 1-nearest-neighb or rule to\n",
      "predicty0at the test-point x0= 0. Denote the training set by T. We can\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"24 2. Overview of Supervised Learning\n",
      "compute the expected prediction error at x0for our procedure, averaging\n",
      "over all such samples of size 1000. Since the problem is deter ministic, this\n",
      "is the mean squared error (MSE) for estimating f(0):\n",
      "MSE(x0) = E T[f(x0)−ˆy0]2\n",
      "= ET[ˆy0−ET(ˆy0)]2+[ET(ˆy0)−f(x0)]2\n",
      "= Var T(ˆy0)+Bias2(ˆy0). (2.25)\n",
      "Figure 2.7 illustrates the setup. We have broken down the MSE into two\n",
      "components that will become familiar as we proceed: varianc e and squared\n",
      "bias.Suchadecompositionisalwayspossibleandoftenusef ul,andisknown\n",
      "as thebias–variance decomposition . Unless the nearest neighbor is at 0,\n",
      "ˆy0will be smaller than f(0) in this example, and so the average estimate\n",
      "will be biased downward. The variance is due to the sampling v ariance of\n",
      "the 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\n",
      "neighbor is very close to 0, and so both the bias and variance a re small. As\n",
      "the dimension increases, the nearest neighbor tends to stra y further from\n",
      "the target point, and both bias and variance are incurred. By p= 10, for\n",
      "more than 99% of the samples the nearest neighbor is a distanc e greater\n",
      "than 0.5 from the origin. Thus as pincreases, the estimate tends to be 0\n",
      "more often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\n",
      "and the variance starts dropping (an artifact of this exampl e).\n",
      "Although this is a highly contrived example, similar phenom ena occur\n",
      "more generally. The complexity of functions of many variabl es can grow\n",
      "exponentially with the dimension, and if we wish to be able to estimate\n",
      "such functions with the same accuracy as function in low dime nsions, then\n",
      "we need the size of our training set to grow exponentially as w ell. In this\n",
      "example, the function is a complex interaction of all pvariables involved.\n",
      "The dependence of the bias term on distance depends on the tru th, and\n",
      "it need not always dominate with 1-nearest neighbor. For exa mple, if the\n",
      "function always involves only a few dimensions as in Figure 2 .8, then the\n",
      "variance can dominate instead.\n",
      "Suppose, on the other hand, that we know that the relationshi p between\n",
      "YandXis linear,\n",
      "Y=XTβ+ε, (2.26)\n",
      "whereε∼N(0,σ2) and we ﬁt the model by least squares to the train-\n",
      "ing data. For an arbitrary test point x0, we have ˆy0=xT\n",
      "0ˆβ, which can\n",
      "be written as ˆ y0=xT\n",
      "0β+∑N\n",
      "i=1ℓi(x0)εi, whereℓi(x0) is theith element\n",
      "ofX(XTX)−1x0. Since under this model the least squares estimates are\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.5 Local Methods in High Dimensions 25\n",
      "Xf(X)\n",
      "-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\n",
      "X1X2\n",
      "-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••1-NN in One vs. Two Dimensions\n",
      "DimensionAverage Distance to Nearest Neighbor\n",
      "2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\n",
      "DimensionMse\n",
      "2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\n",
      "• •••••••• • • •••••••••MSE vs. Dimension\n",
      "•MSE\n",
      "•Variance\n",
      "•Sq. Bias\n",
      "FIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\n",
      "ity and its eﬀect on MSE, bias and variance. The input feature s are uniformly\n",
      "distributed in [−1,1]pforp= 1,...,10The top left panel shows the target func-\n",
      "tion (no noise) in IR:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\n",
      "neighbor makes in estimating f(0). The training point is indicated by the blue tick\n",
      "mark. The top right panel illustrates why the radius of the 1-nearest neighborhood\n",
      "increases with dimension p. The lower left panel shows the average radius of the\n",
      "1-nearest neighborhoods. The lower-right panel shows the MSE , squared bias and\n",
      "variance curves as a function of dimension p.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"26 2. Overview of Supervised Learning\n",
      "Xf(X)\n",
      "-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\n",
      "DimensionMSE\n",
      "2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\n",
      "••••••••••\n",
      "• •• • ••••••MSE  vs. Dimension\n",
      "•MSE\n",
      "•Variance\n",
      "•Sq. Bias\n",
      "FIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\n",
      "the function is constant in all but one dimension: f(X) =1\n",
      "2(X1+ 1)3. The\n",
      "variance dominates.\n",
      "unbiased, we ﬁnd that\n",
      "EPE(x0) = E y0|x0ET(y0−ˆy0)2\n",
      "= Var(y0|x0)+ET[ˆy0−ETˆy0]2+[ETˆy0−xT\n",
      "0β]2\n",
      "= Var(y0|x0)+Var T(ˆy0)+Bias2(ˆy0)\n",
      "=σ2+ETxT\n",
      "0(XTX)−1x0σ2+02. (2.27)\n",
      "Here we have incurred an additional variance σ2in the prediction error,\n",
      "since our target is not deterministic. There is no bias, and t he variance\n",
      "depends on x0. IfNis large andTwere selected at random, and assuming\n",
      "E(X) = 0, then XTX→NCov(X) and\n",
      "Ex0EPE(x0)∼Ex0xT\n",
      "0Cov(X)−1x0σ2/N+σ2\n",
      "= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\n",
      "=σ2(p/N)+σ2. (2.28)\n",
      "Here we see that the expected EPE increases linearly as a func tion ofp,\n",
      "with slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\n",
      "ance is negligible (0 in the deterministic case). By imposin g some heavy\n",
      "restrictions on the class of models being ﬁtted, we have avoi ded the curse\n",
      "of dimensionality. Some of the technical details in (2.27) a nd (2.28) are\n",
      "derived in Exercise 2.5.\n",
      "Figure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\n",
      "tions, both of which have the form Y=f(X) +ε,Xuniform as before,\n",
      "andε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.5 Local Methods in High Dimensions 27\n",
      "DimensionEPE Ratio\n",
      "2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\n",
      "•Linear\n",
      "•Cubic\n",
      "FIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\n",
      "1-nearest neighbor relative to least squares for the model Y=f(X)+ε. For the\n",
      "orange curve, f(x) =x1, while for the blue curve f(x) =1\n",
      "2(x1+1)3.\n",
      "is linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\n",
      "Shown is the relative EPE of 1-nearest neighbor to least squa res, which\n",
      "appears to start at around 2 for the linear case. Least square s is unbiased\n",
      "in this case, and as discussed above the EPE is slightly above σ2= 1.\n",
      "The EPE for 1-nearest neighbor is always above 2, since the va riance of\n",
      "ˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\n",
      "the nearest neighbor strays from the target point. For the cu bic case, least\n",
      "squares is biased, which moderates the ratio. Clearly we cou ld manufacture\n",
      "examples where the bias of least squares would dominate the v ariance, and\n",
      "the 1-nearest neighbor would come out the winner.\n",
      "By relying on rigid assumptions, the linear model has no bias at all and\n",
      "negligible variance, while the error in 1-nearest neighbor is substantially\n",
      "larger. However, if the assumptions are wrong, all bets are o ﬀ and the\n",
      "1-nearest neighbor may dominate. We will see that there is a w hole spec-\n",
      "trum of models between the rigid linear models and the extrem ely ﬂexible\n",
      "1-nearest-neighbor models, each with their own assumption s and biases,\n",
      "which have been proposed speciﬁcally to avoid the exponenti al growth in\n",
      "complexity of functions in high dimensions by drawing heavi ly on these\n",
      "assumptions.\n",
      "Before we delve more deeply, let us elaborate a bit on the conc ept of\n",
      "statistical models and see how they ﬁt into the prediction framework.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"28 2. Overview of Supervised Learning\n",
      "2.6 Statistical Models, Supervised Learning and\n",
      "Function Approximation\n",
      "Our goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\n",
      "underlies the predictive relationship between the inputs a nd outputs. In the\n",
      "theoretical setting of Section 2.4, we saw that squared erro r loss lead us\n",
      "to the regression function f(x) = E(Y|X=x) for a quantitative response.\n",
      "The class of nearest-neighbor methods can be viewed as direc t estimates\n",
      "of this conditional expectation, but we have seen that they c an fail in at\n",
      "least two ways:\n",
      "•if the dimension of the input space is high, the nearest neigh bors need\n",
      "not be close to the target point, and can result in large error s;\n",
      "•if special structure is known to exist, this can be used to red uce both\n",
      "the bias and the variance of the estimates.\n",
      "We anticipate using other classes of models for f(x), in many cases specif-\n",
      "ically designed to overcome the dimensionality problems, a nd here we dis-\n",
      "cuss a framework for incorporating them into the prediction problem.\n",
      "2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\n",
      "Suppose in fact that our data arose from a statistical model\n",
      "Y=f(X)+ε, (2.29)\n",
      "where the random error εhas E(ε) = 0 and is independent of X. Note that\n",
      "for this model, f(x) = E(Y|X=x), and in fact the conditional distribution\n",
      "Pr(Y|X) depends on Xonlythrough the conditional mean f(x).\n",
      "The additive error model is a useful approximation to the tru th. For\n",
      "most systems the input–output pairs ( X,Y) will not have a deterministic\n",
      "relationship Y=f(X). Generally there will be other unmeasured variables\n",
      "thatalsocontributeto Y,includingmeasurementerror.Theadditivemodel\n",
      "assumes that we can capture all these departures from a deter ministic re-\n",
      "lationship via the error ε.\n",
      "For some problems a deterministic relationship does hold. M any of the\n",
      "classiﬁcation problems studied in machine learning are of t his form, where\n",
      "the response surface can be thought of as a colored map deﬁned in IRp.\n",
      "The training data consist of colored examples from the map {xi,gi}, and\n",
      "the goal is to be able to color any point. Here the function is d eterministic,\n",
      "and the randomness enters through the xlocation of the training points.\n",
      "For the moment we will not pursue such problems, but will see t hat they\n",
      "can be handled by techniques appropriate for the error-base d models.\n",
      "The assumption in (2.29) that the errors are independent and identically\n",
      "distributedisnotstrictlynecessary,butseemstobeatthe backofourmind\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.6 Statistical Models, Supervised Learning and Function Ap proximation 29\n",
      "when we average squared errors uniformly in our EPE criterio n. With such\n",
      "a model it becomes natural to use least squares as a data crite rion for\n",
      "model estimation as in (2.1). Simple modiﬁcations can be mad e to avoid\n",
      "the independence assumption; for example, we can have Var( Y|X=x) =\n",
      "σ(x), and now both the mean and variance depend on X. In general the\n",
      "conditional distribution Pr( Y|X) can depend on Xin complicated ways,\n",
      "but the additive error model precludes these.\n",
      "So far we have concentrated on the quantitative response. Ad ditive error\n",
      "models are typically not used for qualitative outputs G; in this case the tar-\n",
      "get function p(X)isthe conditional density Pr( G|X), and this is modeled\n",
      "directly. For example, for two-class data, it is often reaso nable to assume\n",
      "that the data arise from independent binary trials, with the probability of\n",
      "one particular outcome being p(X), and the other 1 −p(X). Thus ifYis\n",
      "the 0–1 coded version of G, then E(Y|X=x) =p(x), but the variance\n",
      "depends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\n",
      "2.6.2 Supervised Learning\n",
      "Before we launch into more statistically oriented jargon, w e present the\n",
      "function-ﬁtting paradigm from a machine learning point of v iew. Suppose\n",
      "for simplicity that the errors are additive and that the mode lY=f(X)+ε\n",
      "is a reasonable assumption. Supervised learning attempts t o learnfby\n",
      "example through a teacher. One observes the system under study, both\n",
      "the inputs and outputs, and assembles a training set of observations T=\n",
      "(xi,yi), i= 1,...,N. The observed input values to the system xiare also\n",
      "fed into an artiﬁcial system, known as a learning algorithm ( usually a com-\n",
      "puter program), which also produces outputs ˆf(xi) in response to the in-\n",
      "puts. The learning algorithm has the property that it can mod ify its in-\n",
      "put/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the\n",
      "original and generated outputs. This process is known as learning by exam-\n",
      "ple. Upon completion of the learning process the hope is that the artiﬁcial\n",
      "and real outputs will be close enough to be useful for all sets of inputs likely\n",
      "to be encountered in practice.\n",
      "2.6.3 Function Approximation\n",
      "The learning paradigm of the previous section has been the mo tivation\n",
      "for research into the supervised learning problem in the ﬁel ds of machine\n",
      "learning (with analogies to human reasoning) and neural net works (with\n",
      "biological analogies to the brain). The approach taken in ap plied mathe-\n",
      "matics and statistics has been from the perspective of funct ion approxima-\n",
      "tion and estimation. Here the data pairs {xi,yi}are viewed as points in a\n",
      "(p+1)-dimensional Euclidean space. The function f(x) has domain equal\n",
      "to thep-dimensional input subspace, and is related to the data via a model\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"30 2. Overview of Supervised Learning\n",
      "such asyi=f(xi)+εi. For convenience in this chapter we will assume the\n",
      "domain is IRp, ap-dimensional Euclidean space, although in general the\n",
      "inputs can be of mixed type. The goal is to obtain a useful appr oximation\n",
      "tof(x) for allxin some region of IRp, given the representations in T.\n",
      "Although somewhat less glamorous than the learning paradig m, treating\n",
      "supervised learning as a problem in function approximation encourages the\n",
      "geometrical concepts of Euclidean spaces and mathematical concepts of\n",
      "probabilistic inference to be applied to the problem. This i s the approach\n",
      "taken in this book.\n",
      "Many of the approximations we will encounter have associate d a set of\n",
      "parameters θthat can be modiﬁed to suit the data at hand. For example,\n",
      "the linear model f(x) =xTβhasθ=β. Another class of useful approxi-\n",
      "mators can be expressed as linear basis expansions\n",
      "fθ(x) =K∑\n",
      "k=1hk(x)θk, (2.30)\n",
      "where thehkare a suitable set of functions or transformations of the inp ut\n",
      "vectorx. Traditional examples are polynomial and trigonometric ex pan-\n",
      "sions, where for example hkmight bex2\n",
      "1,x1x2\n",
      "2, cos(x1) and so on. We\n",
      "also encounter nonlinear expansions, such as the sigmoid tr ansformation\n",
      "common to neural network models,\n",
      "hk(x) =1\n",
      "1+exp(−xTβk). (2.31)\n",
      "We can use least squares to estimate the parameters θinfθas we did\n",
      "for the linear model, by minimizing the residual sum-of-squ ares\n",
      "RSS(θ) =N∑\n",
      "i=1(yi−fθ(xi))2(2.32)\n",
      "as a function of θ. This seems a reasonable criterion for an additive error\n",
      "model. In terms of function approximation, we imagine our pa rameterized\n",
      "function as a surface in p+ 1 space, and what we observe are noisy re-\n",
      "alizations from it. This is easy to visualize when p= 2 and the vertical\n",
      "coordinate is the output y, as in Figure 2.10. The noise is in the output\n",
      "coordinate, so we ﬁnd the set of parameters such that the ﬁtte d surface\n",
      "gets as close to the observed points as possible, where close is measured by\n",
      "the sum of squared vertical errors in RSS( θ).\n",
      "For the linear model we get a simple closed form solution to th e mini-\n",
      "mization problem. This is also true for the basis function me thods, if the\n",
      "basis functions themselves do not have any hidden parameter s. Otherwise\n",
      "the solution requires either iterative methods or numerica l optimization.\n",
      "While least squares is generally very convenient, it is not t he only crite-\n",
      "rion used and in some cases would not make much sense. A more ge neral\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.6 Statistical Models, Supervised Learning and Function Ap proximation 31\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "••• ••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "FIGURE 2.10. Least squares ﬁtting of a function of two inputs. The paramet ers\n",
      "offθ(x)are chosen so as to minimize the sum-of-squared vertical err ors.\n",
      "principle for estimation is maximum likelihood estimation . Suppose we have\n",
      "a random sample yi, i= 1,...,Nfrom a density Pr θ(y) indexed by some\n",
      "parameters θ. The log-probability of the observed sample is\n",
      "L(θ) =N∑\n",
      "i=1logPrθ(yi). (2.33)\n",
      "The principle of maximum likelihood assumes that the most re asonable\n",
      "values forθare those for which the probability of the observed sample is\n",
      "largest. Least squares for the additive error model Y=fθ(X) +ε, with\n",
      "ε∼N(0,σ2), is equivalent to maximum likelihood using the conditiona l\n",
      "likelihood\n",
      "Pr(Y|X,θ) =N(fθ(X),σ2). (2.34)\n",
      "So although the additional assumption of normality seems mo re restrictive,\n",
      "the results are the same. The log-likelihood of the data is\n",
      "L(θ) =−N\n",
      "2log(2π)−Nlogσ−1\n",
      "2σ2N∑\n",
      "i=1(yi−fθ(xi))2,(2.35)\n",
      "and the only term involving θis the last, which is RSS( θ) up to a scalar\n",
      "negative multiplier.\n",
      "A more interesting example is the multinomial likelihood fo r the regres-\n",
      "sion function Pr( G|X) for a qualitative output G. Suppose we have a model\n",
      "Pr(G=Gk|X=x) =pk,θ(x), k= 1,...,Kfor the conditional probabil-\n",
      "ity of each class given X, indexed by the parameter vector θ. Then the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"32 2. Overview of Supervised Learning\n",
      "log-likelihood (also referred to as the cross-entropy) is\n",
      "L(θ) =N∑\n",
      "i=1logpgi,θ(xi), (2.36)\n",
      "and when maximized it delivers values of θthat best conform with the data\n",
      "in this likelihood sense.\n",
      "2.7 Structured Regression Models\n",
      "Wehaveseenthatalthoughnearest-neighborandotherlocal methodsfocus\n",
      "directly on estimating the function at a point, they face pro blems in high\n",
      "dimensions. They may also be inappropriate even in low dimen sions in\n",
      "cases where more structured approaches can make more eﬃcien t use of the\n",
      "data. This section introduces classes of such structured ap proaches. Before\n",
      "we proceed, though, we discuss further the need for such clas ses.\n",
      "2.7.1 Diﬃculty of the Problem\n",
      "Consider the RSS criterion for an arbitrary function f,\n",
      "RSS(f) =N∑\n",
      "i=1(yi−f(xi))2. (2.37)\n",
      "Minimizing (2.37) leads to inﬁnitely many solutions: any fu nctionˆfpassing\n",
      "through the training points ( xi,yi) is a solution. Any particular solution\n",
      "chosen might be a poor predictor at test points diﬀerent from the training\n",
      "points. If there are multiple observation pairs xi,yiℓ, ℓ= 1,...,N iat each\n",
      "value ofxi, the risk is limited. In this case, the solutions pass throug h\n",
      "the average values of the yiℓat eachxi; see Exercise 2.6. The situation is\n",
      "similar to the one we have already visited in Section 2.4; ind eed, (2.37) is\n",
      "the ﬁnite sample version of (2.11) on page 18. If the sample si zeNwere\n",
      "suﬃciently large such that repeats were guaranteed and dens ely arranged,\n",
      "it would seem that these solutions might all tend to the limit ing conditional\n",
      "expectation.\n",
      "In order to obtain useful results for ﬁnite N, we must restrict the eligible\n",
      "solutions to (2.37) to a smaller set of functions. How to deci de on the\n",
      "nature of the restrictions is based on considerations outsi de of the data.\n",
      "Theserestrictions aresometimes encodedviatheparametri crepresentation\n",
      "offθ, or may be built into the learning method itself, either impl icitly or\n",
      "explicitly. These restricted classes of solutions are the m ajor topic of this\n",
      "book. One thing should be clear, though. Any restrictions im posed onf\n",
      "that lead to a unique solution to (2.37) do not really remove t he ambiguity\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.8 Classes of Restricted Estimators 33\n",
      "caused by the multiplicity of solutions. There are inﬁnitel y many possible\n",
      "restrictions, each leading to a unique solution, so the ambi guity has simply\n",
      "been transferred to the choice of constraint.\n",
      "In general the constraints imposed by most learning methods can be\n",
      "described as complexity restrictions of one kind or another. This usually\n",
      "means some kind of regular behavior in small neighborhoods o f the input\n",
      "space. That is, for all input points xsuﬃciently close to each other in\n",
      "some metric, ˆfexhibits some special structure such as nearly constant,\n",
      "linear or low-order polynomial behavior. The estimator is t hen obtained by\n",
      "averaging or polynomial ﬁtting in that neighborhood.\n",
      "The strength of the constraint is dictated by the neighborho od size. The\n",
      "larger the size of the neighborhood, the stronger the constr aint, and the\n",
      "more sensitive the solution is to the particular choice of co nstraint. For\n",
      "example, local constant ﬁts in inﬁnitesimally small neighb orhoods is no\n",
      "constraint at all; local linear ﬁts in very large neighborho ods is almost a\n",
      "globally linear model, and is very restrictive.\n",
      "The nature of the constraint depends on the metric used. Some methods,\n",
      "such as kernel and local regression and tree-based methods, directly specify\n",
      "the metric and size of the neighborhood. The nearest-neighb or methods\n",
      "discussed so far are based on the assumption that locally the function is\n",
      "constant;closetoatargetinput x0,thefunctiondoesnotchangemuch,and\n",
      "so close outputs can be averaged to produce ˆf(x0). Other methods such\n",
      "as splines, neural networks and basis-function methods imp licitly deﬁne\n",
      "neighborhoods of local behavior. In Section 5.4.1 we discus s the concept\n",
      "of anequivalent kernel (see Figure 5.8 on page 157), which describes this\n",
      "local dependence for any method linear in the outputs. These equivalent\n",
      "kernels in many cases look just like the explicitly deﬁned we ighting kernels\n",
      "discussed above—peaked at the target point and falling away s moothly\n",
      "away from it.\n",
      "One fact should be clear by now. Any method that attempts to pr o-\n",
      "duce locally varying functions in small isotropic neighbor hoods will run\n",
      "into problems in high dimensions—again the curse of dimensio nality. And\n",
      "conversely, all methods that overcome the dimensionality p roblems have an\n",
      "associated—andoftenimplicitoradaptive—metricformeasur ingneighbor-\n",
      "hoods, which basically does not allow the neighborhood to be simultane-\n",
      "ously small in all directions.\n",
      "2.8 Classes of Restricted Estimators\n",
      "The variety of nonparametric regressiontechniques or lear ning methods fall\n",
      "intoanumberofdiﬀerentclassesdependingonthenatureoft herestrictions\n",
      "imposed. These classes are not distinct, and indeed some met hods fall in\n",
      "several classes. Here we give a brief summary, since detaile d descriptions\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"34 2. Overview of Supervised Learning\n",
      "are given in later chapters. Each of the classes has associat ed with it one\n",
      "or more parameters, sometimes appropriately called smoothing parameters,\n",
      "that control the eﬀective size of the local neighborhood. He re we describe\n",
      "three broad classes.\n",
      "2.8.1 Roughness Penalty and Bayesian Methods\n",
      "Here the class of functions is controlled by explicitly pena lizing RSS( f)\n",
      "with a roughness penalty\n",
      "PRSS(f;λ) = RSS(f)+λJ(f). (2.38)\n",
      "The user-selected functional J(f) will be large for functions fthat vary too\n",
      "rapidly over small regions of input space. For example, the p opularcubic\n",
      "smoothing spline for one-dimensional inputs is the solution to the penalized\n",
      "least-squares criterion\n",
      "PRSS(f;λ) =N∑\n",
      "i=1(yi−f(xi))2+λ∫\n",
      "[f′′(x)]2dx. (2.39)\n",
      "The roughness penalty here controls large values of the seco nd derivative\n",
      "off, and the amount of penalty is dictated by λ≥0. Forλ= 0 no penalty\n",
      "is imposed, and any interpolating function will do, while fo rλ=∞only\n",
      "functions linear in xare permitted.\n",
      "Penalty functionals Jcan be constructed for functions in any dimension,\n",
      "and special versions can be created to impose special struct ure. For ex-\n",
      "ample, additive penalties J(f) =∑p\n",
      "j=1J(fj) are used in conjunction with\n",
      "additive functions f(X) =∑p\n",
      "j=1fj(Xj) to create additive models with\n",
      "smooth coordinate functions. Similarly, projection pursuit regression mod-\n",
      "els havef(X) =∑M\n",
      "m=1gm(αT\n",
      "mX) for adaptively chosen directions αm, and\n",
      "the functions gmcan each have an associated roughness penalty.\n",
      "Penalty function, or regularization methods, express our prior belief that\n",
      "the type of functions we seek exhibit a certain type of smooth behavior, and\n",
      "indeed can usually be cast in a Bayesian framework. The penal tyJcorre-\n",
      "sponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\n",
      "minimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss\n",
      "roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\n",
      "Chapter 8.\n",
      "2.8.2 Kernel Methods and Local Regression\n",
      "Thesemethodscanbethoughtofasexplicitlyprovidingesti matesofthere-\n",
      "gression function or conditional expectation by specifyin g the nature of the\n",
      "local neighborhood, and of the class of regular functions ﬁt ted locally. The\n",
      "local neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.8 Classes of Restricted Estimators 35\n",
      "weights to points xin a region around x0(see Figure 6.1 on page 192). For\n",
      "example, the Gaussian kernel has a weight function based on t he Gaussian\n",
      "density function\n",
      "Kλ(x0,x) =1\n",
      "λexp[\n",
      "−||x−x0||2\n",
      "2λ]\n",
      "(2.40)\n",
      "and assigns weights to points that die exponentially with th eir squared\n",
      "Euclidean distance from x0. The parameter λcorresponds to the variance\n",
      "of the Gaussian density, and controls the width of the neighb orhood. The\n",
      "simplest form of kernel estimate is the Nadaraya–Watson wei ghted average\n",
      "ˆf(x0) =∑N\n",
      "i=1Kλ(x0,xi)yi∑N\n",
      "i=1Kλ(x0,xi). (2.41)\n",
      "In general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\n",
      "whereˆθminimizes\n",
      "RSS(fθ,x0) =N∑\n",
      "i=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\n",
      "andfθis some parameterized function, such as a low-order polynom ial.\n",
      "Some examples are:\n",
      "•fθ(x) =θ0, the constant function; this results in the Nadaraya–\n",
      "Watson estimate in (2.41) above.\n",
      "•fθ(x) =θ0+θ1xgives the popular local linear regression model.\n",
      "Nearest-neighbor methods can be thought of as kernel method s having a\n",
      "more data-dependent metric. Indeed, the metric for k-nearest neighbors is\n",
      "Kk(x,x0) =I(||x−x0||≤||x(k)−x0||),\n",
      "wherex(k)is the training observation ranked kth in distance from x0, and\n",
      "I(S) is the indicator of the set S.\n",
      "Thesemethodsofcourseneedtobemodiﬁedinhighdimensions ,toavoid\n",
      "the curse of dimensionality. Various adaptations are discu ssed in Chapter 6.\n",
      "2.8.3 Basis Functions and Dictionary Methods\n",
      "This class of methods includes the familiar linear and polyn omial expan-\n",
      "sions, but more importantly a wide variety of more ﬂexible mo dels. The\n",
      "model forfis a linear expansion of basis functions\n",
      "fθ(x) =M∑\n",
      "m=1θmhm(x), (2.43)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"36 2. Overview of Supervised Learning\n",
      "where each of the hmis a function of the input x, and the term linear here\n",
      "refers to the action of the parameters θ. This class covers a wide variety of\n",
      "methods. In some cases the sequence of basis functions is pre scribed, such\n",
      "as a basis for polynomials in xof total degree M.\n",
      "Forone-dimensional x,polynomialsplinesofdegree Kcanberepresented\n",
      "by an appropriate sequence of Mspline basis functions, determined in turn\n",
      "byM−K−1knots.Theseproducefunctionsthatarepiecewisepolynomials\n",
      "of degreeKbetween the knots, and joined up with continuity of degree\n",
      "K−1 at the knots. As an example consider linear splines, or piec ewise\n",
      "linear functions. One intuitively satisfying basis consis ts of the functions\n",
      "b1(x) = 1,b2(x) =x, andbm+2(x) = (x−tm)+,m= 1,...,M−2,\n",
      "wheretmis themth knot, and z+denotes positive part. Tensor products\n",
      "of spline bases can be used for inputs with dimensions larger than one\n",
      "(see Section 5.2, and the CART and MARS models in Chapter 9.) T he\n",
      "parameterMcontrols the degree of the polynomial or the number of knots\n",
      "in the case of splines.\n",
      "Radial basis functions are symmetric p-dimensional kernels located at\n",
      "particular centroids,\n",
      "fθ(x) =M∑\n",
      "m=1Kλm(µm,x)θm; (2.44)\n",
      "for example, the Gaussian kernel Kλ(µ,x) =e−||x−µ||2/2λis popular.\n",
      "Radial basis functions have centroids µmand scales λmthat have to\n",
      "be determined. The spline basis functions have knots. In gen eral we would\n",
      "like the data to dictate them as well. Including these as para meters changes\n",
      "the regression problem from a straightforward linear probl em to a combi-\n",
      "natorially hard nonlinear problem. In practice, shortcuts such as greedy\n",
      "algorithms or two stage processes are used. Section 6.7 desc ribes some such\n",
      "approaches.\n",
      "A single-layer feed-forward neural network model with line ar output\n",
      "weights can be thought of as an adaptive basis function metho d. The model\n",
      "has the form\n",
      "fθ(x) =M∑\n",
      "m=1βmσ(αT\n",
      "mx+bm), (2.45)\n",
      "whereσ(x) = 1/(1 +e−x) is known as the activation function. Here, as\n",
      "in the projection pursuit model, the directions αmand thebiastermsbm\n",
      "havetobedetermined,andtheirestimationisthemeatofthe computation.\n",
      "Details are given in Chapter 11.\n",
      "These adaptively chosen basis function methods are also kno wn asdictio-\n",
      "narymethods, where one has available a possibly inﬁnite set or di ctionary\n",
      "Dof candidate basis functions from which to choose, and model s are built\n",
      "up by employing some kind of search mechanism.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\n",
      "2.9 Model Selection and the Bias–Variance\n",
      "Tradeoﬀ\n",
      "All the models described above and many others discussed in l ater chapters\n",
      "have asmoothing orcomplexity parameter that has to be determined:\n",
      "•the multiplier of the penalty term;\n",
      "•the width of the kernel;\n",
      "•or the number of basis functions.\n",
      "Inthecaseofthesmoothingspline,theparameter λindexesmodelsranging\n",
      "from a straight line ﬁt to the interpolating model. Similarl y a local degree-\n",
      "mpolynomial model ranges between a degree- mglobal polynomial when\n",
      "the window size is inﬁnitely large, to an interpolating ﬁt wh en the window\n",
      "size shrinks to zero. This means that we cannot use residual s um-of-squares\n",
      "on the training data to determine these parameters as well, s ince we would\n",
      "always pick those that gave interpolating ﬁts and hence zero residuals. Such\n",
      "a model is unlikely to predict future data well at all.\n",
      "Thek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\n",
      "peting forces that aﬀect the predictive ability of such appr oximations. Sup-\n",
      "pose the data arise from a model Y=f(X) +ε, with E(ε) = 0 and\n",
      "Var(ε) =σ2. For simplicity here we assume that the values of xiin the\n",
      "sample are ﬁxed in advance (nonrandom). The expected predic tion error\n",
      "atx0, also known as testorgeneralization error, can be decomposed:\n",
      "EPEk(x0) = E[(Y−ˆfk(x0))2|X=x0]\n",
      "=σ2+[Bias2(ˆfk(x0))+Var T(ˆfk(x0))] (2.46)\n",
      "=σ2+[\n",
      "f(x0)−1\n",
      "kk∑\n",
      "ℓ=1f(x(ℓ))]2\n",
      "+σ2\n",
      "k.(2.47)\n",
      "The subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\n",
      "tox0.\n",
      "There are three terms in this expression. The ﬁrst term σ2is their-\n",
      "reducible error—the variance of the new test target—and is beyond our\n",
      "control, even if we know the true f(x0).\n",
      "The second and third terms are under our control, and make up t he\n",
      "mean squared error ofˆfk(x0) in estimating f(x0), which is broken down\n",
      "into a bias component and a variance component. The bias term is the\n",
      "squared diﬀerence between the true mean f(x0) and the expected value of\n",
      "the estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\n",
      "randomness in the training data. This term will most likely i ncrease with\n",
      "k, if the true function is reasonably smooth. For small kthe few closest\n",
      "neighbors will have values f(x(ℓ)) close tof(x0), so their average should\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"38 2. Overview of Supervised Learning\n",
      "High Bias\n",
      "Low VarianceLow Bias\n",
      "High VariancePrediction Error\n",
      "Model ComplexityTraining SampleTest Sample\n",
      "Low High\n",
      "FIGURE 2.11. Test and training error as a function of model complexity.\n",
      "be close to f(x0). Askgrows, the neighbors are further away, and then\n",
      "anything can happen.\n",
      "The variance term is simply the variance of an average here, a nd de-\n",
      "creases as the inverse of k. So askvaries, there is a bias–variance tradeoﬀ.\n",
      "More generally, as the model complexity of our procedureis increased, the\n",
      "variance tends to increase and the squared bias tends to decr ease. The op-\n",
      "posite behavior occurs as the model complexity is decreased . Fork-nearest\n",
      "neighbors, the model complexity is controlled by k.\n",
      "Typically we would like to choose our model complexity to tra de bias\n",
      "oﬀ with variance in such a way as to minimize the test error. An obvious\n",
      "estimate of test error is the training error1\n",
      "N∑\n",
      "i(yi−ˆyi)2. Unfortunately\n",
      "training error is not a good estimate of test error, as it does not properly\n",
      "account for model complexity.\n",
      "Figure 2.11 shows the typical behavior of the test and traini ng error, as\n",
      "model complexity is varied. The training error tends to decr ease whenever\n",
      "we increase the model complexity, that is, whenever we ﬁt the data harder.\n",
      "However with too much ﬁtting, the model adapts itself too clo sely to the\n",
      "training data, and will not generalize well (i.e., have larg e test error). In\n",
      "that case the predictions ˆf(x0) will have large variance, as reﬂected in the\n",
      "last term of expression (2.46). In contrast, if the model is n ot complex\n",
      "enough, it will underﬁt and may have large bias, again resulting in poor\n",
      "generalization. In Chapter 7 we discuss methods for estimat ing the test\n",
      "error of a prediction method, and hence estimating the optim al amount of\n",
      "model complexity for a given prediction method and training set.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 39\n",
      "Bibliographic Notes\n",
      "Some good general books on the learning problem are Duda et al . (2000),\n",
      "Bishop(1995),(Bishop, 2006),Ripley(1996),Cherkasskya ndMulier(2007)\n",
      "and Vapnik (1996). Parts of this chapter are based on Friedma n (1994b).\n",
      "Exercises\n",
      "Ex. 2.1Suppose each of K-classes has an associated target tk, which is a\n",
      "vector of all zeros, except a one in the kth position. Show that classifying to\n",
      "the largest element of ˆ yamounts to choosing the closest target, min k||tk−\n",
      "ˆy||, if the elements of ˆ ysum to one.\n",
      "Ex. 2.2Show how to compute the Bayes decision boundary for the simul a-\n",
      "tion example in Figure 2.5.\n",
      "Ex. 2.3Derive equation (2.24).\n",
      "Ex. 2.4The edge eﬀect problem discussed on page 23 is not peculiar to\n",
      "uniform sampling from bounded domains. Consider inputs dra wn from a\n",
      "spherical multinormal distribution X∼N(0,Ip). The squared distance\n",
      "from any sample point to the origin has a χ2\n",
      "pdistribution with mean p.\n",
      "Consider a prediction point x0drawn from this distribution, and let a=\n",
      "x0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\n",
      "each of the training points on this direction.\n",
      "Show that the ziare distributed N(0,1) with expected squared distance\n",
      "from the origin 1, while the target point has expected square d distancep\n",
      "from the origin.\n",
      "Hence forp= 10, a randomly drawn test point is about 3 .1 standard\n",
      "deviations from the origin, while all the training points ar e on average\n",
      "one standard deviation along direction a. So most prediction points see\n",
      "themselves as lying on the edge of the training set.\n",
      "Ex. 2.5\n",
      "(a) Derive equation (2.27). The last line makes use of (3.8) t hrough a\n",
      "conditioning argument.\n",
      "(b) Derive equation (2.28), making use of the cyclicproperty of the trace\n",
      "operator [trace( AB) = trace(BA)], and its linearity (which allows us\n",
      "to interchange the order of trace and expectation).\n",
      "Ex. 2.6Consider a regression problem with inputs xiand outputs yi, and a\n",
      "parameterized model fθ(x) to be ﬁt by least squares. Show that if there are\n",
      "observations with tiedoridentical values ofx, then the ﬁt can be obtained\n",
      "from a reduced weighted least squares problem.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"40 2. Overview of Supervised Learning\n",
      "Ex. 2.7Suppose we have a sample of Npairsxi,yidrawn i.i.d. from the\n",
      "distribution characterized as follows:\n",
      "xi∼h(x),the design density\n",
      "yi=f(xi)+εi, fis the regression function\n",
      "εi∼(0,σ2) (mean zero, variance σ2)\n",
      "We construct an estimator for flinearin theyi,\n",
      "ˆf(x0) =N∑\n",
      "i=1ℓi(x0;X)yi,\n",
      "where the weights ℓi(x0;X) do not depend on the yi, but do depend on the\n",
      "entire training sequence of xi, denoted here by X.\n",
      "(a)Showthatlinearregressionand k-nearest-neighborregressionaremem-\n",
      "bersofthisclassofestimators.Describeexplicitlythewe ightsℓi(x0;X)\n",
      "in each of these cases.\n",
      "(b) Decompose the conditional mean-squared error\n",
      "EY|X(f(x0)−ˆf(x0))2\n",
      "intoaconditionalsquaredbiasandaconditionalvariancec omponent.\n",
      "LikeX,Yrepresents the entire training sequence of yi.\n",
      "(c) Decompose the (unconditional) mean-squared error\n",
      "EY,X(f(x0)−ˆf(x0))2\n",
      "into a squared bias and a variance component.\n",
      "(d) Establish a relationship between the squared biases and variances in\n",
      "the above two cases.\n",
      "Ex. 2.8Compare the classiﬁcation performance of linear regressio n andk–\n",
      "nearest neighbor classiﬁcation on the zipcodedata. In particular, consider\n",
      "only the 2’s and3’s, andk= 1,3,5,7 and 15. Show both the training and\n",
      "test error for each choice. The zipcodedata are available from the book\n",
      "websitewww-stat.stanford.edu/ElemStatLearn .\n",
      "Ex. 2.9Consider a linear regression model with pparameters, ﬁt by least\n",
      "squares to a set of training data ( x1,y1),...,(xN,yN) drawn at random\n",
      "from a population. Let ˆβbe the least squares estimate. Suppose we have\n",
      "some test data (˜ x1,˜y1),...,(˜xM,˜yM) drawn at random from the same pop-\n",
      "ulation as the training data. If Rtr(β) =1\n",
      "N∑N\n",
      "1(yi−βTxi)2andRte(β) =\n",
      "1\n",
      "M∑M\n",
      "1(˜yi−βT˜xi)2, prove that\n",
      "E[Rtr(ˆβ)]≤E[Rte(ˆβ)],\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 41\n",
      "where the expectations are over all that is random in each exp ression. [This\n",
      "exercisewasbroughttoourattentionbyRyanTibshirani,fr omahomework\n",
      "assignment given by Andrew Ng.]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"42 2. Overview of Supervised Learning\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 45\n",
      "•• •\n",
      "••••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "X1X2Y\n",
      "FIGURE 3.1. Linear least squares ﬁtting with X∈IR2. We seek the linear\n",
      "function of Xthat minimizes the sum of squared residuals from Y.\n",
      "space occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\n",
      "about the validity of model (3.1); it simply ﬁnds the best lin ear ﬁt to the\n",
      "data. Least squares ﬁtting is intuitively satisfying no mat ter how the data\n",
      "arise; the criterion measures the average lack of ﬁt.\n",
      "How do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\n",
      "each row an input vector (with a 1 in the ﬁrst position), and si milarly let\n",
      "ybe theN-vector of outputs in the training set. Then we can write the\n",
      "residual sum-of-squares as\n",
      "RSS(β) = (y−Xβ)T(y−Xβ). (3.3)\n",
      "This is a quadratic function in the p+1 parameters. Diﬀerentiating with\n",
      "respect toβwe obtain\n",
      "∂RSS\n",
      "∂β=−2XT(y−Xβ)\n",
      "∂2RSS\n",
      "∂β∂βT= 2XTX.(3.4)\n",
      "Assuming (for the moment) that Xhas full column rank, and hence XTX\n",
      "is positive deﬁnite, we set the ﬁrst derivative to zero\n",
      "XT(y−Xβ) = 0 (3.5)\n",
      "to obtain the unique solution\n",
      "ˆβ= (XTX)−1XTy. (3.6)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"46 3. Linear Methods for Regression\n",
      "x1x2y\n",
      "ˆ y\n",
      "FIGURE 3.2. TheN-dimensional geometry of least squares regression with two\n",
      "predictors. The outcome vector yis orthogonally projected onto the hyperplane\n",
      "spanned by the input vectors x1andx2. The projection ˆyrepresents the vector\n",
      "of the least squares predictions\n",
      "The predicted values at an input vector x0are given by ˆf(x0) = (1 :x0)Tˆβ;\n",
      "the ﬁtted values at the training inputs are\n",
      "ˆy=Xˆβ=X(XTX)−1XTy, (3.7)\n",
      "where ˆyi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\n",
      "(3.7) is sometimes called the “hat” matrix because it puts th e hat on y.\n",
      "Figure3.2showsadiﬀerentgeometricalrepresentationoft heleastsquares\n",
      "estimate,thistimeinIRN.Wedenotethecolumnvectorsof Xbyx0,x1,...,xp,\n",
      "withx0≡1. For much of what follows, this ﬁrst column is treated like a ny\n",
      "other. These vectors span a subspace of IRN, also referred to as the column\n",
      "space of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the\n",
      "residual vector y−ˆyis orthogonal to this subspace. This orthogonality is\n",
      "expressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\n",
      "jectionofyonto this subspace. The hat matrix Hcomputes the orthogonal\n",
      "projection, and hence it is also known as a projection matrix .\n",
      "It might happen that the columns of Xare not linearly independent, so\n",
      "thatXis not of full rank. This would occur, for example, if two of th e\n",
      "inputs were perfectly correlated, (e.g., x2= 3x1). ThenXTXis singular\n",
      "and the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\n",
      "the ﬁtted values ˆy=Xˆβare still the projection of yonto the column\n",
      "space of X; there is just more than one way to express that projection\n",
      "in terms of the column vectors of X. The non-full-rank case occurs most\n",
      "oftenwhenoneormorequalitative inputsarecodedinaredun dantfashion.\n",
      "There is usually a natural way to resolve the non-unique repr esentation,\n",
      "by recoding and/or dropping redundant columns in X. Most regression\n",
      "software packages detect these redundancies and automatic ally implement\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 47\n",
      "some strategy for removing them. Rank deﬁciencies can also o ccur in signal\n",
      "and image analysis, where the number of inputs pcan exceed the number\n",
      "of training cases N. In this case, the features are typically reduced by\n",
      "ﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\n",
      "Chapter 18).\n",
      "Up to now we have made minimal assumptions about the true dist ribu-\n",
      "tion of the data. In order to pin down the sampling properties ofˆβ, we now\n",
      "assume that the observations yiare uncorrelated and have constant vari-\n",
      "anceσ2, and that the xiare ﬁxed (non random). The variance–covariance\n",
      "matrix of the least squares parameter estimates is easily de rived from (3.6)\n",
      "and is given by\n",
      "Var(ˆβ) = (XTX)−1σ2. (3.8)\n",
      "Typically one estimates the variance σ2by\n",
      "ˆσ2=1\n",
      "N−p−1N∑\n",
      "i=1(yi−ˆyi)2.\n",
      "TheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\n",
      "estimate of σ2: E(ˆσ2) =σ2.\n",
      "To draw inferences about the parameters and the model, addit ional as-\n",
      "sumptions are needed. We now assume that (3.1) is the correct model for\n",
      "the mean; that is, the conditional expectation of Yis linear inX1,...,X p.\n",
      "Wealsoassumethatthedeviationsof Yarounditsexpectationareadditive\n",
      "and Gaussian. Hence\n",
      "Y= E(Y|X1,...,X p)+ε\n",
      "=β0+p∑\n",
      "j=1Xjβj+ε, (3.9)\n",
      "where the error εis a Gaussian random variable with expectation zero and\n",
      "varianceσ2, writtenε∼N(0,σ2).\n",
      "Under (3.9), it is easy to show that\n",
      "ˆβ∼N(β,(XTX)−1σ2). (3.10)\n",
      "This is a multivariate normal distribution with mean vector and variance–\n",
      "covariance matrix as shown. Also\n",
      "(N−p−1)ˆσ2∼σ2χ2\n",
      "N−p−1, (3.11)\n",
      "a chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\n",
      "and ˆσ2are statistically independent. We use these distributiona l properties\n",
      "to form tests of hypothesis and conﬁdence intervals for the p arametersβj.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"48 3. Linear Methods for Regression\n",
      "ZTail Probabilities\n",
      "2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\n",
      "t100\n",
      "normal\n",
      "FIGURE 3.3. The tail probabilities Pr(|Z|>z)for three distributions, t30,t100\n",
      "and standard normal. Shown are the appropriate quantiles for t esting signiﬁcance\n",
      "at thep= 0.05and0.01levels. The diﬀerence between tand the standard normal\n",
      "becomes negligible for Nbigger than about 100.\n",
      "To test the hypothesis that a particular coeﬃcient βj= 0, we form the\n",
      "standardized coeﬃcient or Z-score\n",
      "zj=ˆβj\n",
      "ˆσ√vj, (3.12)\n",
      "wherevjisthejthdiagonalelementof( XTX)−1.Underthenullhypothesis\n",
      "thatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\n",
      "degrees of freedom), and hence a large (absolute) value of zjwill lead to\n",
      "rejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\n",
      "zjwould have a standard normal distribution. The diﬀerence be tween the\n",
      "tail quantiles of a t-distribution and a standard normal become negligible\n",
      "as the sample size increases, and so we typically use the norm al quantiles\n",
      "(see Figure 3.3).\n",
      "Often we need to test for the signiﬁcance of groups of coeﬃcie nts simul-\n",
      "taneously. For example, to test if a categorical variable wi thklevels can\n",
      "be excluded from a model, we need to test whether the coeﬃcien ts of the\n",
      "dummy variables used to represent the levels can all be set to zero. Here\n",
      "we use the Fstatistic,\n",
      "F=(RSS0−RSS1)/(p1−p0)\n",
      "RSS1/(N−p1−1), (3.13)\n",
      "whereRSS 1istheresidualsum-of-squaresfortheleastsquaresﬁtofth ebig-\n",
      "ger model with p1+1 parameters, and RSS 0the same for the nested smaller\n",
      "model with p0+1 parameters, having p1−p0parameters constrained to be\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 49\n",
      "zero. TheFstatistic measures the change in residual sum-of-squares p er\n",
      "additional parameter in the bigger model, and it is normaliz ed by an esti-\n",
      "mate ofσ2. Under the Gaussian assumptions, and the null hypothesis th at\n",
      "the smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\n",
      "tribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\n",
      "to theFstatistic for dropping the single coeﬃcient βjfrom the model. For\n",
      "largeN, the quantiles of Fp1−p0,N−p1−1approach those of χ2\n",
      "p1−p0/(p1−p0).\n",
      "Similarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\n",
      "forβj:\n",
      "(ˆβj−z(1−α)v1\n",
      "2\n",
      "jˆσ,ˆβj+z(1−α)v1\n",
      "2\n",
      "jˆσ). (3.14)\n",
      "Herez(1−α)is the 1−αpercentile of the normal distribution:\n",
      "z(1−0.025)= 1.96,\n",
      "z(1−.05)= 1.645,etc.\n",
      "Hence the standard practice of reporting ˆβ±2·se(ˆβ) amounts to an ap-\n",
      "proximate 95% conﬁdence interval. Even if the Gaussian erro r assumption\n",
      "does not hold, this interval will be approximately correct, with its coverage\n",
      "approaching 1−2αas the sample size N→∞.\n",
      "In a similar fashion we can obtain an approximate conﬁdence s et for the\n",
      "entire parameter vector β, namely\n",
      "Cβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\n",
      "p+1(1−α)}, (3.15)\n",
      "whereχ2\n",
      "ℓ(1−α)is the 1−αpercentile of the chi-squared distribution on ℓ\n",
      "degrees of freedom: for example, χ2\n",
      "5(1−0.05)= 11.1,χ2\n",
      "5(1−0.1)= 9.2. This\n",
      "conﬁdence set for βgenerates a corresponding conﬁdence set for the true\n",
      "functionf(x) =xTβ, namely{xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\n",
      "ure 5.4 in Section 5.2.2 for examples of conﬁdence bands for f unctions).\n",
      "3.2.1 Example: Prostate Cancer\n",
      "The data for this example come from a study by Stamey et al. (19 89). They\n",
      "examined the correlation between the level of prostate-spe ciﬁc antigen and\n",
      "a number of clinical measures in men who were about to receive a radical\n",
      "prostatectomy. The variables are log cancer volume ( lcavol), log prostate\n",
      "weight ( lweight),age, log of the amount of benign prostatic hyperplasia\n",
      "(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\n",
      "Gleason score ( gleason), and percent of Gleason scores 4 or 5 ( pgg45).\n",
      "The correlation matrix of the predictors given in Table 3.1 s hows many\n",
      "strong correlations. Figure 1.1 (page 3) of Chapter 1 is a sca tterplot matrix\n",
      "showing every pairwise plot between the variables. We see th atsviis a\n",
      "binary variable, and gleasonis an ordered categorical variable. We see, for\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"50 3. Linear Methods for Regression\n",
      "TABLE 3.1. Correlations of predictors in the prostate cancer data.\n",
      "lcavol lweight age lbph svi lcp gleason\n",
      "lweight 0.300\n",
      "age0.286 0.317\n",
      "lbph0.063 0.437 0.287\n",
      "svi0.593 0.181 0.129 −0.139\n",
      "lcp0.692 0.157 0.173 −0.089 0.671\n",
      "gleason 0.426 0.024 0.366 0.033 0.307 0.476\n",
      "pgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\n",
      "TABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\n",
      "coeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\n",
      "in absolute value is signiﬁcantly nonzero at the p= 0.05level.\n",
      "Term Coeﬃcient Std. Error ZScore\n",
      "Intercept 2.46 0.09 27.60\n",
      "lcavol 0.68 0.13 5.37\n",
      "lweight 0.26 0.10 2.75\n",
      "age−0.14 0.10 −1.40\n",
      "lbph 0.21 0.10 2.06\n",
      "svi 0.31 0.12 2.47\n",
      "lcp−0.29 0.15 −1.87\n",
      "gleason−0.02 0.15 −0.15\n",
      "pgg45 0.27 0.15 1.74\n",
      "example, that both lcavolandlcpshow a strong relationship with the\n",
      "response lpsa, and with each other. We need to ﬁt the eﬀects jointly to\n",
      "untangle the relationships between the predictors and the r esponse.\n",
      "We ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\n",
      "ﬁrst standardizing the predictors to have unit variance. We randomly split\n",
      "the dataset into a training set of size 67 and a test set of size 30. We ap-\n",
      "plied least squares estimation to the training set, produci ng the estimates,\n",
      "standard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\n",
      "in (3.12), and measure the eﬀect of dropping that variable fr om the model.\n",
      "AZ-score greater than 2 in absolute value is approximately sig niﬁcant at\n",
      "the 5% level. (For our example, we have nine parameters, and t he 0.025 tail\n",
      "quantiles of the t67−9distribution are±2.002!) The predictor lcavolshows\n",
      "the strongest eﬀect, with lweightandsvialso strong. Notice that lcpis\n",
      "not signiﬁcant, once lcavolis in the model (when used in a model without\n",
      "lcavol,lcpis strongly signiﬁcant). We can also test for the exclusion o f\n",
      "a number of terms at once, using the F-statistic (3.13). For example, we\n",
      "consider dropping all the non-signiﬁcant terms in Table 3.2 , namely age,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 51\n",
      "lcp,gleason, andpgg45. We get\n",
      "F=(32.81−29.43)/(9−5)\n",
      "29.43/(67−9)= 1.67, (3.16)\n",
      "which has a p-value of 0.17 (Pr(F4,58>1.67) = 0.17), and hence is not\n",
      "signiﬁcant.\n",
      "The mean prediction error on the test data is 0 .521. In contrast, predic-\n",
      "tion using the mean training value of lpsahas a test error of 1 .057, which\n",
      "is called the “base error rate.” Hence the linear model reduc es the base\n",
      "error rate by about 50%. We will return to this example later t o compare\n",
      "various selection and shrinkage methods.\n",
      "3.2.2 The Gauss–Markov Theorem\n",
      "One of the most famous results in statistics asserts that the least squares\n",
      "estimates of the parameters βhave the smallest variance among all linear\n",
      "unbiased estimates. We will make this precise here, and also make clear\n",
      "that the restriction to unbiased estimates is not necessari ly a wise one. This\n",
      "observationwillleadustoconsiderbiasedestimatessucha sridgeregression\n",
      "later in the chapter. We focus on estimation of any linear com bination of\n",
      "the parameters θ=aTβ; for example, predictions f(x0) =xT\n",
      "0βare of this\n",
      "form. The least squares estimate of aTβis\n",
      "ˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\n",
      "Considering Xto be ﬁxed, this is a linear function cT\n",
      "0yof the response\n",
      "vectory. If we assume that the linear model is correct, aTˆβis unbiased\n",
      "since\n",
      "E(aTˆβ) = E(aT(XTX)−1XTy)\n",
      "=aT(XTX)−1XTXβ\n",
      "=aTβ. (3.18)\n",
      "The Gauss–Markov theorem states that if we have any other lin ear estima-\n",
      "tor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\n",
      "Var(aTˆβ)≤Var(cTy). (3.19)\n",
      "The proof (Exercise 3.3) uses the triangle inequality. For s implicity we have\n",
      "stated the result in terms of estimation of a single paramete raTβ, but with\n",
      "a few more deﬁnitions one can state it in terms of the entire pa rameter\n",
      "vectorβ(Exercise 3.3).\n",
      "Consider the mean squared error of an estimator ˜θin estimating θ:\n",
      "MSE(˜θ) = E( ˜θ−θ)2\n",
      "= Var( ˜θ)+[E(˜θ)−θ]2. (3.20)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"52 3. Linear Methods for Regression\n",
      "The ﬁrst term is the variance, while the second term is the squ ared bias.\n",
      "TheGauss-Markovtheoremimpliesthattheleastsquaresest imatorhasthe\n",
      "smallest mean squared error of all linear estimators with no bias. However,\n",
      "there may well exist a biased estimator with smaller mean squ ared error.\n",
      "Suchanestimatorwouldtradealittlebiasforalargerreduc tioninvariance.\n",
      "Biased estimates are commonly used. Any method that shrinks or sets to\n",
      "zero some of the least squares coeﬃcients may result in a bias ed estimate.\n",
      "We discuss many examples, including variable subset select ion and ridge\n",
      "regression, later in this chapter. From a more pragmatic poi nt of view, most\n",
      "models are distortions of the truth, and hence are biased; pi cking the right\n",
      "model amounts to creating the right balance between bias and variance.\n",
      "We go into these issues in more detail in Chapter 7.\n",
      "Mean squared error is intimately related to prediction accu racy, as dis-\n",
      "cussed in Chapter 2. Consider the prediction of the new respo nse at input\n",
      "x0,\n",
      "Y0=f(x0)+ε0. (3.21)\n",
      "Then the expected prediction error of an estimate ˜f(x0) =xT\n",
      "0˜βis\n",
      "E(Y0−˜f(x0))2=σ2+E(xT\n",
      "0˜β−f(x0))2\n",
      "=σ2+MSE( ˜f(x0)). (3.22)\n",
      "Therefore, expected prediction error and mean squared erro r diﬀer only by\n",
      "the constant σ2, representing the variance of the new observation y0.\n",
      "3.2.3 Multiple Regression from Simple Univariate Regressi on\n",
      "The linear model (3.1) with p >1 inputs is called the multiple linear\n",
      "regression model . The least squares estimates (3.6) for this model are best\n",
      "understood in terms of the estimates for the univariate (p= 1) linear\n",
      "model, as we indicate in this section.\n",
      "Suppose ﬁrst that we have a univariate model with no intercep t, that is,\n",
      "Y=Xβ+ε. (3.23)\n",
      "The least squares estimate and residuals are\n",
      "ˆβ=∑N\n",
      "1xiyi∑N\n",
      "1x2\n",
      "i,\n",
      "ri=yi−xiˆβ.(3.24)\n",
      "In convenient vector notation, we let y= (y1,...,y N)T,x= (x1,...,x N)T\n",
      "and deﬁne\n",
      "⟨x,y⟩=N∑\n",
      "i=1xiyi,\n",
      "=xTy, (3.25)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 53\n",
      "theinner product between xandy1. Then we can write\n",
      "ˆβ=⟨x,y⟩\n",
      "⟨x,x⟩,\n",
      "r=y−xˆβ.(3.26)\n",
      "As we will see, this simple univariate regression provides t he building block\n",
      "for multiple linear regression. Suppose next that the input sx1,x2,...,xp\n",
      "(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\n",
      "for allj̸=k. Then it is easy to check that the multiple least squares esti -\n",
      "matesˆβjare equal to⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\n",
      "words, when the inputs are orthogonal, they have no eﬀect on e ach other’s\n",
      "parameter estimates in the model.\n",
      "Orthogonal inputs occur most often with balanced, designed experiments\n",
      "(where orthogonality is enforced), but almost never with ob servational\n",
      "data. Hence we will have to orthogonalize them in order to car ry this idea\n",
      "further. Suppose next that we have an intercept and a single i nputx. Then\n",
      "the least squares coeﬃcient of xhas the form\n",
      "ˆβ1=⟨x−¯x1,y⟩\n",
      "⟨x−¯x1,x−¯x1⟩, (3.27)\n",
      "where ¯x=∑\n",
      "ixi/N, and1=x0, the vector of Nones. We can view the\n",
      "estimate (3.27) as the result of two applications of the simp le regression\n",
      "(3.26). The steps are:\n",
      "1. regress xon1to produce the residual z=x−¯x1;\n",
      "2. regress yon the residual zto give the coeﬃcient ˆβ1.\n",
      "Inthisprocedure,“regress bona”meansasimpleunivariateregressionof b\n",
      "onawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\n",
      "vectorb−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\n",
      "respect to a.\n",
      "Step 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\n",
      "univariate regression, using the orthogonal predictors 1andz. Figure 3.4\n",
      "shows this process for two general inputs x1andx2. The orthogonalization\n",
      "does not change the subspace spanned by x1andx2, it simply produces an\n",
      "orthogonal basis for representing it.\n",
      "This recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\n",
      "Note that the inputs z0,...,zj−1in step 2 are orthogonal, hence the simple\n",
      "regression coeﬃcients computed there are in fact also the mu ltiple regres-\n",
      "sion coeﬃcients.\n",
      "1The inner-product notation is suggestive of generalizations of linea r regression to\n",
      "diﬀerent metric spaces, as well as to probability spaces.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"54 3. Linear Methods for Regression\n",
      "x1x2y\n",
      "ˆ yz z z z z\n",
      "FIGURE 3.4. Least squares regression by orthogonalization of the inputs . The\n",
      "vectorx2is regressed on the vector x1, leaving the residual vector z. The regres-\n",
      "sion ofyonzgives the multiple regression coeﬃcient of x2. Adding together the\n",
      "projections of yon each of x1andzgives the least squares ﬁt ˆy.\n",
      "Algorithm 3.1 Regression by Successive Orthogonalization.\n",
      "1. Initialize z0=x0=1.\n",
      "2. Forj= 1,2,...,p\n",
      "Regress xjonz0,z1,...,,zj−1to produce coeﬃcients ˆ γℓj=\n",
      "⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,...,j−1 and residual vector zj=\n",
      "xj−∑j−1\n",
      "k=0ˆγkjzk.\n",
      "3. Regress yon the residual zpto give the estimate ˆβp.\n",
      "The result of this algorithm is\n",
      "ˆβp=⟨zp,y⟩\n",
      "⟨zp,zp⟩. (3.28)\n",
      "Re-arranging the residual in step 2, we can see that each of th exjis a linear\n",
      "combination of the zk, k≤j. Since the zjare all orthogonal, they form\n",
      "a basis for the column space of X, and hence the least squares projection\n",
      "onto this subspace is ˆy. Sincezpalone involves xp(with coeﬃcient 1), we\n",
      "see that the coeﬃcient (3.28) is indeed the multiple regress ion coeﬃcient of\n",
      "yonxp. This key result exposes the eﬀect of correlated inputs in mu ltiple\n",
      "regression. Note also that by rearranging the xj, any one of them could\n",
      "be in the last position, and a similar results holds. Hence st ated more\n",
      "generally, we have shown that the jth multiple regression coeﬃcient is the\n",
      "univariate regression coeﬃcient of yonxj·012...(j−1)(j+1)...,p, the residual\n",
      "after regressing xjonx0,x1,...,xj−1,xj+1,...,xp:\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2 Linear Regression Models and Least Squares 55\n",
      "The multiple regression coeﬃcient ˆβjrepresents the additional\n",
      "contribution of xjony, afterxjhas been adjusted for x0,x1,...,xj−1,\n",
      "xj+1,...,xp.\n",
      "Ifxpis highly correlated with some of the other xk’s, the residual vector\n",
      "zpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\n",
      "unstable. This will be true for all the variables in the corre lated set. In\n",
      "such situations, we might have all the Z-scores (as in Table 3 .2) be small—\n",
      "any one of the set can be deleted—yet we cannot delete them all. From\n",
      "(3.28) we also obtain an alternate formula for the variance e stimates (3.8),\n",
      "Var(ˆβp) =σ2\n",
      "⟨zp,zp⟩=σ2\n",
      "∥zp∥2. (3.29)\n",
      "In other words, the precision with which we can estimate ˆβpdepends on\n",
      "the length of the residual vector zp; this represents how much of xpis\n",
      "unexplained by the other xk’s.\n",
      "Algorithm 3.1 is known as the Gram–Schmidt procedure for multiple\n",
      "regression, and is also a useful numerical strategy for comp uting the esti-\n",
      "mates. We can obtain from it not just ˆβp, but also the entire multiple least\n",
      "squares ﬁt, as shown in Exercise 3.4.\n",
      "We can represent step 2 of Algorithm 3.1 in matrix form:\n",
      "X=ZΓ, (3.30)\n",
      "whereZhas as columns the zj(in order), and Γis the upper triangular ma-\n",
      "trix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\n",
      "entryDjj=∥zj∥, we get\n",
      "X=ZD−1DΓ\n",
      "=QR, (3.31)\n",
      "the so-called QRdecomposition of X. HereQis anN×(p+1) orthogonal\n",
      "matrix,QTQ=I, andRis a (p+1)×(p+1) upper triangular matrix.\n",
      "TheQRdecomposition represents a convenient orthogonal basis fo r the\n",
      "column space of X. It is easy to see, for example, that the least squares\n",
      "solution is given by\n",
      "ˆβ=R−1QTy, (3.32)\n",
      "ˆy=QQTy. (3.33)\n",
      "Equation (3.32) is easy to solve because Ris upper triangular\n",
      "(Exercise 3.4).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"56 3. Linear Methods for Regression\n",
      "3.2.4 Multiple Outputs\n",
      "Suppose we have multiple outputs Y1,Y2,...,Y Kthat we wish to predict\n",
      "from our inputs X0,X1,X2,...,X p. We assume a linear model for each\n",
      "output\n",
      "Yk=β0k+p∑\n",
      "j=1Xjβjk+εk (3.34)\n",
      "=fk(X)+εk. (3.35)\n",
      "WithNtraining cases we can write the model in matrix notation\n",
      "Y=XB+E. (3.36)\n",
      "HereYis theN×Kresponse matrix, with ikentryyik,Xis theN×(p+1)\n",
      "input matrix, Bis the (p+ 1)×Kmatrix of parameters and Eis the\n",
      "N×Kmatrix of errors. A straightforward generalization of the u nivariate\n",
      "loss function (3.2) is\n",
      "RSS(B) =K∑\n",
      "k=1N∑\n",
      "i=1(yik−fk(xi))2(3.37)\n",
      "= tr[(Y−XB)T(Y−XB)]. (3.38)\n",
      "The least squares estimates have exactly the same form as bef ore\n",
      "ˆB= (XTX)−1XTY. (3.39)\n",
      "Hence the coeﬃcients for the kth outcome are just the least squares es-\n",
      "timates in the regression of ykonx0,x1,...,xp. Multiple outputs do not\n",
      "aﬀect one another’s least squares estimates.\n",
      "If the errors ε= (ε1,...,ε K) in (3.34) are correlated, then it might seem\n",
      "appropriate to modify (3.37) in favor of a multivariate vers ion. Speciﬁcally,\n",
      "suppose Cov( ε) =Σ, then the multivariate weighted criterion\n",
      "RSS(B;Σ) =N∑\n",
      "i=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\n",
      "arises naturally from multivariate Gaussian theory. Here f(x) is the vector\n",
      "function (f1(x),...,f K(x))T, andyithe vector of Kresponses for obser-\n",
      "vationi. However, it can be shown that again the solution is given by\n",
      "(3.39);Kseparate regressions that ignore the correlations (Exerci se 3.11).\n",
      "If theΣivary among observations, then this is no longer the case, and the\n",
      "solution for Bno longer decouples.\n",
      "In Section 3.7 we pursue the multiple outcome problem, and co nsider\n",
      "situations where it does pay to combine the regressions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.3 Subset Selection 57\n",
      "3.3 Subset Selection\n",
      "There are two reasons why we are often not satisﬁed with the le ast squares\n",
      "estimates (3.6).\n",
      "•The ﬁrst is prediction accuracy : the least squares estimates often have\n",
      "low bias but large variance. Prediction accuracy can someti mes be\n",
      "improved by shrinking or setting some coeﬃcients to zero. By doing\n",
      "sowesacriﬁcealittlebitofbiastoreducethevarianceofth epredicted\n",
      "values, and hence may improve the overall prediction accura cy.\n",
      "•The second reason is interpretation . With a large number of predic-\n",
      "tors, we often would like to determine a smaller subset that e xhibit\n",
      "the strongest eﬀects. In order to get the “big picture,” we ar e willing\n",
      "to sacriﬁce some of the small details.\n",
      "In this section we describe a number of approaches to variabl e subset selec-\n",
      "tionwithlinearregression.Inlatersectionswediscusssh rinkageandhybrid\n",
      "approaches for controlling variance, as well as other dimen sion-reduction\n",
      "strategies. These all fall under the general heading model selection . Model\n",
      "selection is not restricted to linear models; Chapter 7 cove rs this topic in\n",
      "some detail.\n",
      "With subset selection we retain only a subset of the variable s, and elim-\n",
      "inate the rest from the model. Least squares regression is us ed to estimate\n",
      "the coeﬃcients of the inputs that are retained. There are a nu mber of dif-\n",
      "ferent strategies for choosing the subset.\n",
      "3.3.1 Best-Subset Selection\n",
      "Best subset regression ﬁnds for each k∈{0,1,2,...,p}the subset of size k\n",
      "that gives smallest residual sum of squares (3.2). An eﬃcien t algorithm—\n",
      "theleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\n",
      "feasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\n",
      "for the prostate cancer example. The lower boundary represe nts the models\n",
      "that are eligible for selection by the best-subsets approac h. Note that the\n",
      "best subset of size 2, for example, need not include the varia ble that was\n",
      "in the best subset of size 1 (for this example all the subsets a re nested).\n",
      "The best-subset curve (red lower boundary in Figure 3.5) is n ecessarily\n",
      "decreasing, so cannot be used to select the subset size k. The question of\n",
      "how to choose kinvolves the tradeoﬀ between bias and variance, along with\n",
      "the more subjective desire for parsimony. There are a number of criteria\n",
      "that one may use; typically we choose the smallest model that minimizes\n",
      "an estimate of the expected prediction error.\n",
      "Many of the other approaches that we discuss in this chapter a re similar,\n",
      "in that they use the training data to produce a sequence of mod els varying\n",
      "in complexity and indexed by a single parameter. In the next s ection we use\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"58 3. Linear Methods for Regression\n",
      "Subset Size kResidual Sum−of−Squares\n",
      "0 20 40 60 80 100\n",
      "0 1 2 3 4 5 6 7 8•\n",
      "••••••••\n",
      "••••••••••••••••••••••••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••••\n",
      "••••••••••••••••••••••••••\n",
      "••••••••\n",
      "••\n",
      "•\n",
      "•••••• •\n",
      "FIGURE 3.5. All possible subset models for the prostate cancer example. At\n",
      "each subset size is shown the residual sum-of-squares for ea ch model of that size.\n",
      "cross-validation to estimate prediction error and select k; the AIC criterion\n",
      "is a popular alternative. We defer more detailed discussion of these and\n",
      "other approaches to Chapter 7.\n",
      "3.3.2 Forward- and Backward-Stepwise Selection\n",
      "Rather than search through all possible subsets (which beco mes infeasible\n",
      "forpmuchlargerthan40),wecanseekagoodpaththroughthem. Forward-\n",
      "stepwise selection starts with the intercept, and then sequentially adds into\n",
      "the model the predictor that most improves the ﬁt. With many c andidate\n",
      "predictors, this might seem like a lot of computation; howev er, clever up-\n",
      "dating algorithms can exploit the QR decomposition for the c urrent ﬁt to\n",
      "rapidly establish the next candidate (Exercise 3.9). Like b est-subset re-\n",
      "gression, forward stepwise produces a sequence of models in dexed byk, the\n",
      "subset size, which must be determined.\n",
      "Forward-stepwise selection is a greedy algorithm , producing a nested se-\n",
      "quence of models. In this sense it might seem sub-optimal com pared to\n",
      "best-subset selection. However, there are several reasons why it might be\n",
      "preferred:\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.3 Subset Selection 59\n",
      "•Computational; for largepwe cannot compute the best subset se-\n",
      "quence, but we can always compute the forward stepwise seque nce\n",
      "(even when p≫N).\n",
      "•Statistical; a price is paid in variance for selecting the best subset\n",
      "of each size; forward stepwise is a more constrained search, and will\n",
      "have lower variance, but perhaps more bias.\n",
      "0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\n",
      "Forward Stepwise\n",
      "Backward Stepwise\n",
      "Forward StagewiseE||ˆβ(k)−β||2\n",
      "Subset Size k\n",
      "FIGURE 3.6. Comparison of four subset-selection techniques on a simulate d lin-\n",
      "ear regression problem Y=XTβ+ε. There are N= 300observations on p= 31\n",
      "standard Gaussian variables, with pairwise correlations all e qual to0.85. For10of\n",
      "the variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\n",
      "the rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\n",
      "0.64. Results are averaged over 50simulations. Shown is the mean-squared error\n",
      "of the estimated coeﬃcient ˆβ(k)at each step from the true β.\n",
      "Backward-stepwise selection starts with the full model, and sequentially\n",
      "deletes the predictor that has the least impact on the ﬁt. The candidate for\n",
      "droppingisthevariablewiththesmallestZ-score(Exercis e3.10).Backward\n",
      "selection can only be used when N >p, while forward stepwise can always\n",
      "be used.\n",
      "Figure 3.6 shows the results of a small simulation study to co mpare\n",
      "best-subset regression with the simpler alternatives forw ard and backward\n",
      "selection. Their performance is very similar, as is often th e case. Included in\n",
      "the ﬁgure is forward stagewise regression (next section), w hich takes longer\n",
      "to reach minimum error.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"60 3. Linear Methods for Regression\n",
      "On the prostate cancer example, best-subset, forward and ba ckward se-\n",
      "lection all gave exactly the same sequence of terms.\n",
      "Some software packages implement hybrid stepwise-selecti on strategies\n",
      "that consider both forward and backward moves at each step, a nd select\n",
      "the “best” of the two. For example in the Rpackage the stepfunction uses\n",
      "the AIC criterion for weighing the choices, which takes prop er account of\n",
      "the number of parameters ﬁt; at each step an add or drop will be performed\n",
      "that minimizes the AIC score.\n",
      "Othermoretraditionalpackagesbasetheselectionon F-statistics,adding\n",
      "“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\n",
      "of fashion, since they do not take proper account of the multi ple testing\n",
      "issues. It is also tempting after a model search to print out a summary of\n",
      "the chosen model, such as in Table 3.2; however, the standard errors are\n",
      "not valid, since they do not account for the search process. T he bootstrap\n",
      "(Section 8.2) can be useful in such settings.\n",
      "Finally, we note that often variables come in groups (such as the dummy\n",
      "variables that code a multi-level categorical predictor). Smart stepwise pro-\n",
      "cedures (such as stepinR) will add or drop whole groups at a time, taking\n",
      "proper account of their degrees-of-freedom.\n",
      "3.3.3 Forward-Stagewise Regression\n",
      "Forward-stagewise regression (FS) is even more constraine d than forward-\n",
      "stepwise regression. It starts like forward-stepwise regr ession, with an in-\n",
      "tercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\n",
      "At each step the algorithm identiﬁes the variable most corre lated with the\n",
      "current residual. It then computes the simple linear regres sion coeﬃcient\n",
      "of the residual on this chosen variable, and then adds it to th e current co-\n",
      "eﬃcient for that variable. This is continued till none of the variables have\n",
      "correlation with the residuals—i.e. the least-squares ﬁt wh enN >p.\n",
      "Unlike forward-stepwise regression, none of the other vari ables are ad-\n",
      "justed when a term is added to the model. As a consequence, for ward\n",
      "stagewise can take many more than psteps to reach the least squares ﬁt,\n",
      "and historically has been dismissed as being ineﬃcient. It t urns out that\n",
      "this “slow ﬁtting” can pay dividends in high-dimensional pr oblems. We\n",
      "see in Section 3.8.1 that both forward stagewise and a varian t which is\n",
      "slowed down even further are quite competitive, especially in very high-\n",
      "dimensional problems.\n",
      "Forward-stagewise regression is included in Figure 3.6. In this example it\n",
      "takes over 1000 steps to get all the correlations below 10−4. For subset size\n",
      "k, we plotted the error for the last step for which there where knonzero\n",
      "coeﬃcients. Although it catches up with the best ﬁt, it takes longer to\n",
      "do so.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 61\n",
      "3.3.4 Prostate Cancer Data Example (Continued)\n",
      "Table 3.3 shows the coeﬃcients from a number of diﬀerent sele ction and\n",
      "shrinkagemethods.Theyare best-subset selection usinganall-subsetssearch,\n",
      "ridge regression , thelasso,principal components regression andpartial least\n",
      "squares. Each method has a complexity parameter, and this was chosen to\n",
      "minimize an estimate of prediction error based on tenfold cr oss-validation;\n",
      "fulldetailsaregiveninSection7.10.Brieﬂy,cross-valid ationworksbydivid-\n",
      "ing the training data randomly into ten equal parts. The lear ning method\n",
      "is ﬁt—for a range of values of the complexity parameter—to nine -tenths of\n",
      "the data, and the prediction error is computed on the remaini ng one-tenth.\n",
      "This is done in turn for each one-tenth of the data, and the ten prediction\n",
      "error estimates are averaged. From this we obtain an estimat ed prediction\n",
      "error curve as a function of the complexity parameter.\n",
      "Note that we have already divided these data into a training s et of size\n",
      "67 and a test set of size 30. Cross-validation is applied to th e training set,\n",
      "since selecting the shrinkage parameter is part of the train ing process. The\n",
      "test set is there to judge the performance of the selected mod el.\n",
      "The estimated prediction error curves are shown in Figure 3. 7. Many of\n",
      "the curves are very ﬂat over large ranges near their minimum. Included\n",
      "are estimated standard error bands for each estimated error rate, based on\n",
      "the ten error estimates computed by cross-validation. We ha ve used the\n",
      "“one-standard-error” rule—we pick the most parsimonious mo del within\n",
      "one standard error of the minimum (Section 7.10, page 244). S uch a rule\n",
      "acknowledges the fact that the tradeoﬀ curve is estimated wi th error, and\n",
      "hence takes a conservative approach.\n",
      "Best-subset selection chose to use the two predictors lcvolandlweight.\n",
      "The last two lines of the table give the average prediction er ror (and its\n",
      "estimated standard error) over the test set.\n",
      "3.4 Shrinkage Methods\n",
      "By retaining a subset of the predictors and discarding the re st, subset selec-\n",
      "tion produces a model that is interpretable and has possibly lower predic-\n",
      "tion error than the full model. However, because it is a discr ete process—\n",
      "variables are either retained or discarded—it often exhibit s high variance,\n",
      "and so doesn’t reduce the prediction error of the full model. Shrinkage\n",
      "methods are more continuous, and don’t suﬀer as much from hig h\n",
      "variability.\n",
      "3.4.1 Ridge Regression\n",
      "Ridge regression shrinks the regression coeﬃcients by impo sing a penalty\n",
      "on their size. The ridge coeﬃcients minimize a penalized res idual sum of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"62 3. Linear Methods for Regression\n",
      "Subset SizeCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•••••••All Subsets\n",
      "Degrees of FreedomCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•\n",
      "••••••Ridge Regression\n",
      "Shrinkage Factor sCV Error\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•\n",
      "••••••Lasso\n",
      "Number of DirectionsCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "••••••••Principal Components Regression\n",
      "Number of  DirectionsCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "••••••• •Partial Least Squares\n",
      "FIGURE 3.7. Estimated prediction error curves and their standard error s for\n",
      "the various selection and shrinkage methods. Each curve is plo tted as a function\n",
      "of the corresponding complexity parameter for that method. T he horizontal axis\n",
      "has been chosen so that the model complexity increases as we mo ve from left to\n",
      "right. The estimates of prediction error and their standard errors were obtained by\n",
      "tenfold cross-validation; full details are given in Section 7. 10. The least complex\n",
      "model within one standard error of the best is chosen, indica ted by the purple\n",
      "vertical broken lines.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 63\n",
      "TABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent su bset\n",
      "and shrinkage methods applied to the prostate data. The blank e ntries correspond\n",
      "to variables omitted.\n",
      "Term LS Best Subset Ridge Lasso PCR PLS\n",
      "Intercept 2.465 2.477 2.452 2.468 2.497 2.452\n",
      "lcavol 0.680 0.740 0.420 0.533 0.543 0.419\n",
      "lweight 0.263 0.316 0.238 0.169 0.289 0.344\n",
      "age−0.141 −0.046 −0.152−0.026\n",
      "lbph 0.210 0.162 0.002 0.214 0.220\n",
      "svi0.305 0.227 0.094 0.315 0.243\n",
      "lcp−0.288 0.000 −0.051 0.079\n",
      "gleason −0.021 0.040 0.232 0.011\n",
      "pgg45 0.267 0.133 −0.056 0.084\n",
      "Test Error 0.521 0.492 0.492 0.479 0.449 0.528\n",
      "Std Error 0.179 0.143 0.165 0.164 0.105 0.152\n",
      "squares,\n",
      "ˆβridge= argmin\n",
      "β{N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1β2\n",
      "j}\n",
      ".(3.41)\n",
      "Hereλ≥0 is a complexity parameter that controls the amount of shrin k-\n",
      "age: the larger the value of λ, the greater the amount of shrinkage. The\n",
      "coeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\n",
      "ing by the sum-of-squares of the parameters is also used in ne ural networks,\n",
      "where it is known as weight decay (Chapter 11).\n",
      "An equivalent way to write the ridge problem is\n",
      "ˆβridge= argmin\n",
      "βN∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ",\n",
      "subject top∑\n",
      "j=1β2\n",
      "j≤t,(3.42)\n",
      "which makes explicit the size constraint on the parameters. There is a one-\n",
      "to-one correspondence between the parameters λin (3.41) and tin (3.42).\n",
      "When there are many correlated variables in a linear regress ion model,\n",
      "their coeﬃcients can become poorly determined and exhibit h igh variance.\n",
      "A wildly large positive coeﬃcient on one variable can be canc eled by a\n",
      "similarly large negative coeﬃcient on its correlated cousi n. By imposing a\n",
      "size constraint on the coeﬃcients, as in (3.42), this proble m is alleviated.\n",
      "The ridge solutions are not equivariant under scaling of the inputs, and\n",
      "so one normally standardizes the inputs before solving (3.4 1). In addition,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"64 3. Linear Methods for Regression\n",
      "notice that the intercept β0has been left out of the penalty term. Penal-\n",
      "ization of the intercept would make the procedure depend on t he origin\n",
      "chosen forY; that is, adding a constant cto each of the targets yiwould\n",
      "not simply result in a shift of the predictions by the same amo untc. It\n",
      "can be shown (Exercise 3.5) that the solution to (3.41) can be separated\n",
      "into two parts, after reparametrization using centered inputs: each xijgets\n",
      "replaced by xij−¯xj. We estimate β0by ¯y=1\n",
      "N∑N\n",
      "1yi. The remaining co-\n",
      "eﬃcients get estimated by a ridge regression without interc ept, using the\n",
      "centeredxij. Henceforth we assume that this centering has been done, so\n",
      "that the input matrix Xhasp(rather than p+1) columns.\n",
      "Writing the criterion in (3.41) in matrix form,\n",
      "RSS(λ) = (y−Xβ)T(y−Xβ)+λβTβ, (3.43)\n",
      "the ridge regression solutions are easily seen to be\n",
      "ˆβridge= (XTX+λI)−1XTy, (3.44)\n",
      "whereIis thep×pidentity matrix. Notice that with the choice of quadratic\n",
      "penaltyβTβ, the ridge regression solution is again a linear function of\n",
      "y. The solution adds a positive constant to the diagonal of XTXbefore\n",
      "inversion. This makes the problem nonsingular, even if XTXis not of full\n",
      "rank, and was the main motivation for ridge regression when i t was ﬁrst\n",
      "introducedinstatistics(HoerlandKennard,1970).Tradit ionaldescriptions\n",
      "of ridge regression start with deﬁnition (3.44). We choose t o motivate it via\n",
      "(3.41) and (3.42), as these provide insight into how it works .\n",
      "Figure 3.8 shows the ridge coeﬃcient estimates for the prost ate can-\n",
      "cer example, plotted as functions of df( λ), theeﬀective degrees of freedom\n",
      "implied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\n",
      "thonormal inputs, the ridge estimates are just a scaled vers ion of the least\n",
      "squares estimates, that is, ˆβridge=ˆβ/(1+λ).\n",
      "Ridge regression can also be derived as the mean or mode of a po ste-\n",
      "rior distribution, with a suitably chosen prior distributi on. In detail, sup-\n",
      "poseyi∼N(β0+xT\n",
      "iβ,σ2), and the parameters βjare each distributed as\n",
      "N(0,τ2), independently of one another. Then the (negative) log-po sterior\n",
      "density ofβ, withτ2andσ2assumed known, is equal to the expression\n",
      "in curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge\n",
      "estimate is the mode of the posterior distribution; since th e distribution is\n",
      "Gaussian, it is also the posterior mean.\n",
      "Thesingular value decomposition (SVD) of the centered input matrix X\n",
      "gives us some additional insight into the nature of ridge reg ression. This de-\n",
      "composition is extremely useful in the analysis of many stat istical methods.\n",
      "The SVD of the N×pmatrixXhas the form\n",
      "X=UDVT. (3.45)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 65Coefficients\n",
      "0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\n",
      "•••••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "••••••\n",
      "•lcavol\n",
      "••••••••••••••••••••••••\n",
      "•lweight\n",
      "•••••••••••••••••••••••••\n",
      "age••••••••••••••••••••••••\n",
      "•lbph••••••••••••••••••••••••\n",
      "•svi\n",
      "••••••••••••••••••••••••\n",
      "•\n",
      "lcp••••••••••••••••••••••••\n",
      "•gleason•\n",
      "•••••••••••••••••••••••\n",
      "•pgg45\n",
      "df(λ)\n",
      "FIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, a s\n",
      "the tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective\n",
      "degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by\n",
      "cross-validation.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"66 3. Linear Methods for Regression\n",
      "HereUandVareN×pandp×porthogonal matrices, with the columns\n",
      "ofUspanning the column space of X, and the columns of Vspanning the\n",
      "row space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\n",
      "···≥dp≥0 called the singular values of X. If one or more values dj= 0,\n",
      "Xis singular.\n",
      "Using the singular value decomposition we can write the leas t squares\n",
      "ﬁtted vector as\n",
      "Xˆβls=X(XTX)−1XTy\n",
      "=UUTy, (3.46)\n",
      "after some simpliﬁcation. Note that UTyare the coordinates of ywith\n",
      "respect to the orthonormal basis U. Note also the similarity with (3.33);\n",
      "QandUare generally diﬀerent orthogonal bases for the column spac e of\n",
      "X(Exercise 3.8).\n",
      "Now the ridge solutions are\n",
      "Xˆβridge=X(XTX+λI)−1XTy\n",
      "=U D(D2+λI)−1D UTy\n",
      "=p∑\n",
      "j=1ujd2\n",
      "j\n",
      "d2\n",
      "j+λuT\n",
      "jy, (3.47)\n",
      "where the ujare the columns of U. Note that since λ≥0, we haved2\n",
      "j/(d2\n",
      "j+\n",
      "λ)≤1. Like linear regression, ridge regression computes the co ordinates of\n",
      "ywithrespecttotheorthonormalbasis U.Itthenshrinksthesecoordinates\n",
      "by the factors d2\n",
      "j/(d2\n",
      "j+λ). This means that a greater amount of shrinkage\n",
      "is applied to the coordinates of basis vectors with smaller d2\n",
      "j.\n",
      "What does a small value of d2\n",
      "jmean? The SVD of the centered matrix\n",
      "Xis another way of expressing the principal components of the variables\n",
      "inX. The sample covariance matrix is given by S=XTX/N, and from\n",
      "(3.45) we have\n",
      "XTX=VD2VT, (3.48)\n",
      "which is the eigen decomposition ofXTX(and ofS, up to a factor N).\n",
      "The eigenvectors vj(columns of V) are also called the principal compo-\n",
      "nents(or Karhunen–Loeve) directions of X. The ﬁrst principal component\n",
      "directionv1has the property that z1=Xv1has the largest sample vari-\n",
      "ance amongst all normalized linear combinations of the colu mns ofX. This\n",
      "sample variance is easily seen to be\n",
      "Var(z1) = Var(Xv1) =d2\n",
      "1\n",
      "N, (3.49)\n",
      "and in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\n",
      "principal component of X, and hence u1is the normalized ﬁrst principal\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 67\n",
      "-4 -2 0 2 4-4 -2 0 2 4ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooLargest Principal\n",
      "Component\n",
      "Smallest Principal\n",
      "Component\n",
      "X1X2\n",
      "FIGURE 3.9. Principal components of some input data points. The largest p rin-\n",
      "cipal component is the direction that maximizes the varianc e of the projected data,\n",
      "and the smallest principal component minimizes that varianc e. Ridge regression\n",
      "projects yonto these components, and then shrinks the coeﬃcients of th e low–\n",
      "variance components more than the high-variance component s.\n",
      "component. Subsequent principal components zjhave maximum variance\n",
      "d2\n",
      "j/N, subject to being orthogonal to the earlier ones. Conversel y the last\n",
      "principal component has minimum variance. Hence the small singular val-\n",
      "uesdjcorrespond to directions in the column space of Xhaving small\n",
      "variance, and ridge regression shrinks these directions th e most.\n",
      "Figure 3.9 illustrates the principal components of some dat a points in\n",
      "two dimensions. If we consider ﬁtting a linear surface over t his domain\n",
      "(theY-axis is sticking out of the page), the conﬁguration of the da ta allow\n",
      "us to determine its gradient more accurately in the long dire ction than\n",
      "the short. Ridge regression protects against the potential ly high variance\n",
      "of gradients estimated in the short directions. The implici t assumption is\n",
      "that the response will tend to vary most in the directions of h igh variance\n",
      "of the inputs. This is often a reasonable assumption, since p redictors are\n",
      "often chosen for study because they vary with the response va riable, but\n",
      "need not hold in general.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"68 3. Linear Methods for Regression\n",
      "In Figure 3.7 we have plotted the estimated prediction error versus the\n",
      "quantity\n",
      "df(λ) = tr[ X(XTX+λI)−1XT],\n",
      "= tr(Hλ)\n",
      "=p∑\n",
      "j=1d2\n",
      "j\n",
      "d2\n",
      "j+λ. (3.50)\n",
      "This monotone decreasing function of λis theeﬀective degrees of freedom\n",
      "of the ridge regression ﬁt. Usually in a linear-regression ﬁ t withpvariables,\n",
      "the degrees-of-freedom of the ﬁt is p, the number of free parameters. The\n",
      "idea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they\n",
      "are ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\n",
      "λ= 0 (no regularization) and df( λ)→0 asλ→ ∞. Of course there\n",
      "is always an additional one degree of freedom for the interce pt, which was\n",
      "removed apriori. This deﬁnition is motivated in more detail in Section 3.4.4\n",
      "and Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5.0.\n",
      "Table 3.3 shows that ridge regression reduces the test error of the full least\n",
      "squares estimates by a small amount.\n",
      "3.4.2 The Lasso\n",
      "The lasso is a shrinkage method like ridge, with subtle but im portant dif-\n",
      "ferences. The lasso estimate is deﬁned by\n",
      "ˆβlasso= argmin\n",
      "βN∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      "subject top∑\n",
      "j=1|βj|≤t. (3.51)\n",
      "Just as in ridge regression, we can re-parametrize the const antβ0by stan-\n",
      "dardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\n",
      "model without an intercept (Exercise 3.5). In the signal pro cessing litera-\n",
      "ture, the lasso is also known as basis pursuit (Chen et al., 1998).\n",
      "We can also write the lasso problem in the equivalent Lagrangian form\n",
      "ˆβlasso= argmin\n",
      "β{1\n",
      "2N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1|βj|}\n",
      ".(3.52)\n",
      "Notice the similarity to the ridge regression problem (3.42 ) or (3.41): the\n",
      "L2ridge penalty∑p\n",
      "1β2\n",
      "jis replaced by the L1lasso penalty∑p\n",
      "1|βj|. This\n",
      "latter constraint makes the solutions nonlinear in the yi, and there is no\n",
      "closed form expression as in ridge regression. Computing th e lasso solution\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 69\n",
      "is a quadratic programming problem, although we see in Secti on 3.4.4 that\n",
      "eﬃcient algorithms are available for computing the entire p ath of solutions\n",
      "asλis varied, with the same computational cost as for ridge regr ession.\n",
      "Because of the nature of the constraint, making tsuﬃciently small will\n",
      "cause some of the coeﬃcients to be exactly zero. Thus the lass o does a kind\n",
      "of continuous subset selection. If tis chosen larger than t0=∑p\n",
      "1|ˆβj|(where\n",
      "ˆβj=ˆβls\n",
      "j, the least squares estimates), then the lasso estimates are theˆβj’s.\n",
      "On the other hand, for t=t0/2 say, then the least squares coeﬃcients are\n",
      "shrunk by about 50% on average. However, the nature of the shr inkage\n",
      "is not obvious, and we investigate it further in Section 3.4. 4 below. Like\n",
      "the subset size in variable subset selection, or the penalty parameter in\n",
      "ridge regression, tshould be adaptively chosen to minimize an estimate of\n",
      "expected prediction error.\n",
      "In Figure 3.7, for ease of interpretation, we have plotted th e lasso pre-\n",
      "diction error estimates versus the standardized parameter s=t/∑p\n",
      "1|ˆβj|.\n",
      "A value ˆs≈0.36 was chosen by 10-fold cross-validation; this caused four\n",
      "coeﬃcients to be set to zero (ﬁfth column of Table 3.3). The re sulting\n",
      "model has the second lowest test error, slightly lower than t he full least\n",
      "squares model, but the standard errors of the test error esti mates (last line\n",
      "of Table 3.3) are fairly large.\n",
      "Figure 3.10 shows the lasso coeﬃcients as the standardized t uning pa-\n",
      "rameters=t/∑p\n",
      "1|ˆβj|is varied. At s= 1.0 these are the least squares\n",
      "estimates; they decrease to 0 as s→0. This decrease is not always strictly\n",
      "monotonic, although it is in this example. A vertical line is drawn at\n",
      "s= 0.36, the value chosen by cross-validation.\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression and t he\n",
      "Lasso\n",
      "Inthissectionwediscussandcomparethethreeapproachesd iscussedsofar\n",
      "for restricting the linear regression model: subset select ion, ridge regression\n",
      "and the lasso.\n",
      "In the case of an orthonormal input matrix Xthe three procedures have\n",
      "explicit solutions. Each method applies a simple transform ation to the least\n",
      "squares estimate ˆβj, as detailed in Table 3.4.\n",
      "Ridge regression does a proportional shrinkage. Lasso tran slates each\n",
      "coeﬃcient by a constant factor λ, truncating at zero. This is called “soft\n",
      "thresholding,”andisusedinthecontextofwavelet-baseds moothinginSec-\n",
      "tion 5.9. Best-subset selection drops all variables with co eﬃcients smaller\n",
      "than theMth largest; this is a form of “hard-thresholding.”\n",
      "Back to the nonorthogonal case; some pictures help understa nd their re-\n",
      "lationship. Figure 3.11 depicts the lasso (left) and ridge r egression (right)\n",
      "when there are only two parameters. The residual sum of squar es has ellip-\n",
      "tical contours, centered at the full least squares estimate . The constraint\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"70 3. Linear Methods for Regression\n",
      "0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\n",
      "Shrinkage Factor sCoefficientslcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "FIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\n",
      "Coeﬃcients are plotted versus s=t/∑p\n",
      "1|ˆβj|. A vertical line is drawn at s= 0.36,\n",
      "the value chosen by cross-validation. Compare Figure 3.8 on pa ge 65; the lasso\n",
      "proﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,\n",
      "and so are computed only at the points displayed; see Section 3. 4.4 for details.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 71\n",
      "TABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\n",
      "are constants chosen by the corresponding techniques; signdenotes the sign of its\n",
      "argument ( ±1), andx+denotes “positive part” of x. Below the table, estimators\n",
      "are shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\n",
      "for reference.\n",
      "Estimator Formula\n",
      "Best subset (size M)ˆβj·I(|ˆβj|≥|ˆβ(M)|)\n",
      "Ridge ˆβj/(1+λ)\n",
      "Lasso sign( ˆβj)(|ˆβj|−λ)+\n",
      "(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\n",
      "β^β^ 2. . β\n",
      "1β2\n",
      "β1β\n",
      "FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\n",
      "(right). Shown are contours of the error and constraint func tions. The solid blue\n",
      "areas are the constraint regions |β1|+|β2| ≤tandβ2\n",
      "1+β2\n",
      "2≤t2, respectively,\n",
      "while the red ellipses are the contours of the least squares erro r function.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"72 3. Linear Methods for Regression\n",
      "region for ridge regression is the disk β2\n",
      "1+β2\n",
      "2≤t, while that for lasso is\n",
      "the diamond|β1|+|β2|≤t. Both methods ﬁnd the ﬁrst point where the\n",
      "elliptical contours hit the constraint region. Unlike the d isk, the diamond\n",
      "has corners; if the solution occurs at a corner, then it has on e parameter\n",
      "βjequal to zero. When p>2, the diamond becomes a rhomboid, and has\n",
      "many corners, ﬂat edges and faces; there are many more opport unities for\n",
      "the estimated parameters to be zero.\n",
      "We can generalize ridge regression and the lasso, and view th em as Bayes\n",
      "estimates. Consider the criterion\n",
      "˜β= argmin\n",
      "β{N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1|βj|q}\n",
      "(3.53)\n",
      "forq≥0. The contours of constant value of∑\n",
      "j|βj|qare shown in Fig-\n",
      "ure 3.12, for the case of two inputs.\n",
      "Thinking of|βj|qas the log-prior density for βj, these are also the equi-\n",
      "contours of the prior distribution of the parameters. The va lueq= 0 corre-\n",
      "spondstovariablesubsetselection,asthepenaltysimplyc ountsthenumber\n",
      "of nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\n",
      "regression. Notice that for q≤1, the prior is not uniform in direction, but\n",
      "concentrates more mass in the coordinate directions. The pr ior correspond-\n",
      "ing to theq= 1 case is an independent double exponential (or Laplace)\n",
      "distribution for each input, with density (1 /2τ)exp(−|β|/τ) andτ= 1/λ.\n",
      "The caseq= 1 (lasso) is the smallest qsuch that the constraint region\n",
      "is convex; non-convex constraint regions make the optimiza tion problem\n",
      "more diﬃcult.\n",
      "In this view, the lasso, ridge regression and best subset sel ection are\n",
      "Bayes estimates with diﬀerent priors. Note, however, that t hey are derived\n",
      "as posterior modes, that is, maximizers of the posterior. It is more common\n",
      "to use the mean of the posterior as the Bayes estimate. Ridge r egression is\n",
      "also the posterior mean, but the lasso and best subset select ion are not.\n",
      "Looking again at the criterion (3.53), we might try using oth er values\n",
      "ofqbesides 0, 1, or 2. Although one might consider estimating qfrom\n",
      "the data, our experience is that it is not worth the eﬀort for t he extra\n",
      "variance incurred. Values of q∈(1,2) suggest a compromise between the\n",
      "lasso and ridge regression. Although this is the case, with q >1,|βj|qis\n",
      "diﬀerentiable at 0, and so does not share the ability of lasso (q= 1) for\n",
      "q= 4 q= 2 q= 1 q= 0.5 q= 0.1\n",
      "FIGURE 3.12. Contours of constant value of∑\n",
      "j|βj|qfor given values of q.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 73\n",
      "q= 1.2 α= 0.2\n",
      "Lq Elastic Net\n",
      "FIGURE 3.13. Contours of constant value of∑\n",
      "j|βj|qforq= 1.2(left plot),\n",
      "and the elastic-net penalty∑\n",
      "j(αβ2\n",
      "j+(1−α)|βj|)forα= 0.2(right plot). Although\n",
      "visually very similar, the elastic-net has sharp (non-diﬀeren tiable) corners, while\n",
      "theq= 1.2penalty does not.\n",
      "setting coeﬃcients exactly to zero. Partly for this reason a s well as for\n",
      "computational tractability, Zou and Hastie (2005) introdu ced theelastic-\n",
      "netpenalty\n",
      "λp∑\n",
      "j=1(\n",
      "αβ2\n",
      "j+(1−α)|βj|)\n",
      ", (3.54)\n",
      "a diﬀerent compromise between ridge and lasso. Figure 3.13 c ompares the\n",
      "Lqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\n",
      "hard to detect the diﬀerence by eye. The elastic-net selects variables like\n",
      "the lasso, and shrinks together the coeﬃcients of correlate d predictors like\n",
      "ridge. It also has considerable computational advantages o ver theLqpenal-\n",
      "ties. We discuss the elastic-net further in Section 18.4.\n",
      "3.4.4 Least Angle Regression\n",
      "Least angle regression (LAR) is a relative newcomer (Efron e t al., 2004),\n",
      "and can be viewed as a kind of “democratic” version of forward stepwise\n",
      "regression (Section 3.3.2). As we will see, LAR is intimatel y connected\n",
      "with the lasso, and in fact provides an extremely eﬃcient alg orithm for\n",
      "computing the entire lasso path as in Figure 3.10.\n",
      "Forward stepwise regression builds a model sequentially, a dding one vari-\n",
      "able at a time. At each step, it identiﬁes the best variable to include in the\n",
      "active set , and then updates the least squares ﬁt to include all the acti ve\n",
      "variables.\n",
      "Least angle regression uses a similar strategy, but only ent ers “as much”\n",
      "of a predictor as it deserves. At the ﬁrst step it identiﬁes th e variable\n",
      "most correlated with the response. Rather than ﬁt this varia ble completely,\n",
      "LAR moves the coeﬃcient of this variable continuously towar d its least-\n",
      "squares value (causing its correlation with the evolving re sidual to decrease\n",
      "in absolute value). As soon as another variable “catches up” in terms of\n",
      "correlation with the residual, the process is paused. The se cond variable\n",
      "then joins the active set, and their coeﬃcients are moved tog ether in a way\n",
      "that keeps their correlations tied and decreasing. This pro cess is continued\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"74 3. Linear Methods for Regression\n",
      "until all the variables are in the model, and ends at the full l east-squares\n",
      "ﬁt. Algorithm 3.2 provides the details. The termination con dition in step 5\n",
      "requires some explanation. If p>N−1, the LAR algorithm reaches a zero\n",
      "residual solution after N−1 steps (the−1 is because we have centered the\n",
      "data).\n",
      "Algorithm 3.2 Least Angle Regression.\n",
      "1. Standardize the predictors to have mean zero and unit norm . Start\n",
      "with the residual r=y−¯y,β1,β2,...,β p= 0.\n",
      "2. Find the predictor xjmost correlated with r.\n",
      "3. Moveβjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\n",
      "other competitor xkhas as much correlation with the current residual\n",
      "as doesxj.\n",
      "4. Moveβjandβkin the direction deﬁned by their joint least squares\n",
      "coeﬃcient of the current residual on ( xj,xk), until some other com-\n",
      "petitorxlhas as much correlation with the current residual.\n",
      "5. Continue in this way until all ppredictors have been entered. After\n",
      "min(N−1,p) steps, we arrive at the full least-squares solution.\n",
      "SupposeAkis the active set of variables at the beginning of the kth\n",
      "step, and let βAkbe the coeﬃcient vector for these variables at this step;\n",
      "there will be k−1 nonzero values, and the one just entered will be zero. If\n",
      "rk=y−XAkβAkis the current residual, then the direction for this step is\n",
      "δk= (XT\n",
      "AkXAk)−1XT\n",
      "Akrk. (3.55)\n",
      "The coeﬃcient proﬁle then evolves as βAk(α) =βAk+α·δk. Exercise 3.23\n",
      "veriﬁes that the directions chosen in this fashion do what is claimed: keep\n",
      "the correlations tied and decreasing. If the ﬁt vector at the beginning of\n",
      "this step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α·uk, whereuk=XAkδk\n",
      "is the new ﬁt direction. The name “least angle” arises from a g eometrical\n",
      "interpretation of this process; ukmakes the smallest (and equal) angle\n",
      "with each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\n",
      "absolute correlations decreasing and joining ranks with ea ch step of the\n",
      "LAR algorithm, using simulated data.\n",
      "By construction the coeﬃcients in LAR change in a piecewise l inear fash-\n",
      "ion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\n",
      "function of their L1arc length2. Note that we do not need to take small\n",
      "2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0,S] is given by TV( β,S) =∫S\n",
      "0||˙β(s)||1ds,where˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\n",
      "this amounts to summing the L1norms of the changes in coeﬃcients from step to step.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 75\n",
      "0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\n",
      "L1Arc LengthAbsolute Correlations\n",
      "FIGURE 3.14. Progression of the absolute correlations during each step of t he\n",
      "LAR procedure, using a simulated data set with six predictors . The labels at the\n",
      "top of the plot indicate which variables enter the active set at each step. The step\n",
      "length are measured in units of L1arc length.\n",
      "0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\n",
      "0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso\n",
      "L1Arc Length L1Arc Length\n",
      "CoeﬃcientsCoeﬃcients\n",
      "FIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\n",
      "data, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\n",
      "They are identical until the dark-blue coeﬃcient crosses zer o at an arc length of\n",
      "about18.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"76 3. Linear Methods for Regression\n",
      "steps and recheck the correlations in step 3; using knowledg e of the covari-\n",
      "ance of the predictors and the piecewise linearity of the alg orithm, we can\n",
      "workouttheexactsteplengthatthebeginningofeachstep(E xercise3.25).\n",
      "The right panel of Figure 3.15 shows the lasso coeﬃcient proﬁ les on the\n",
      "same data. They are almost identical to those in the left pane l, and diﬀer\n",
      "for the ﬁrst time when the blue coeﬃcient passes back through zero. For the\n",
      "prostate data, the LAR coeﬃcient proﬁle turns out to be ident ical to the\n",
      "lasso proﬁle in Figure 3.10, which never crosses zero. These observations\n",
      "lead to a simple modiﬁcation of the LAR algorithm that gives t he entire\n",
      "lasso path, which is also piecewise-linear.\n",
      "Algorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\n",
      "4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\n",
      "of variables and recompute the current joint least squares d irection.\n",
      "TheLAR(lasso)algorithmisextremelyeﬃcient,requiringt hesameorder\n",
      "of computation as that of a single least squares ﬁt using the ppredictors.\n",
      "Least angle regression always takes psteps to get to the full least squares\n",
      "estimates. The lasso path can have more than psteps, although the two\n",
      "are often quite similar. Algorithm 3.2 with the lasso modiﬁc ation 3.2a is\n",
      "an eﬃcient way of computing the solution to any lasso problem , especially\n",
      "whenp≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\n",
      "for computing the lasso, which they called a homotopy algorithm.\n",
      "Wenowgiveaheuristicargumentforwhytheseproceduresare sosimilar.\n",
      "Although the LAR algorithm is stated in terms of correlation s, if the input\n",
      "features are standardized, it is equivalent and easier to wo rk with inner-\n",
      "products. Suppose Ais the active set of variables at some stage in the\n",
      "algorithm, tied in their absolute inner-product with the cu rrent residuals\n",
      "y−Xβ. We can express this as\n",
      "xT\n",
      "j(y−Xβ) =γ·sj,∀j∈A (3.56)\n",
      "wheresj∈{−1,1}indicates the sign of the inner-product, and γis the\n",
      "common value. Also |xT\n",
      "k(y−Xβ)|≤γ∀k̸∈A. Now consider the lasso\n",
      "criterion (3.52), which we write in vector form\n",
      "R(β) =1\n",
      "2||y−Xβ||2\n",
      "2+λ||β||1. (3.57)\n",
      "LetBbe the active set of variables in the solution for a given valu e ofλ.\n",
      "For these variables R(β) is diﬀerentiable, and the stationarity conditions\n",
      "give\n",
      "xT\n",
      "j(y−Xβ) =λ·sign(βj),∀j∈B (3.58)\n",
      "Comparing (3.58) with (3.56), we see that they are identical only if the\n",
      "sign ofβjmatches the sign of the inner product. That is why the LAR\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.4 Shrinkage Methods 77\n",
      "algorithm and lasso start to diﬀer when an active coeﬃcient p asses through\n",
      "zero; condition (3.58) is violated for that variable, and it is kicked out of the\n",
      "active setB. Exercise 3.23 shows that these equations imply a piecewise -\n",
      "linear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\n",
      "non-active variables require that\n",
      "|xT\n",
      "k(y−Xβ)|≤λ,∀k̸∈B, (3.59)\n",
      "which again agrees with the LAR algorithm.\n",
      "Figure 3.16 compares LAR and lasso to forward stepwise and st agewise\n",
      "regression. The setup is the same as in Figure 3.6 on page 59, e xcept here\n",
      "N= 100 here rather than 300, so the problem is more diﬃcult. We s ee\n",
      "that the more aggressive forward stepwise starts to overﬁt q uite early (well\n",
      "before the 10 true variables can enter the model), and ultima tely performs\n",
      "worse than the slower forward stagewise regression. The beh avior of LAR\n",
      "and lasso is similar to that of forward stagewise regression . Incremental\n",
      "forward stagewise is similar to LAR and lasso, and is describ ed in Sec-\n",
      "tion 3.8.1.\n",
      "Degrees-of-Freedom Formula for LAR and Lasso\n",
      "Suppose that we ﬁt a linear model via the least angle regressi on procedure,\n",
      "stoppingatsomenumberofsteps k<p,orequivalentlyusingalassobound\n",
      "tthat produces a constrained version of the full least square s ﬁt. How many\n",
      "parameters, or “degrees of freedom” have we used?\n",
      "Considerﬁrstalinearregressionusingasubsetof kfeatures.Ifthissubset\n",
      "is prespeciﬁed in advance without reference to the training data, then the\n",
      "degrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\n",
      "classical statistics, the number of linearly independent p arameters is what\n",
      "is meant by “degrees of freedom.” Alternatively, suppose th at we carry out\n",
      "a best subset selection to determine the “optimal” set of kpredictors. Then\n",
      "the resulting model has kparameters, but in some sense we have used up\n",
      "more thankdegrees of freedom.\n",
      "We need a more general deﬁnition for the eﬀective degrees of f reedom of\n",
      "an adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\n",
      "vectorˆy= (ˆy1,ˆy2,...,ˆyN) as\n",
      "df(ˆy) =1\n",
      "σ2N∑\n",
      "i=1Cov(ˆyi,yi). (3.60)\n",
      "Here Cov(ˆyi,yi) refers to the sampling covariance between the predicted\n",
      "value ˆyiand its corresponding outcome value yi. This makes intuitive sense:\n",
      "the harder that we ﬁt to the data, the larger this covariance a nd hence\n",
      "df(ˆy). Expression (3.60) is a useful notion of degrees of freedom , one that\n",
      "can be applied to any model prediction ˆy. This includes models that are\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"78 3. Linear Methods for Regression\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\n",
      "LAR\n",
      "Lasso\n",
      "Forward Stagewise\n",
      "Incremental Forward StagewiseE||ˆβ(k)−β||2\n",
      "Fraction of L1arc-length\n",
      "FIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\n",
      "stagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\n",
      "is the same as in Figure 3.6, except N= 100here rather than 300. Here the\n",
      "slower FS regression ultimately outperforms forward stepwise . LAR and lasso\n",
      "show similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\n",
      "of steps (across simulation replicates and methods), we plot th e MSE as a function\n",
      "of the fraction of total L1arc-length toward the least-squares ﬁt.\n",
      "adaptively ﬁtted to the training data. This deﬁnition is mot ivated and\n",
      "discussed further in Sections 7.4–7.6.\n",
      "Now for a linear regression with kﬁxed predictors, it is easy to show\n",
      "that df(ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the\n",
      "closed-form expression (3.50) on page 68: df( ˆy) = tr(Sλ). In both these\n",
      "cases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\n",
      "If we think about deﬁnition (3.60) in the context of a best sub set selection\n",
      "of sizek, it seems clear that df( ˆy) will be larger than k, and this can be\n",
      "veriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\n",
      "is no closed form method for estimating df( ˆy) for best subset selection.\n",
      "For LAR and lasso, something magical happens. These techniq ues are\n",
      "adaptiveinasmootherwaythanbestsubsetselection,andhe nceestimation\n",
      "of degrees of freedom is more tractable. Speciﬁcally it can b e shown that\n",
      "after thekth step of the LAR procedure, the eﬀective degrees of freedom of\n",
      "the ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.5 Methods Using Derived Input Directions 79\n",
      "often takes more than psteps, since predictors can drop out. Hence the\n",
      "deﬁnitionisalittlediﬀerent;forthelasso,atanystagedf (ˆy)approximately\n",
      "equals the number of predictors in the model. While this appr oximation\n",
      "works reasonably well anywhere in the lasso path, for each kit works best\n",
      "at thelastmodel in the sequence that contains kpredictors. A detailed\n",
      "study of the degrees of freedom for the lasso may be found in Zo u et al.\n",
      "(2007).\n",
      "3.5 Methods Using Derived Input Directions\n",
      "In many situations we have a large number of inputs, often ver y correlated.\n",
      "The methods in this section produce a small number of linear c ombinations\n",
      "Zm, m= 1,...,Mof the original inputs Xj, and theZmare then used in\n",
      "place of the Xjas inputs in the regression. The methods diﬀer in how the\n",
      "linear combinations are constructed.\n",
      "3.5.1 Principal Components Regression\n",
      "In this approach the linear combinations Zmused are the principal com-\n",
      "ponents as deﬁned in Section 3.4.1 above.\n",
      "Principal component regression forms the derived input col umnszm=\n",
      "Xvm, and then regresses yonz1,z2,...,zMfor someM≤p. Since the zm\n",
      "are orthogonal, this regression is just a sum of univariate r egressions:\n",
      "ˆypcr\n",
      "(M)= ¯y1+M∑\n",
      "m=1ˆθmzm, (3.61)\n",
      "whereˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\n",
      "of the original xj, we can express the solution (3.61) in terms of coeﬃcients\n",
      "of thexj(Exercise 3.13):\n",
      "ˆβpcr(M) =M∑\n",
      "m=1ˆθmvm. (3.62)\n",
      "As with ridge regression, principal components depend on th e scaling of\n",
      "the inputs, so typically we ﬁrst standardize them. Note that ifM=p, we\n",
      "would just get back the usual least squares estimates, since the columns of\n",
      "Z=UDspan the column space of X. ForM <pwe get a reduced regres-\n",
      "sion. We see that principal components regression is very si milar to ridge\n",
      "regression: both operate via the principal components of th e input ma-\n",
      "trix. Ridge regression shrinks the coeﬃcients of the princi pal components\n",
      "(Figure 3.17), shrinking more depending on the size of the co rresponding\n",
      "eigenvalue; principal components regression discards the p−Msmallest\n",
      "eigenvalue components. Figure 3.17 illustrates this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"80 3. Linear Methods for Regression\n",
      "IndexShrinkage Factor\n",
      "2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\n",
      "••\n",
      "••••\n",
      "•• • • • • • •\n",
      "• •ridge\n",
      "pcr\n",
      "FIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the p rin-\n",
      "cipal components, using shrinkage factors d2\n",
      "j/(d2\n",
      "j+λ)as in (3.47). Principal\n",
      "component regression truncates them. Shown are the shrinka ge and truncation\n",
      "patterns corresponding to Figure 3.7, as a function of the pr incipal component\n",
      "index.\n",
      "In Figure 3.7 we see that cross-validation suggests seven te rms; the re-\n",
      "sulting model has the lowest test error in Table 3.3.\n",
      "3.5.2 Partial Least Squares\n",
      "This technique also constructs a set of linear combinations of the inputs\n",
      "for regression, but unlike principal components regressio n it uses y(in ad-\n",
      "dition to X) for this construction. Like principal component regressi on,\n",
      "partial least squares (PLS) is not scale invariant, so we ass ume that each\n",
      "xjis standardized to have mean 0 and variance 1. PLS begins by co m-\n",
      "puting ˆϕ1j=⟨xj,y⟩for eachj. From this we construct the derived input\n",
      "z1=∑\n",
      "jˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\n",
      "in the construction of each zm, the inputs are weighted by the strength\n",
      "of their univariate eﬀect on y3. The outcome yis regressed on z1giving\n",
      "coeﬃcient ˆθ1, and then we orthogonalize x1,...,xpwith respect to z1. We\n",
      "continue this process, until M≤pdirections have been obtained. In this\n",
      "manner, partial least squares produces a sequence of derive d, orthogonal\n",
      "inputs or directions z1,z2,...,zM. As with principal-component regres-\n",
      "sion, if we were to construct all M=pdirections, we would get back a\n",
      "solution equivalent to the usual least squares estimates; u singM < pdi-\n",
      "rections produces a reduced regression. The procedure is de scribed fully in\n",
      "Algorithm 3.3.\n",
      "3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression\n",
      "coeﬃcients (up to an irrelevant constant); this is not the case for subsequen t directions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.5 Methods Using Derived Input Directions 81\n",
      "Algorithm 3.3 Partial Least Squares.\n",
      "1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\n",
      "¯y1, andx(0)\n",
      "j=xj, j= 1,...,p.\n",
      "2. Form= 1,2,...,p\n",
      "(a)zm=∑p\n",
      "j=1ˆϕmjx(m−1)\n",
      "j, where ˆϕmj=⟨x(m−1)\n",
      "j,y⟩.\n",
      "(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\n",
      "(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\n",
      "(d) Orthogonalize each x(m−1)\n",
      "jwith respect to zm:x(m)\n",
      "j=x(m−1)\n",
      "j−\n",
      "[⟨zm,x(m−1)\n",
      "j⟩/⟨zm,zm⟩]zm,j= 1,2,...,p.\n",
      "3. Output the sequence of ﬁtted vectors {ˆy(m)}p\n",
      "1. Since the{zℓ}m\n",
      "1are\n",
      "linear in the original xj, so isˆy(m)=Xˆβpls(m). These linear coeﬃ-\n",
      "cients can be recovered from the sequence of PLS transformat ions.\n",
      "In the prostate cancer example, cross-validation chose M= 2 PLS direc-\n",
      "tions in Figure 3.7. This produced the model given in the righ tmost column\n",
      "of Table 3.3.\n",
      "What optimization problem is partial least squares solving ? Since it uses\n",
      "the response yto construct its directions, its solution path is a nonlinea r\n",
      "function of y. It can be shown (Exercise 3.15) that partial least squares\n",
      "seeks directions that have high variance andhave high correlation with the\n",
      "response, in contrast to principal components regression w hich keys only\n",
      "on high variance (Stone and Brooks, 1990; Frank and Friedman , 1993). In\n",
      "particular, the mth principal component direction vmsolves:\n",
      "maxαVar(Xα) (3.63)\n",
      "subject to||α||= 1, αTSvℓ= 0, ℓ= 1,...,m−1,\n",
      "whereSis the sample covariance matrix of the xj. The conditions αTSvℓ=\n",
      "0 ensures that zm=Xαis uncorrelated with all the previous linear com-\n",
      "binations zℓ=Xvℓ. Themth PLS direction ˆ ϕmsolves:\n",
      "maxαCorr2(y,Xα)Var(Xα) (3.64)\n",
      "subject to||α||= 1, αTSˆϕℓ= 0, ℓ= 1,...,m−1.\n",
      "Further analysis reveals that the variance aspect tends to d ominate, and\n",
      "so partial least squares behaves much like ridge regression and principal\n",
      "components regression. We discuss this further in the next s ection.\n",
      "If the input matrix Xis orthogonal, then partial least squares ﬁnds the\n",
      "least squares estimates after m= 1 steps. Subsequent steps have no eﬀect\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"82 3. Linear Methods for Regression\n",
      "since the ˆϕmjare zero for m>1 (Exercise 3.14). It can also be shown that\n",
      "the sequence of PLS coeﬃcients for m= 1,2,...,prepresents the conjugate\n",
      "gradient sequence for computing the least squares solution s (Exercise 3.18).\n",
      "3.6 Discussion: A Comparison of the Selection and\n",
      "Shrinkage Methods\n",
      "There are some simple settings where we can understand bette r the rela-\n",
      "tionship between the diﬀerent methods described above. Con sider an exam-\n",
      "ple with two correlated inputs X1andX2, with correlation ρ. We assume\n",
      "that the true regression coeﬃcients are β1= 4 andβ2= 2. Figure 3.18\n",
      "shows the coeﬃcient proﬁles for the diﬀerent methods, as the ir tuning pa-\n",
      "rameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\n",
      "The tuning parameters for ridge and lasso vary over a continu ous range,\n",
      "while best subset, PLS and PCR take just two discrete steps to the least\n",
      "squares solution. In the top panel, starting at the origin, r idge regression\n",
      "shrinks the coeﬃcients together until it ﬁnally converges t o least squares.\n",
      "PLS and PCR show similar behavior to ridge, although are disc rete and\n",
      "more extreme. Best subset overshoots the solution and then b acktracks.\n",
      "The behavior of the lasso is intermediate to the other method s. When the\n",
      "correlation is negative (lower panel), again PLS and PCR rou ghly track\n",
      "the ridge path, while all of the methods are more similar to on e another.\n",
      "It is interesting to compare the shrinkage behavior of these diﬀerent\n",
      "methods. Recall that ridge regression shrinks all directio ns, but shrinks\n",
      "low-variance directions more. Principal components regre ssion leaves M\n",
      "high-variance directions alone, and discards the rest. Int erestingly, it can\n",
      "be shown that partial least squares also tends to shrink the l ow-variance\n",
      "directions, but can actually inﬂate some of the higher varia nce directions.\n",
      "This can make PLS a little unstable, and cause it to have sligh tly higher\n",
      "prediction error compared to ridge regression. A full study is given in Frank\n",
      "and Friedman (1993). These authors conclude that for minimi zing predic-\n",
      "tion error, ridge regression is generally preferable to var iable subset selec-\n",
      "tion, principal components regression and partial least sq uares. However\n",
      "the improvement over the latter two methods was only slight.\n",
      "To summarize, PLS, PCR and ridge regression tend to behave si milarly.\n",
      "Ridge regression may be preferred because it shrinks smooth ly, rather than\n",
      "in discrete steps. Lasso falls somewhere between ridge regr ession and best\n",
      "subset regression, and enjoys some of the properties of each .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\n",
      "0 1 2 3 4 5 6-1 0 1 2 3Least Squares\n",
      "0Ridge\n",
      "Lasso\n",
      "Best SubsetPLS PCR\n",
      "•\n",
      "0 1 2 3 4 5 6-1 0 1 2 3Least Squares\n",
      "Ridge\n",
      "Best Subset\n",
      "PLS\n",
      "PCRLasso•\n",
      "0ρ= 0.5\n",
      "ρ=−0.5\n",
      "β1β1β2 β2\n",
      "FIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\n",
      "two inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"84 3. Linear Methods for Regression\n",
      "3.7 Multiple Outcome Shrinkage and Selection\n",
      "As noted in Section 3.2.4, the least squares estimates in a mu ltiple-output\n",
      "linear model are simply the individual least squares estima tes for each of\n",
      "the outputs.\n",
      "To apply selection and shrinkage methods in the multiple out put case,\n",
      "one could apply a univariate technique individually to each outcome or si-\n",
      "multaneously to all outcomes. With ridge regression, for ex ample, we could\n",
      "apply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\n",
      "using possibly diﬀerent parameters λ, or apply it to all columns using the\n",
      "same value of λ. The former strategy would allow diﬀerent amounts of\n",
      "regularization to be applied to diﬀerent outcomes but requi re estimation\n",
      "ofkseparate regularization parameters λ1,...,λ k, while the latter would\n",
      "permit allkoutputs to be used in estimating the sole regularization pa-\n",
      "rameterλ.\n",
      "Other more sophisticated shrinkage and selection strategi es that exploit\n",
      "correlations in the diﬀerent responses can be helpful in the multiple output\n",
      "case. Suppose for example that among the outputs we have\n",
      "Yk=f(X)+εk (3.65)\n",
      "Yℓ=f(X)+εℓ; (3.66)\n",
      "i.e., (3.65) and (3.66) share the same structural part f(X) in their models.\n",
      "It is clear in this case that we should pool our observations o nYkandYl\n",
      "to estimate the common f.\n",
      "Combining responses is at the heart of canonical correlation analysis\n",
      "(CCA), a data reduction technique developed for the multipl e output case.\n",
      "Similar to PCA, CCA ﬁnds a sequence of uncorrelated linear co mbina-\n",
      "tionsXvm, m= 1,...,M of thexj, and a corresponding sequence of\n",
      "uncorrelated linear combinations Yumof the responses yk, such that the\n",
      "correlations\n",
      "Corr2(Yum,Xvm) (3.67)\n",
      "are successively maximized. Note that at most M= min(K,p) directions\n",
      "can be found. The leading canonical response variates are th ose linear com-\n",
      "binations (derived responses) best predicted by the xj; in contrast, the\n",
      "trailing canonical variates can be poorly predicted by the xj, and are can-\n",
      "didates for being dropped. The CCA solution is computed usin g a general-\n",
      "ized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\n",
      "Xare centered; Exercise 3.20).\n",
      "Reduced-rank regression (Izenman,1975;vanderMerweandZidek,1980)\n",
      "formalizes this approach in terms of a regression model that explicitly pools\n",
      "information. Given an error covariance Cov( ε) =Σ, we solve the following\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.7 Multiple Outcome Shrinkage and Selection 85\n",
      "restricted multivariate regression problem:\n",
      "ˆBrr(m) = argmin\n",
      "rank(B)=mN∑\n",
      "i=1(yi−BTxi)TΣ−1(yi−BTxi).(3.68)\n",
      "WithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\n",
      "that the solution is given by a CCA of YandX:\n",
      "ˆBrr(m) =ˆBUmU−\n",
      "m, (3.69)\n",
      "whereUmis theK×msub-matrix of Uconsisting of the ﬁrst mcolumns,\n",
      "andUis theK×Mmatrix of leftcanonical vectors u1,u2,...,u M.U−\n",
      "m\n",
      "is its generalized inverse. Writing the solution as\n",
      "ˆBrr(M) = (XTX)−1XT(YUm)U−\n",
      "m, (3.70)\n",
      "we see that reduced-rank regression performs a linear regre ssion on the\n",
      "pooled response matrix YUm, and then maps the coeﬃcients (and hence\n",
      "the ﬁts as well) back to the original response space. The redu ced-rank ﬁts\n",
      "are given by\n",
      "ˆYrr(m) =X(XTX)−1XTYUmU−\n",
      "m\n",
      "=HYPm,(3.71)\n",
      "whereHis the usual linear regression projection operator, and Pmis the\n",
      "rank-mCCA response projection operator. Although a better estima te of\n",
      "Σwould be( Y−XˆB)T(Y−XˆB)/(N−pK), onecanshowthat thesolution\n",
      "remains the same (Exercise 3.22).\n",
      "Reduced-rank regression borrows strength among responses by truncat-\n",
      "ing the CCA. Breiman and Friedman (1997) explored with some s uccess\n",
      "shrinkage of the canonical variates between XandY, a smooth version of\n",
      "reduced rank regression. Their proposal has the form (compare (3.69))\n",
      "ˆBc+w=ˆBUΛU−1, (3.72)\n",
      "whereΛis a diagonal shrinkage matrix (the “c+w” stands for “Curds\n",
      "and Whey,” the name they gave to their procedure). Based on op timal\n",
      "prediction in the population setting, they show that Λhas diagonal entries\n",
      "λm=c2\n",
      "m\n",
      "c2m+p\n",
      "N(1−c2m), m= 1,...,M, (3.73)\n",
      "wherecmis themth canonical correlation coeﬃcient. Note that as the ratio\n",
      "of the number of input variables to sample size p/Ngets small, the shrink-\n",
      "age factors approach 1. Breiman and Friedman (1997) propose d modiﬁed\n",
      "versions of Λbased on training data and cross-validation, but the genera l\n",
      "form is the same. Here the ﬁtted response has the form\n",
      "ˆYc+w=HYSc+w, (3.74)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"86 3. Linear Methods for Regression\n",
      "whereSc+w=UΛU−1is the response shrinkage operator.\n",
      "Breiman and Friedman (1997) also suggested shrinking in bot h theY\n",
      "space andXspace. This leads to hybrid shrinkage models of the form\n",
      "ˆYridge,c+w=AλYSc+w, (3.75)\n",
      "whereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\n",
      "as in (3.46) on page 66. Their paper and the discussions there of contain\n",
      "many more details.\n",
      "3.8 More on the Lasso and Related Path\n",
      "Algorithms\n",
      "Since the publication of the LAR algorithm (Efron et al., 200 4) there has\n",
      "been a lot of activity in developing algorithms for ﬁtting re gularization\n",
      "paths for a variety of diﬀerent problems. In addition, L1regularization has\n",
      "taken on a life of its own, leading to the development of the ﬁe ldcompressed\n",
      "sensingin the signal-processing literature. (Donoho, 2006a; Cand es, 2006).\n",
      "Inthissectionwediscusssomerelatedproposalsandotherp athalgorithms,\n",
      "starting oﬀ with a precursor to the LAR algorithm.\n",
      "3.8.1 Incremental Forward Stagewise Regression\n",
      "Here we present another LAR-like algorithm, this time focus ed on forward\n",
      "stagewiseregression.Interestingly,eﬀortstounderstan daﬂexiblenonlinear\n",
      "regression procedure (boosting) led to a new algorithm for l inear models\n",
      "(LAR). In reading the ﬁrst edition of this book and the forwar d stagewise\n",
      "Algorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\n",
      "1. Start with the residual requal to yandβ1,β2,...,β p= 0. All the\n",
      "predictors are standardized to have mean zero and unit norm.\n",
      "2. Find the predictor xjmost correlated with r\n",
      "3. Updateβj←βj+δj, whereδj=ǫ·sign[⟨xj,r⟩] andǫ>0 is a small\n",
      "step size, and set r←r−δjxj.\n",
      "4. Repeat steps 2 and 3 many times, until the residuals are unc orrelated\n",
      "with all the predictors.\n",
      "Algorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\n",
      "4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "0 50 100 150 200\n",
      "−0.2 0.0 0.2 0.4 0.6lcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "0.0 0.5 1.0 1.5 2.0FSǫ FS0\n",
      "Iteration\n",
      "CoeﬃcientsCoeﬃcients\n",
      "L1Arc-length of Coeﬃcients\n",
      "FIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\n",
      "incremental forward stagewise regression with step size ǫ= 0.01. The right panel\n",
      "shows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\n",
      "the modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\n",
      "are monotone, and hence identical to those of lasso and LAR.\n",
      "linearmodels,onecouldexplicitlyconstructthepiecewis e-linearlassopaths\n",
      "of Figure 3.10. This led him to propose the LAR procedure of Se ction 3.4.4,\n",
      "as well as the incremental version of forward-stagewise reg ression presented\n",
      "here.\n",
      "Consider the linear-regression version of the forward-sta gewise boosting\n",
      "algorithm16.1proposedinSection16.1(page608).Itgener atesacoeﬃcient\n",
      "proﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\n",
      "variable most correlated with the current residuals. Algor ithm 3.4 gives\n",
      "the details. Figure 3.19 (left panel) shows the progress of t he algorithm on\n",
      "the prostate data with step size ǫ= 0.01. Ifδj=⟨xj,r⟩(the least-squares\n",
      "coeﬃcient of the residual on jth predictor), then this is exactly the usual\n",
      "forward stagewise procedure (FS) outlined in Section 3.3.3 .\n",
      "Here we are mainly interested in small values of ǫ. Lettingǫ→0 gives\n",
      "the right panel of Figure 3.19, which in this case is identica l to the lasso\n",
      "path in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\n",
      "stagewise regression or FS0. This procedure plays an important role in\n",
      "non-linear, adaptive methods like boosting (Chapters 10 an d 16) and is the\n",
      "version of incremental forward stagewise regression that i s most amenable\n",
      "to theoretical analysis. B¨ uhlmann and Hothorn (2007) refe r to the same\n",
      "procedure as “L2boost”, because of its connections to boost ing.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"88 3. Linear Methods for Regression\n",
      "Efron originally thought that the LAR Algorithm 3.2 was an im plemen-\n",
      "tation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\n",
      "cients in a balanced way, while remaining tied in correlatio n. However, he\n",
      "then realized that the LAR least-squares ﬁt amongst the tied predictors\n",
      "can result in coeﬃcients moving in the opposite direction to their correla-\n",
      "tion, which cannot happen in Algorithm 3.4. The following mo diﬁcation of\n",
      "the LAR algorithm implements FS 0:\n",
      "Algorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\n",
      "4. Find the new direction by solving the constrained least sq uares prob-\n",
      "lem\n",
      "min\n",
      "b||r−XAb||2\n",
      "2subject tobjsj≥0, j∈A,\n",
      "wheresjis the sign of⟨xj,r⟩.\n",
      "The modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\n",
      "signs of the coeﬃcients the same as those of the correlations . One can show\n",
      "that this achieves the optimal balancing of inﬁnitesimal “u pdate turns”\n",
      "for the variables tied for maximal correlation (Hastie et al ., 2007). Like\n",
      "lasso, the entire FS 0path can be computed very eﬃciently via the LAR\n",
      "algorithm.\n",
      "As a consequence of these results, if the LAR proﬁles are mono tone non-\n",
      "increasing or non-decreasing, as they are in Figure 3.19, th en all three\n",
      "methods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\n",
      "not monotone but do not cross the zero axis, then LAR and lasso are\n",
      "identical.\n",
      "Since FS 0is diﬀerent from the lasso, it is natural to ask if it optimize s\n",
      "a criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\n",
      "proﬁle is the solution to a diﬀerential equation. While the l asso makes op-\n",
      "timal progress in terms of reducing the residual sum-of-squ ares per unit\n",
      "increase in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\n",
      "increase in L1arc-length traveled along the coeﬃcient path. Hence its co-\n",
      "eﬃcient path is discouraged from changing directions too of ten.\n",
      "FS0is more constrained than lasso, and in fact can be viewed as a m ono-\n",
      "tone version of the lasso; see Figure 16.3 on page 614 for a dra matic exam-\n",
      "ple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\n",
      "are much smoother and hence have less variance than those of l asso. More\n",
      "details on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\n",
      "ure 3.16 includes FS 0where its performance is very similar to that of the\n",
      "lasso.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.8 More on the Lasso and Related Path Algorithms 89\n",
      "3.8.2 Piecewise-Linear Path Algorithms\n",
      "The least angle regression procedure exploits the piecewis e linear nature of\n",
      "the lasso solution paths. It has led to similar “path algorit hms” for other\n",
      "regularized problems. Suppose we solve\n",
      "ˆβ(λ) = argminβ[R(β)+λJ(β)], (3.76)\n",
      "with\n",
      "R(β) =N∑\n",
      "i=1L(yi,β0+p∑\n",
      "j=1xijβj), (3.77)\n",
      "where both the loss function Land the penalty function Jare convex.\n",
      "Then the following are suﬃcient conditions for the solution pathˆβ(λ) to\n",
      "be piecewise linear (Rosset and Zhu, 2007):\n",
      "1.Ris quadratic or piecewise-quadratic as a function of β, and\n",
      "2.Jis piecewise linear in β.\n",
      "This also implies (in principle) that the solution path can b e eﬃciently\n",
      "computed. Examples include squared- and absolute-error lo ss, “Huberized”\n",
      "losses, and the L1,L∞penalties on β. Another example is the “hinge loss”\n",
      "function used in the support vector machine. There the loss i s piecewise\n",
      "linear, and the penalty is quadratic. Interestingly, this l eads to a piecewise-\n",
      "linear path algorithm in the dual space ; more details are given in Sec-\n",
      "tion 12.3.5.\n",
      "3.8.3 The Dantzig Selector\n",
      "Candes and Tao (2007) proposed the following criterion:\n",
      "minβ||β||1subject to||XT(y−Xβ)||∞≤s. (3.78)\n",
      "They call the solution the Dantzig selector (DS). It can be written equiva-\n",
      "lently as\n",
      "minβ||XT(y−Xβ)||∞subject to||β||1≤t. (3.79)\n",
      "Here||·||∞denotes the L∞norm, the maximum absolute value of the\n",
      "components of the vector. In this form it resembles the lasso , replacing\n",
      "squared error loss by the maximum absolute value of its gradi ent. Note\n",
      "that astgets large, both procedures yield the least squares solutio n if\n",
      "N <p. Ifp≥N, they both yield the least squares solution with minimum\n",
      "L1norm. However for smaller values of t, the DS procedure produces a\n",
      "diﬀerent path of solutions than the lasso.\n",
      "Candes and Tao (2007) show that the solution to DS is a linear p ro-\n",
      "gramming problem; hence the name Dantzig selector, in honor of the late\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"90 3. Linear Methods for Regression\n",
      "George Dantzig, the inventor of the simplex method for linea r program-\n",
      "ming. They also prove a number of interesting mathematical p roperties for\n",
      "the method, related to its ability to recover an underlying s parse coeﬃ-\n",
      "cient vector. These same properties also hold for the lasso, as shown later\n",
      "by Bickel et al. (2008).\n",
      "Unfortunately the operating properties of the DS method are somewhat\n",
      "unsatisfactory. The method seems similar in spirit to the la sso, especially\n",
      "when we look at the lasso’s stationary conditions (3.58). Li ke the LAR al-\n",
      "gorithm, the lasso maintains the same inner product (and cor relation) with\n",
      "the current residual for all variables in the active set, and moves their co-\n",
      "eﬃcients to optimally decrease the residual sum of squares. In the process,\n",
      "this common correlation is decreased monotonically (Exerc ise 3.23), and at\n",
      "all times this correlation is larger than that for non-activ e variables. The\n",
      "Dantzig selector instead tries to minimize the maximum inne r product of\n",
      "the current residual with all the predictors. Hence it can ac hieve a smaller\n",
      "maximum than the lasso, but in the process a curious phenomen on can\n",
      "occur. If the size of the active set is m, there will be mvariables tied with\n",
      "maximum correlation. However, these need not coincide with the active set!\n",
      "Hence it can include a variable in the model that has smaller c orrelation\n",
      "with the current residual than some of the excluded variable s (Efron et\n",
      "al., 2007). This seems unreasonable and may be responsible f or its some-\n",
      "times inferior prediction accuracy. Efron et al. (2007) als o show that DS\n",
      "canyieldextremelyerraticcoeﬃcientpathsastheregulari zation parameter\n",
      "sis varied.\n",
      "3.8.4 The Grouped Lasso\n",
      "In some problems, the predictors belong to pre-deﬁned group s; for example\n",
      "genes that belong to the same biological pathway, or collect ions of indicator\n",
      "(dummy) variables for representing the levels of a categori cal predictor. In\n",
      "this situation it may be desirable to shrink and select the me mbers of a\n",
      "group together. The grouped lasso is one way to achieve this. Suppose that\n",
      "theppredictors are divided into Lgroups, with pℓthe number in group\n",
      "ℓ. For ease of notation, we use a matrix Xℓto represent the predictors\n",
      "corresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\n",
      "The grouped-lasso minimizes the convex criterion\n",
      "min\n",
      "β∈IRp(\n",
      "||y−β01−L∑\n",
      "ℓ=1Xℓβℓ||2\n",
      "2+λL∑\n",
      "ℓ=1√pℓ||βℓ||2)\n",
      ", (3.80)\n",
      "where the√pℓterms accounts for the varying group sizes, and ||·||2is\n",
      "the Euclidean norm (not squared). Since the Euclidean norm o f a vector\n",
      "βℓis zero only if all of its components are zero, this procedure encourages\n",
      "sparsity at both the group and individual levels. That is, fo r some values of\n",
      "λ, an entire group of predictors may drop out of the model. This procedure\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.8 More on the Lasso and Related Path Algorithms 91\n",
      "was proposed by Bakin (1999) and Lin and Zhang (2006), and stu died and\n",
      "generalized by Yuan and Lin (2007). Generalizations includ e more general\n",
      "L2norms||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\n",
      "predictors (Zhao et al., 2008). There are also connections t o methods for\n",
      "ﬁtting sparse additive models (Lin and Zhang, 2006; Ravikum ar et al.,\n",
      "2008).\n",
      "3.8.5 Further Properties of the Lasso\n",
      "A number of authors have studied the ability of the lasso and r elated pro-\n",
      "cedures to recover the correct model, as Nandpgrow. Examples of this\n",
      "work include Knight and Fu (2000), Greenshtein and Ritov (20 04), Tropp\n",
      "(2004), Donoho (2006b), Meinshausen (2007), Meinshausen a nd B¨ uhlmann\n",
      "(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006) , and Bunea\n",
      "et al. (2007). For example Donoho (2006b) focuses on the p>Ncase and\n",
      "considers the lasso solution as the bound tgets large. In the limit this gives\n",
      "the solution with minimum L1norm among all models with zero training\n",
      "error. He shows that under certain assumptions on the model m atrixX, if\n",
      "the true model is sparse, this solution identiﬁes the correc t predictors with\n",
      "high probability.\n",
      "Many of the results in this area assume a condition on the mode l matrix\n",
      "of the form\n",
      "max\n",
      "j∈Sc||xT\n",
      "jXS(XSTXS)−1||1≤(1−ǫ) for someǫ∈(0,1].(3.81)\n",
      "HereSindexes the subset of features with non-zero coeﬃcients in t he true\n",
      "underlying model, and XSare the columns of Xcorresponding to those\n",
      "features. Similarly Scare the features with true coeﬃcients equal to zero,\n",
      "andXScthe corresponding columns. This says that the least squares coef-\n",
      "ﬁcients for the columns of XSconXSare not too large, that is, the “good”\n",
      "variablesSare not too highly correlated with the nuisance variables Sc.\n",
      "Regarding the coeﬃcients themselves, the lasso shrinkage c auses the esti-\n",
      "mates of the non-zero coeﬃcients to be biased towards zero, a nd in general\n",
      "they are not consistent5. One approach for reducing this bias is to run\n",
      "the lasso to identify the set of non-zero coeﬃcients, and the n ﬁt an un-\n",
      "restricted linear model to the selected set of features. Thi s is not always\n",
      "feasible, if the selected set is large. Alternatively, one c an use the lasso to\n",
      "select the set of non-zero predictors, and then apply the las so again, but\n",
      "using only the selected predictors from the ﬁrst step. This i s known as the\n",
      "relaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\n",
      "estimate the initial penalty parameter for the lasso, and th en again for a\n",
      "second penalty parameter applied to the selected set of pred ictors. Since\n",
      "5Statistical consistency means as the sample size grows, the estimates converge to\n",
      "the true values.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"92 3. Linear Methods for Regression\n",
      "the variables in the second step have less “competition” fro m noise vari-\n",
      "ables, cross-validation will tend to pick a smaller value fo rλ, and hence\n",
      "their coeﬃcients will be shrunken less than those in the init ial estimate.\n",
      "Alternatively,onecanmodifythelassopenaltyfunctionso thatlargerco-\n",
      "eﬃcients are shrunken less severely; the smoothly clipped absolute deviation\n",
      "(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\n",
      "dJa(β,λ)\n",
      "dβ=λ·sign(β)[\n",
      "I(|β|≤λ)+(aλ−|β|)+\n",
      "(a−1)λI(|β|>λ)]\n",
      "(3.82)\n",
      "for somea≥2. The second term in square-braces reduces the amount of\n",
      "shrinkage in the lasso for larger values of β, with ultimately no shrinkage\n",
      "asa→∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\n",
      "−4 −2 0 2 40 1 2 3 4 5\n",
      "−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\n",
      "−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\n",
      "β β β\n",
      "FIGURE 3.20. The lasso and two alternative non-convex penalties designed to\n",
      "penalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, andν=1\n",
      "2in\n",
      "the last panel.\n",
      "|β|1−ν. However this criterion is non-convex, which is a drawback s ince it\n",
      "makes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\n",
      "uses a weighted penalty of the form∑p\n",
      "j=1wj|βj|wherewj= 1/|ˆβj|ν,ˆβjis\n",
      "the ordinary least squares estimate and ν >0. This is a practical approxi-\n",
      "mation to the|β|qpenalties (q= 1−νhere) discussed in Section 3.4.3. The\n",
      "adaptive lasso yields consistent estimates of the paramete rs while retaining\n",
      "the attractive convexity property of the lasso.\n",
      "3.8.6 Pathwise Coordinate Optimization\n",
      "An alternate approach to the LARS algorithm for computing th e lasso\n",
      "solution is simple coordinate descent. This idea was propos ed by Fu (1998)\n",
      "andDaubechiesetal.(2004),andlaterstudiedandgenerali zedbyFriedman\n",
      "etal.(2007),WuandLange(2008)andothers.Theideaistoﬁx thepenalty\n",
      "parameterλin the Lagrangian form (3.52) and optimize successively ove r\n",
      "each parameter, holding the other parameters ﬁxed at their c urrent values.\n",
      "Suppose the predictors are all standardized to have mean zer o and unit\n",
      "norm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.9 Computational Considerations 93\n",
      "λ. We can rearrange (3.52) to isolate βj,\n",
      "R(˜β(λ),βj) =1\n",
      "2N∑\n",
      "i=1(\n",
      "yi−∑\n",
      "k̸=jxik˜βk(λ)−xijβj)2\n",
      "+λ∑\n",
      "k̸=j|˜βk(λ)|+λ|βj|,\n",
      "(3.83)\n",
      "where we have suppressed the intercept and introduced a fact or1\n",
      "2for con-\n",
      "venience. This can be viewed as a univariate lasso problem wi th response\n",
      "variable the partial residual yi−˜y(j)\n",
      "i=yi−∑\n",
      "k̸=jxik˜βk(λ). This has an\n",
      "explicit solution, resulting in the update\n",
      "˜βj(λ)←S(N∑\n",
      "i=1xij(yi−˜y(j)\n",
      "i),λ)\n",
      ". (3.84)\n",
      "HereS(t,λ) = sign(t)(|t|−λ)+isthesoft-thresholdingoperatorinTable3.4\n",
      "on page 71. The ﬁrst argument to S(·) is the simple least-squares coeﬃcient\n",
      "of the partial residual on the standardized variable xij. Repeated iteration\n",
      "of (3.84)—cycling through each variable in turn until conver gence—yields\n",
      "the lasso estimate ˆβ(λ).\n",
      "We can also use this simple algorithm to eﬃciently compute th e lasso\n",
      "solutions at a grid of values of λ. We start with the smallest value λmax\n",
      "for which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\n",
      "until convergence. Then λis decreased again and the process is repeated,\n",
      "using the previous solution as a “warm start” for the new valu e ofλ. This\n",
      "can be faster than the LARS algorithm, especially in large pr oblems. A\n",
      "key to its speed is the fact that the quantities in (3.84) can b e updated\n",
      "quickly asjvaries, and often the update is to leave ˜βj= 0. On the other\n",
      "hand, it delivers solutions over a grid of λvalues, rather than the entire\n",
      "solution path. The same kind of algorithm can be applied to th e elastic\n",
      "net, the grouped lasso and many other models in which the pena lty is a\n",
      "sum of functions of the individual parameters (Friedman et a l., 2010). It\n",
      "can also be applied, with some substantial modiﬁcations, to the fused lasso\n",
      "(Section 18.4.2); details are in Friedman et al. (2007).\n",
      "3.9 Computational Considerations\n",
      "Least squares ﬁtting is usually done via the Cholesky decomp osition of\n",
      "the matrix XTXor a QR decomposition of X. WithNobservations and p\n",
      "features, the Cholesky decomposition requires p3+Np2/2 operations, while\n",
      "the QR decomposition requires Np2operations. Depending on the relative\n",
      "size ofNandp, the Cholesky can sometimes be faster; on the other hand,\n",
      "it can be less numerically stable (Lawson and Hansen, 1974). Computation\n",
      "of the lasso via the LAR algorithm has the same order of comput ation as\n",
      "a least squares ﬁt.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 95\n",
      "Ex. 3.4Show how the vector of least squares coeﬃcients can be obtain ed\n",
      "from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\n",
      "resent your solution in terms of the QR decomposition of X.\n",
      "Ex. 3.5Consider the ridge regression problem (3.41). Show that thi s prob-\n",
      "lem is equivalent to the problem\n",
      "ˆβc= argmin\n",
      "βc{N∑\n",
      "i=1[\n",
      "yi−βc\n",
      "0−p∑\n",
      "j=1(xij−¯xj)βc\n",
      "j]2+λp∑\n",
      "j=1βc\n",
      "j2}\n",
      ".(3.85)\n",
      "Give the correspondence between βcand the original βin (3.41). Char-\n",
      "acterize the solution to this modiﬁed criterion. Show that a similar result\n",
      "holds for the lasso.\n",
      "Ex. 3.6Show that the ridge regression estimate is the mean (and mode )\n",
      "of the posterior distribution, under a Gaussian prior β∼N(0,τI), and\n",
      "Gaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\n",
      "the regularization parameter λin the ridge formula, and the variances τ\n",
      "andσ2.\n",
      "Ex. 3.7Assumeyi∼N(β0+xT\n",
      "iβ,σ2),i= 1,2,...,N, and the parameters\n",
      "βj, j= 1,...,pare each distributed as N(0,τ2), independently of one\n",
      "another. Assuming σ2andτ2are known, and β0is not governed by a\n",
      "prior (or has a ﬂat improper prior), show that the (minus) log -posterior\n",
      "density ofβis proportional to∑N\n",
      "i=1(yi−β0−∑\n",
      "jxijβj)2+λ∑p\n",
      "j=1β2\n",
      "j\n",
      "whereλ=σ2/τ2.\n",
      "Ex. 3.8Consider the QR decomposition of the uncentered N×(p+ 1)\n",
      "matrixX(whose ﬁrst column is all ones), and the SVD of the N×p\n",
      "centered matrix ˜X. Show that Q2andUspan the same subspace, where\n",
      "Q2is the sub-matrix of Qwith the ﬁrst column removed. Under what\n",
      "circumstances will they be the same, up to sign ﬂips?\n",
      "Ex. 3.9Forward stepwise regression. Suppose we have the QR decomposi-\n",
      "tion for the N×qmatrixX1in a multiple regression problem with response\n",
      "y, and we have an additional p−qpredictors in the matrix X2. Denote the\n",
      "current residual by r. We wish to establish which one of these additional\n",
      "variables will reduce the residual-sum-of squares the most when included\n",
      "with those in X1. Describe an eﬃcient procedure for doing this.\n",
      "Ex. 3.10 Backward stepwise regression. Suppose we have the multiple re-\n",
      "gression ﬁt of yonX, along with the standard errors and Z-scores as in\n",
      "Table 3.2. We wish to establish which variable, when dropped , will increase\n",
      "the residual sum-of-squares the least. How would you do this ?\n",
      "Ex. 3.11 Show that the solution to the multivariate linear regressio n prob-\n",
      "lem (3.40) is given by (3.39). What happens if the covariance matrices Σi\n",
      "are diﬀerent for each observation?\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"96 3. Linear Methods for Regression\n",
      "Ex. 3.12 Show that the ridge regression estimates can be obtained by\n",
      "ordinary least squares regression on an augmented data set. We augment\n",
      "the centered matrix Xwithpadditional rows√\n",
      "λI, and augment ywithp\n",
      "zeros. By introducing artiﬁcial data having response value zero, the ﬁtting\n",
      "procedure is forced to shrink the coeﬃcients toward zero. Th is is related to\n",
      "the idea of hintsdue to Abu-Mostafa (1995), where model constraints are\n",
      "implemented by adding artiﬁcial data examples that satisfy them.\n",
      "Ex. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\n",
      "Ex. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\n",
      "because subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\n",
      "Ex. 3.15 Verify expression (3.64), and hence show that the partial le ast\n",
      "squares directions are a compromise between the ordinary re gression coef-\n",
      "ﬁcient and the principal component directions.\n",
      "Ex. 3.16 Derive the entries in Table 3.4, the explicit forms for estim ators\n",
      "in the orthogonal case.\n",
      "Ex. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\n",
      "Chapter 1.\n",
      "Ex.3.18 Readaboutconjugategradientalgorithms(Murrayetal.,19 81,for\n",
      "example), and establish a connection between these algorit hms and partial\n",
      "least squares.\n",
      "Ex. 3.19 Show that∥ˆβridge∥increases as its tuning parameter λ→0. Does\n",
      "the same property hold for the lasso and partial least square s estimates?\n",
      "For the latter, consider the “tuning parameter” to be the suc cessive steps\n",
      "in the algorithm.\n",
      "Ex. 3.20 Consider the canonical-correlation problem (3.67). Show t hat the\n",
      "leading pair of canonical variates u1andv1solve the problem\n",
      "max\n",
      "uT(YTY)u=1\n",
      "vT(XTX)v=1uT(YTX)v, (3.86)\n",
      "a generalized SVD problem. Show that the solution is given by u1=\n",
      "(YTY)−1\n",
      "2u∗\n",
      "1, andv1= (XTX)−1\n",
      "2v∗\n",
      "1, whereu∗\n",
      "1andv∗\n",
      "1are the leading left\n",
      "and right singular vectors in\n",
      "(YTY)−1\n",
      "2(YTX)(XTX)−1\n",
      "2=U∗D∗V∗T. (3.87)\n",
      "Show that the entire sequence um, vm, m= 1,...,min(K,p) is also given\n",
      "by (3.87).\n",
      "Ex. 3.21 Show that the solution to the reduced-rank regression probl em\n",
      "(3.68), with Σestimated by YTY/N, is given by (3.69). Hint:Transform\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 97\n",
      "YtoY∗=YΣ−1\n",
      "2, and solved in terms of the canonical vectors u∗\n",
      "m. Show\n",
      "thatUm=Σ−1\n",
      "2U∗\n",
      "m, and a generalized inverse is U−\n",
      "m=U∗\n",
      "mTΣ1\n",
      "2.\n",
      "Ex. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\n",
      "estimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\n",
      "Ex. 3.23 Consider a regression problem with all variables and respon se hav-\n",
      "ing mean zero and standard deviation one. Suppose also that e ach variable\n",
      "has identical absolute correlation with the response:\n",
      "1\n",
      "N|⟨xj,y⟩|=λ, j= 1,...,p.\n",
      "Letˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\n",
      "α∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\n",
      "u. LetRSSbe the residual sum-of-squares from the full least squares ﬁ t.\n",
      "(a) Show that\n",
      "1\n",
      "N|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,...,p,\n",
      "and hence the correlations of each xjwith the residuals remain equal\n",
      "in magnitude as we progress toward u.\n",
      "(b) Show that these correlations are all equal to\n",
      "λ(α) =(1−α)√\n",
      "(1−α)2+α(2−α)\n",
      "N·RSS·λ,\n",
      "and hence they decrease monotonically to zero.\n",
      "(c) Use these results to show that the LAR algorithm in Sectio n 3.4.4\n",
      "keeps the correlations tied and monotonically decreasing, as claimed\n",
      "in (3.55).\n",
      "Ex. 3.24 LAR directions. Using the notation around equation (3.55) on\n",
      "page 74, show that the LAR direction makes an equal angle with each of\n",
      "the predictors in Ak.\n",
      "Ex. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\n",
      "ginning of the kth step of the LAR algorithm, derive expressions to identify\n",
      "the next variable to enter the active set at step k+1, and the value of αat\n",
      "which this occurs (using the notation around equation (3.55 ) on page 74).\n",
      "Ex. 3.26 Forward stepwise regression enters the variable at each ste p that\n",
      "most reduces the residual sum-of-squares. LAR adjusts vari ables that have\n",
      "the most (absolute) correlation with the current residuals . Show that these\n",
      "two entry criteria are not necessarily the same. [Hint: let xj.Abe thejth\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"98 3. Linear Methods for Regression\n",
      "variable, linearly adjusted for all the variables currentl y in the model. Show\n",
      "that the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\n",
      "is largest in magnitude.\n",
      "Ex. 3.27 Lasso and LAR : Consider thelassoprobleminLagrange multiplier\n",
      "form: with L(β) =1\n",
      "2∑\n",
      "i(yi−∑\n",
      "jxijβj)2, we minimize\n",
      "L(β)+λ∑\n",
      "j|βj| (3.88)\n",
      "for ﬁxedλ>0.\n",
      "(a) Setting βj=β+\n",
      "j−β−\n",
      "jwithβ+\n",
      "j,β−\n",
      "j≥0, expression (3.88) becomes\n",
      "L(β)+λ∑\n",
      "j(β+\n",
      "j+β−\n",
      "j). Show that the Lagrange dual function is\n",
      "L(β)+λ∑\n",
      "j(β+\n",
      "j+β−\n",
      "j)−∑\n",
      "jλ+\n",
      "jβ+\n",
      "j−∑\n",
      "jλ−\n",
      "jβ−\n",
      "j(3.89)\n",
      "and the Karush–Kuhn–Tucker optimality conditions are\n",
      "∇L(β)j+λ−λ+\n",
      "j= 0\n",
      "−∇L(β)j+λ−λ−\n",
      "j= 0\n",
      "λ+\n",
      "jβ+\n",
      "j= 0\n",
      "λ−\n",
      "jβ−\n",
      "j= 0,\n",
      "along with the non-negativity constraints on the parameter s and all\n",
      "the Lagrange multipliers.\n",
      "(b) Show that|∇L(β)j|≤λ∀j,and that the KKT conditions imply one\n",
      "of the following three scenarios:\n",
      "λ= 0⇒ ∇L(β)j= 0∀j\n",
      "β+\n",
      "j>0, λ>0⇒λ+\n",
      "j= 0,∇L(β)j=−λ<0, β−\n",
      "j= 0\n",
      "β−\n",
      "j>0, λ>0⇒λ−\n",
      "j= 0,∇L(β)j=λ>0, β+\n",
      "j= 0.\n",
      "Hence show that for any “active” predictor having βj̸= 0, we must\n",
      "have∇L(β)j=−λifβj>0, and∇L(β)j=λifβj<0. Assuming\n",
      "the predictors are standardized, relate λto the correlation between\n",
      "thejth predictor and the current residuals.\n",
      "(c) Suppose that the set of active predictors is unchanged fo rλ0≥λ≥λ1.\n",
      "Show that there is a vector γ0such that\n",
      "ˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\n",
      "Thus the lasso solution path is linear as λranges from λ0toλ1(Efron\n",
      "et al., 2004; Rosset and Zhu, 2007).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 99\n",
      "Ex. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\n",
      "variableXjisˆβj=a. Suppose we augment our set of variables with an\n",
      "identical copy X∗\n",
      "j=Xj. Characterize the eﬀect of this exact collinearity\n",
      "by describing the set of solutions for ˆβjandˆβ∗\n",
      "j, using the same value of t.\n",
      "Ex. 3.29 Suppose we run a ridge regression with parameter λon a single\n",
      "variableX, and get coeﬃcient a. We now include an exact copy X∗=X,\n",
      "and reﬁt our ridge regression. Show that both coeﬃcients are identical, and\n",
      "derive their value. Show in general that if mcopies of a variable Xjare\n",
      "included in a ridge regression, their coeﬃcients are all the same.\n",
      "Ex. 3.30 Consider the elastic-net optimization problem:\n",
      "min\n",
      "β||y−Xβ||2+λ[\n",
      "α||β||2\n",
      "2+(1−α)||β||1]\n",
      ". (3.91)\n",
      "Show how one can turn this into a lasso problem, using an augme nted\n",
      "version of Xandy.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"100 3. Linear Methods for Regression\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 101\n",
      "Printer: Opaque this\n",
      "4\n",
      "Linear Methods for Classiﬁcation\n",
      "4.1 Introduction\n",
      "In this chapter we revisit the classiﬁcation problem and foc us on linear\n",
      "methods for classiﬁcation. Since our predictor G(x) takes values in a dis-\n",
      "crete setG, we can always divide the input space into a collection of reg ions\n",
      "labeledaccordingtotheclassiﬁcation.WesawinChapter2t hatthebound-\n",
      "aries of these regions can be rough or smooth, depending on th e prediction\n",
      "function. For an important class of procedures, these decision boundaries\n",
      "are linear; this is what we will mean by linear methods for cla ssiﬁcation.\n",
      "There are several diﬀerent ways in which linear decision bou ndaries can\n",
      "be found. In Chapter 2 we ﬁt linear regression models to the cl ass indicator\n",
      "variables, and classify to the largest ﬁt. Suppose there are Kclasses, for\n",
      "convenience labeled 1 ,2,...,K, and the ﬁtted linear model for the kth\n",
      "indicator response variable is ˆfk(x) =ˆβk0+ˆβT\n",
      "kx. The decision boundary\n",
      "between class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\n",
      "the set{x: (ˆβk0−ˆβℓ0)+(ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane.1\n",
      "Since the same is true for any pair of classes, the input space is divided\n",
      "into regions of constant classiﬁcation, with piecewise hyp erplanar decision\n",
      "boundaries. This regression approach is a member of a class o f methods\n",
      "that model discriminant functions δk(x) for each class, and then classify x\n",
      "to the class with the largest value for its discriminant func tion. Methods\n",
      "1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\n",
      "not. We sometimes ignore the distinction and refer in general to hyperpl anes.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"102 4. Linear Methods for Classiﬁcation\n",
      "that model the posterior probabilities Pr( G=k|X=x) are also in this\n",
      "class. Clearly, if either the δk(x) or Pr(G=k|X=x) are linear in x, then\n",
      "the decision boundaries will be linear.\n",
      "Actually, all we require is that some monotone transformati on ofδkor\n",
      "Pr(G=k|X=x) be linear for the decision boundaries to be linear. For\n",
      "example, if there are two classes, a popular model for the pos terior proba-\n",
      "bilities is\n",
      "Pr(G= 1|X=x) =exp(β0+βTx)\n",
      "1+exp(β0+βTx),\n",
      "Pr(G= 2|X=x) =1\n",
      "1+exp(β0+βTx).(4.1)\n",
      "Herethemonotonetransformationisthe logittransformation:log[ p/(1−p)],\n",
      "and in fact we see that\n",
      "logPr(G= 1|X=x)\n",
      "Pr(G= 2|X=x)=β0+βTx. (4.2)\n",
      "The decision boundary is the set of points for which the log-odds are zero,\n",
      "and this is a hyperplane deﬁned by{\n",
      "x|β0+βTx= 0}\n",
      ". We discuss two very\n",
      "popular but diﬀerent methods that result in linear log-odds or logits: linear\n",
      "discriminant analysis and linear logistic regression. Alt hough they diﬀer in\n",
      "their derivation, the essential diﬀerence between them is i n the way the\n",
      "linear function is ﬁt to the training data.\n",
      "A more direct approach is to explicitly model the boundaries between\n",
      "the classes as linear. For a two-class problem in a p-dimensional input\n",
      "space, this amounts to modeling the decision boundary as a hy perplane—in\n",
      "other words, a normal vector and a cut-point. We will look at t wo methods\n",
      "that explicitly look for “separating hyperplanes.” The ﬁrs t is the well-\n",
      "knownperceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\n",
      "a separating hyperplane in the training data, if one exists. The second\n",
      "method, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\n",
      "one exists, else ﬁnds a hyperplane that minimizes some measu re of overlap\n",
      "in the training data. We treat the separable case here, and de fer treatment\n",
      "of the nonseparable case to Chapter 12.\n",
      "Whilethisentirechapterisdevotedtolineardecisionboun daries,thereis\n",
      "considerable scope for generalization. For example, we can expand our vari-\n",
      "ablesetX1,...,X pbyincludingtheirsquaresandcross-products X2\n",
      "1,X2\n",
      "2,...,\n",
      "X1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\n",
      "in the augmented space map down to quadratic functions in the original\n",
      "space—hence linear decision boundaries to quadratic decisi on boundaries.\n",
      "Figure 4.1 illustrates the idea. The data are the same: the le ft plot uses\n",
      "linear decision boundaries in the two-dimensional space sh own, while the\n",
      "rightplotuseslineardecisionboundariesintheaugmented ﬁve-dimensional\n",
      "space described above. This approach can be used with any bas is transfor-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.2 Linear Regression of an Indicator Matrix 103\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "FIGURE 4.1. The left plot shows some data from three classes, with linear\n",
      "decision boundaries found by linear discriminant analysis. T he right plot shows\n",
      "quadratic decision boundaries. These were obtained by ﬁndi ng linear boundaries\n",
      "in the ﬁve-dimensional space X1,X2,X1X2,X2\n",
      "1,X2\n",
      "2. Linear inequalities in this\n",
      "space are quadratic inequalities in the original space.\n",
      "mationh(X) whereh: IRp↦→IRqwithq>p, and will be explored in later\n",
      "chapters.\n",
      "4.2 Linear Regression of an Indicator Matrix\n",
      "Here each of the response categories are coded via an indicat or variable.\n",
      "Thus ifGhasKclasses, there will be Ksuch indicators Yk, k= 1,...,K,\n",
      "withYk= 1 ifG=kelse 0. These are collected together in a vector\n",
      "Y= (Y1,...,Y K), and theNtraining instances of these form an N×K\n",
      "indicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\n",
      "having a single 1. We ﬁt a linear regression model to each of th e columns\n",
      "ofYsimultaneously, and the ﬁt is given by\n",
      "ˆY=X(XTX)−1XTY. (4.3)\n",
      "Chapter 3 has more details on linear regression. Note that we have a coeﬃ-\n",
      "cient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient\n",
      "matrixˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\n",
      "corresponding to the pinputs, and a leading column of 1’s for the intercept.\n",
      "A new observation with input xis classiﬁed as follows:\n",
      "•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\n",
      "•identify the largest component and classify accordingly:\n",
      "ˆG(x) = argmaxk∈Gˆfk(x). (4.4)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"104 4. Linear Methods for Classiﬁcation\n",
      "What is the rationale for this approach? One rather formal ju stiﬁcation\n",
      "is to view the regression as an estimate of conditional expec tation. For the\n",
      "random variable Yk,E(Yk|X=x) = Pr(G=k|X=x), so conditional\n",
      "expectation of each of the Ykseems a sensible goal. The real issue is: how\n",
      "good an approximation to conditional expectation is the rat her rigid linear\n",
      "regression model? Alternatively, are the ˆfk(x) reasonable estimates of the\n",
      "posterior probabilities Pr( G=k|X=x), and more importantly, does this\n",
      "matter?\n",
      "It is quite straightforward to verify that∑\n",
      "k∈Gˆfk(x) = 1 for any x, as\n",
      "long as there is an intercept in the model (column of 1’s in X). However,\n",
      "theˆfk(x) can be negative or greater than 1, and typically some are. Th is\n",
      "is a consequence of the rigid nature of linear regression, es pecially if we\n",
      "make predictions outside the hull of the training data. Thes e violations in\n",
      "themselves do not guarantee that this approach will not work , and in fact\n",
      "on many problems it gives similar results to more standard li near meth-\n",
      "ods for classiﬁcation. If we allow linear regression onto ba sis expansions\n",
      "h(X) of the inputs, this approach can lead to consistent estimat es of the\n",
      "probabilities. As the size of the training set Ngrows bigger, we adaptively\n",
      "include more basis elements so that linear regression onto t hese basis func-\n",
      "tions approaches conditional expectation. We discuss such approaches in\n",
      "Chapter 5.\n",
      "A more simplistic viewpoint is to construct targetstkfor each class,\n",
      "wheretkis thekth column of the K×Kidentity matrix. Our prediction\n",
      "problem is to try and reproduce the appropriate target for an observation.\n",
      "With the same coding as before, the response vector yi(ith row of Y) for\n",
      "observation ihas the value yi=tkifgi=k. We might then ﬁt the linear\n",
      "model by least squares:\n",
      "min\n",
      "BN∑\n",
      "i=1||yi−[(1,xT\n",
      "i)B]T||2. (4.5)\n",
      "The criterion is a sum-of-squared Euclidean distances of th e ﬁtted vectors\n",
      "from their targets. A new observation is classiﬁed by comput ing its ﬁtted\n",
      "vectorˆf(x) and classifying to the closest target:\n",
      "ˆG(x) = argmin\n",
      "k||ˆf(x)−tk||2. (4.6)\n",
      "This is exactly the same as the previous approach:\n",
      "•The sum-of-squared-norm criterion is exactly the criterio n for multi-\n",
      "ple response linear regression, just viewed slightly diﬀer ently. Since\n",
      "a squared norm is itself a sum of squares, the components deco uple\n",
      "and can be rearranged as a separate linear model for each elem ent.\n",
      "Note that this is only possible because there is nothing in th e model\n",
      "that binds the diﬀerent responses together.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.2 Linear Regression of an Indicator Matrix 105\n",
      "Linear Regression\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11111\n",
      "111\n",
      "11 111\n",
      "111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11111\n",
      "111\n",
      "11\n",
      "1111\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111111\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "1 1\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "11111\n",
      "1\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111 11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "111 111\n",
      "11\n",
      "11\n",
      "11\n",
      "111111\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11 1\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "111 1\n",
      "11\n",
      "1\n",
      "11 11\n",
      "1\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "1111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "1\n",
      "1111111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1 12\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22222\n",
      "2222\n",
      "2\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2222222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2 2\n",
      "22\n",
      "2222\n",
      "2\n",
      "2\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "2222\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22222\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222 22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22 222\n",
      "222\n",
      "2\n",
      "22\n",
      "22 2\n",
      "2\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22 22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "2\n",
      "2 2222 2\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22 2\n",
      "22\n",
      "2\n",
      "22\n",
      "22222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2 22 222\n",
      "22\n",
      "22\n",
      "22 2\n",
      "22 22\n",
      "222\n",
      "22\n",
      "22\n",
      "22 22\n",
      "22\n",
      "223\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33333\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "333\n",
      "333\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333\n",
      "333\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "33333333333\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "3 33\n",
      "333\n",
      "3\n",
      "33 33\n",
      "3\n",
      "33\n",
      "333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "33333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33 3\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333333\n",
      "3333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "33 3\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33Linear Discriminant Analysis\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11111\n",
      "111\n",
      "11 111\n",
      "111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11111\n",
      "111\n",
      "11\n",
      "1111\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111111\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "1 1\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "11111\n",
      "1\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111 11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "111 111\n",
      "11\n",
      "11\n",
      "11\n",
      "111111\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11 1\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "111 1\n",
      "11\n",
      "1\n",
      "11 11\n",
      "1\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "1111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "1\n",
      "1111111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1 12\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22222\n",
      "2222\n",
      "2\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2222222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2 2\n",
      "22\n",
      "2222\n",
      "2\n",
      "2\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "2222\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22222\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222 22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22 222\n",
      "222\n",
      "2\n",
      "22\n",
      "22 2\n",
      "2\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22 22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "2\n",
      "2 2222 2\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22 2\n",
      "22\n",
      "2\n",
      "22\n",
      "22222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2 22 222\n",
      "22\n",
      "22\n",
      "22 2\n",
      "22 22\n",
      "222\n",
      "22\n",
      "22\n",
      "22 22\n",
      "22\n",
      "223\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33333\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "333\n",
      "333\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3\n",
      "33\n",
      "33\n",
      "33333\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "333\n",
      "333\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333\n",
      "333\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "33333333333\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "3 33\n",
      "333\n",
      "3\n",
      "33 33\n",
      "3\n",
      "33\n",
      "333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "33333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33 3\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333333\n",
      "3333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "33 3\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "X1 X1\n",
      "X2X2\n",
      "FIGURE 4.2. The data come from three classes in IR2and are easily separated\n",
      "by linear decision boundaries. The right plot shows the bounda ries found by linear\n",
      "discriminant analysis. The left plot shows the boundaries foun d by linear regres-\n",
      "sion of the indicator response variables. The middle class is co mpletely masked\n",
      "(never dominates).\n",
      "•The closest target classiﬁcation rule (4.6) is easily seen t o be exactly\n",
      "the same as the maximum ﬁtted component criterion (4.4).\n",
      "Thereisaseriousproblemwiththeregressionapproachwhen thenumber\n",
      "of classesK≥3, especially prevalent when Kis large. Because of the rigid\n",
      "nature of the regression model, classes can be maskedby others. Figure 4.2\n",
      "illustrates anextremesituation when K= 3.Thethreeclasses areperfectly\n",
      "separated by linear decision boundaries, yet linear regres sion misses the\n",
      "middle class completely.\n",
      "In Figure 4.3 we have projected the data onto the line joining the three\n",
      "centroids (there is no information in the orthogonal direct ion in this case),\n",
      "and we have included and coded the three response variables Y1,Y2and\n",
      "Y3. The three regression lines (left panel) are included, and w e see that\n",
      "the line corresponding to the middle class is horizontal and its ﬁtted values\n",
      "are never dominant! Thus, observations from class 2 are clas siﬁed either\n",
      "as class 1 or class 3. The right panel uses quadratic regressi on rather than\n",
      "linear regression. For this simple example a quadratic rath er than linear\n",
      "ﬁt (for the middle class at least) would solve the problem. Ho wever, it\n",
      "can be seen that if there were four rather than three classes l ined up like\n",
      "this, a quadratic would not come down fast enough, and a cubic would\n",
      "be needed as well. A loose but general rule is that if K≥3 classes are\n",
      "lined up, polynomial terms up to degree K−1 might be needed to resolve\n",
      "them. Note also that these are polynomials along the derived direction\n",
      "passing through the centroids, which can have arbitrary ori entation. So in\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"106 4. Linear Methods for Classiﬁcation\n",
      "111\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11111\n",
      "111\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "111\n",
      "111111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "12222 2222222 2 2 222 2 2 22 2222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222 222 22 2 222222222 22222222 222 222 2 22222222222222 22\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3333\n",
      "33333\n",
      "3333\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "33333\n",
      "0.00.51.0\n",
      "0.0 0.2 0.4 0.6 0.8 1.0111\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "111\n",
      "111111\n",
      "111\n",
      "11111\n",
      "111111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "1111111 111\n",
      "111111111111\n",
      "1 111 1111111 11111111 111111112\n",
      "2\n",
      "22\n",
      "2\n",
      "2222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "2\n",
      "22222\n",
      "2\n",
      "22\n",
      "222\n",
      "22\n",
      "222222\n",
      "222222\n",
      "2\n",
      "22222\n",
      "2\n",
      "22 2\n",
      "222222222 222222\n",
      "22\n",
      "22222222 222222222 22 22\n",
      "22 22\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "222\n",
      "222\n",
      "233 3333\n",
      "333333\n",
      "3333\n",
      "33333333\n",
      "333333\n",
      "33\n",
      "33333\n",
      "3 333333333333 3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333 333\n",
      "33333\n",
      "33\n",
      "3\n",
      "333333333\n",
      "33\n",
      "3333\n",
      "33333\n",
      "3333\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "33333\n",
      "0.00.51.0\n",
      "0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\n",
      "FIGURE 4.3. The eﬀects of masking on linear regression in IRfor a three-class\n",
      "problem. The rug plot at the base indicates the positions and class membership of\n",
      "each observation. The three curves in each panel are the ﬁtte d regressions to the\n",
      "three-class indicator variables; for example, for the blue class ,yblueis1for the\n",
      "blue observations, and 0for the green and orange. The ﬁts are linear and quadratic\n",
      "polynomials. Above each plot is the training error rate. The Bay es error rate is\n",
      "0.025for this problem, as is the LDA error rate.\n",
      "p-dimensional input space, one would need general polynomia l terms and\n",
      "cross-products of total degree K−1,O(pK−1) terms in all, to resolve such\n",
      "worst-case scenarios.\n",
      "The example is extreme, but for large Kand smallpsuch maskings\n",
      "naturally occur. As a more realistic illustration, Figure 4 .4 is a projection\n",
      "of the training data for a vowel recognition problem onto an i nformative\n",
      "two-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\n",
      "This is a diﬃcult classiﬁcation problem, and the best method s achieve\n",
      "around 40% errors on the test data. The main point here is summ arized in\n",
      "Table 4.1; linear regression has an error rate of 67%, while a close relative,\n",
      "lineardiscriminantanalysis,hasanerrorrateof56%.Itse emsthatmasking\n",
      "has hurt in this case. While all the other methods in this chap ter are based\n",
      "on linear functions of xas well, they use them in such a way that avoids\n",
      "this masking problem.\n",
      "4.3 Linear Discriminant Analysis\n",
      "Decision theory for classiﬁcation (Section 2.4) tells us th at we need to know\n",
      "the class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\n",
      "the class-conditional density of Xin classG=k, and letπkbe the prior\n",
      "probability of class k, with∑K\n",
      "k=1πk= 1. A simple application of Bayes\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 107\n",
      "Coordinate 1 for Training DataCoordinate 2 for Training Data\n",
      "-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o ooooooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooooo o\n",
      "o\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooooooooooo\n",
      "oooooooooooooooooo\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oooooooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooooooooo oooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooooooooo\n",
      "o ooooooooooooooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "oooooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "••••••••••••••\n",
      "••\n",
      "••\n",
      "••••Linear Discriminant Analysis\n",
      "FIGURE 4.4. A two-dimensional plot of the vowel training data. There are\n",
      "eleven classes with X∈IR10, and this is the best view in terms of a LDA model\n",
      "(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\n",
      "The class overlap is considerable.\n",
      "TABLE 4.1. Training and test error rates using a variety of linear techni ques\n",
      "on the vowel data. There are eleven classes in ten dimensions, o f which three\n",
      "account for 90%of the variance (via a principal components analysis). We see\n",
      "that linear regression is hurt by masking, increasing the tes t and training error\n",
      "by over10%.\n",
      "Technique Error Rates\n",
      "Training Test\n",
      "Linear regression 0.48 0.67\n",
      "Linear discriminant analysis 0.32 0.56\n",
      "Quadratic discriminant analysis 0.01 0.53\n",
      "Logistic regression 0.22 0.51\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"108 4. Linear Methods for Classiﬁcation\n",
      "theorem gives us\n",
      "Pr(G=k|X=x) =fk(x)πk∑K\n",
      "ℓ=1fℓ(x)πℓ. (4.7)\n",
      "We see that in terms of ability to classify, having the fk(x) is almost equiv-\n",
      "alent to having the quantity Pr( G=k|X=x).\n",
      "Many techniques are based on models for the class densities:\n",
      "•linear and quadratic discriminant analysis use Gaussian de nsities;\n",
      "•moreﬂexiblemixturesofGaussiansallowfornonlineardeci sionbound-\n",
      "aries (Section 6.8);\n",
      "•general nonparametric density estimates for each class den sity allow\n",
      "the most ﬂexibility (Section 6.6.2);\n",
      "•Naive Bayes models are a variant of the previous case, and assume\n",
      "that each of the class densities are products of marginal den sities;\n",
      "that is, they assume that the inputs are conditionally indep endent in\n",
      "each class (Section 6.6.3).\n",
      "Suppose that we model each class density as multivariate Gau ssian\n",
      "fk(x) =1\n",
      "(2π)p/2|Σk|1/2e−1\n",
      "2(x−µk)TΣ−1\n",
      "k(x−µk). (4.8)\n",
      "Linear discriminant analysis (LDA) arises in the special ca se when we\n",
      "assume that the classes have a common covariance matrix Σk=Σ∀k. In\n",
      "comparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\n",
      "we see that\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=ℓ|X=x)= logfk(x)\n",
      "fℓ(x)+logπk\n",
      "πℓ\n",
      "= logπk\n",
      "πℓ−1\n",
      "2(µk+µℓ)TΣ−1(µk−µℓ)\n",
      "+xTΣ−1(µk−µℓ),(4.9)\n",
      "an equation linear in x. The equal covariance matrices cause the normal-\n",
      "ization factors to cancel, as well as the quadratic part in th e exponents.\n",
      "This linear log-odds function implies that the decision bou ndary between\n",
      "classeskandℓ—the set where Pr( G=k|X=x) = Pr(G=ℓ|X=x)—is\n",
      "linear inx; inpdimensions a hyperplane. This is of course true for any pair\n",
      "of classes, so all the decision boundaries are linear. If we d ivide IRpinto\n",
      "regions that are classiﬁed as class 1, class 2, etc., these re gions will be sep-\n",
      "arated by hyperplanes. Figure 4.5 (left panel) shows an idea lized example\n",
      "with three classes and p= 2. Here the data do arise from three Gaus-\n",
      "sian distributions with a common covariance matrix. We have included in\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 109\n",
      "+++\n",
      "3\n",
      "21\n",
      "11\n",
      "233\n",
      "3\n",
      "123\n",
      "32\n",
      "11211\n",
      "33\n",
      "12 1\n",
      "23\n",
      "23\n",
      "3\n",
      "12\n",
      "211\n",
      "1\n",
      "13\n",
      "222\n",
      "21 3\n",
      "2 23\n",
      "13\n",
      "13\n",
      "32\n",
      "13\n",
      "3\n",
      "23\n",
      "133\n",
      "2133\n",
      "22\n",
      "3\n",
      "22\n",
      "21\n",
      "11\n",
      "11\n",
      "2\n",
      "133\n",
      "1\n",
      "13\n",
      "32\n",
      "222 3\n",
      "12\n",
      "FIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\n",
      "covariance and diﬀerent means. Included are the contours of c onstant density\n",
      "enclosing 95% of the probability in each case. The Bayes decision boundari es\n",
      "between each pair of classes are shown (broken straight lines) , and the Bayes\n",
      "decision boundaries separating all three classes are the thi cker solid lines (a subset\n",
      "of the former). On the right we see a sample of 30drawn from each Gaussian\n",
      "distribution, and the ﬁtted LDA decision boundaries.\n",
      "the ﬁgure the contours corresponding to 95% highest probabi lity density,\n",
      "as well as the class centroids. Notice that the decision boun daries are not\n",
      "the perpendicular bisectors of the line segments joining th e centroids. This\n",
      "would be the case if the covariance Σwere spherical σ2I, and the class\n",
      "priors were equal. From (4.9) we see that the linear discriminant functions\n",
      "δk(x) =xTΣ−1µk−1\n",
      "2µT\n",
      "kΣ−1µk+logπk (4.10)\n",
      "areanequivalentdescriptionofthedecisionrule,with G(x) = argmaxkδk(x).\n",
      "In practice we do not know the parameters of the Gaussian dist ributions,\n",
      "and will need to estimate them using our training data:\n",
      "•ˆπk=Nk/N, whereNkis the number of class- kobservations;\n",
      "•ˆµk=∑\n",
      "gi=kxi/Nk;\n",
      "•ˆΣ=∑K\n",
      "k=1∑\n",
      "gi=k(xi−ˆµk)(xi−ˆµk)T/(N−K).\n",
      "Figure 4.5 (right panel) shows the estimated decision bound aries based on\n",
      "a sample of size 30 each from three Gaussian distributions. F igure 4.1 on\n",
      "page 103 is another example, but here the classes are not Gaus sian.\n",
      "With two classes there is a simple correspondence between li near dis-\n",
      "criminant analysis and classiﬁcation by linear regression , as in (4.5). The\n",
      "LDA rule classiﬁes to class 2 if\n",
      "xTˆΣ−1(ˆµ2−ˆµ1)>1\n",
      "2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),(4.11)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"110 4. Linear Methods for Classiﬁcation\n",
      "and class 1 otherwise. Suppose we code the targets in the two c lasses as +1\n",
      "and−1, respectively. It is easy to show that the coeﬃcient vector from least\n",
      "squares is proportional to the LDA direction given in (4.11) (Exercise 4.2).\n",
      "[In fact, this correspondence occurs for any (distinct) cod ing of the targets;\n",
      "see Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\n",
      "hence the resulting decision rules are diﬀerent.\n",
      "SincethisderivationoftheLDAdirectionvialeastsquares doesnotusea\n",
      "Gaussian assumption for the features, its applicability ex tends beyond the\n",
      "realm of Gaussian data. However the derivation of the partic ular intercept\n",
      "or cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\n",
      "sense to instead choose the cut-point that empirically mini mizes training\n",
      "error for a given dataset. This is something we have found to w ork well in\n",
      "practice, but have not seen it mentioned in the literature.\n",
      "With more than two classes, LDA is not the same as linear regre ssion of\n",
      "the class indicator matrix, and it avoids the masking proble ms associated\n",
      "with that approach (Hastie et al., 1994). A correspondence b etween regres-\n",
      "sion and LDA can be established through the notion of optimal scoring ,\n",
      "discussed in Section 12.5.\n",
      "Getting back to the general discriminant problem (4.8), if t heΣkare\n",
      "not assumed to be equal, then the convenient cancellations i n (4.9) do not\n",
      "occur; in particular the pieces quadratic in xremain. We then get quadratic\n",
      "discriminant functions (QDA),\n",
      "δk(x) =−1\n",
      "2log|Σk|−1\n",
      "2(x−µk)TΣ−1\n",
      "k(x−µk)+logπk.(4.12)\n",
      "The decision boundary between each pair of classes kandℓis described by\n",
      "a quadratic equation {x:δk(x) =δℓ(x)}.\n",
      "Figure 4.6 shows an example (from Figure 4.1 on page 103) wher e the\n",
      "three classes are Gaussian mixtures (Section 6.8) and the de cision bound-\n",
      "aries are approximated by quadratic equations in x. Here we illustrate\n",
      "two popular ways of ﬁtting these quadratic boundaries. The r ight plot\n",
      "uses QDA as described here, while the left plot uses LDA in the enlarged\n",
      "ﬁve-dimensional quadratic polynomial space. The diﬀerenc es are generally\n",
      "small; QDA is the preferred approach, with the LDA method a co nvenient\n",
      "substitute2.\n",
      "TheestimatesforQDAaresimilartothoseforLDA,excepttha tseparate\n",
      "covariance matrices must be estimated for each class. When pis large this\n",
      "can mean a dramatic increase in parameters. Since the decisi on boundaries\n",
      "are functions of the parameters of the densities, counting t he number of\n",
      "parameters must be done with care. For LDA, it seems there are (K−\n",
      "1)×(p+1) parameters, since we only need the diﬀerences δk(x)−δK(x)\n",
      "2For this ﬁgure and many similar ﬁgures in the book we compute the d ecision bound-\n",
      "aries by an exhaustive contouring method. We compute the decision rule on a ﬁne lattice\n",
      "of points, and then use contouring algorithms to compute the bound aries.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "FIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot sho ws\n",
      "the quadratic decision boundaries for the data in Figure 4.1 (obtained using LDA\n",
      "in the ﬁve-dimensional space X1,X2,X1X2,X2\n",
      "1,X2\n",
      "2). The right plot shows the\n",
      "quadratic decision boundaries found by QDA. The diﬀerences are small, as is\n",
      "usually the case.\n",
      "between the discriminant functions where Kis some pre-chosen class (here\n",
      "we have chosen the last), and each diﬀerence requires p+ 1 parameters3.\n",
      "Likewise for QDA there will be ( K−1)×{p(p+ 3)/2 + 1}parameters.\n",
      "Both LDA and QDA perform well on an amazingly large and divers e set\n",
      "of classiﬁcation tasks. For example, in the STATLOG project (Michie et\n",
      "al., 1994) LDA was among the top three classiﬁers for 7 of the 2 2 datasets,\n",
      "QDA among the top three for four datasets, and one of the pair w ere in the\n",
      "top threefor 10datasets. Both techniques arewidely used,a ndentirebooks\n",
      "are devoted to LDA. It seems that whatever exotic tools are th e rage of the\n",
      "day, we should always have available these two simple tools. The question\n",
      "arises why LDA and QDA have such a good track record. The reaso n is not\n",
      "likely to be that the data are approximately Gaussian, and in addition for\n",
      "LDA that the covariances are approximately equal. More like ly a reason is\n",
      "that the data can only support simple decision boundaries su ch as linear or\n",
      "quadratic, and the estimates provided via the Gaussian mode ls are stable.\n",
      "This is a bias variance tradeoﬀ—we can put up with the bias of a l inear\n",
      "decision boundary because it can be estimated with much lowe r variance\n",
      "than more exotic alternatives. This argument is less believ able for QDA,\n",
      "since it can have many parameters itself, although perhaps f ewer than the\n",
      "non-parametric alternatives.\n",
      "3Althoughweﬁtthecovariancematrix ˆΣtocomputetheLDAdiscriminantfunctions,\n",
      "a much reduced function of it is all that is required to estimate the O(p) parameters\n",
      "needed to compute the decision boundaries.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"112 4. Linear Methods for Classiﬁcation\n",
      "Misclassification Rate\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\n",
      "Test Data\n",
      "Train Data\n",
      "α\n",
      "FIGURE 4.7. Test and training errors for the vowel data, using regularize d\n",
      "discriminant analysis with a series of values of α∈[0,1]. The optimum for the\n",
      "test data occurs around α= 0.9, close to quadratic discriminant analysis.\n",
      "4.3.1 Regularized Discriminant Analysis\n",
      "Friedman (1989) proposed a compromise between LDA and QDA, w hich\n",
      "allows one to shrink the separate covariances of QDA toward a common\n",
      "covariance as in LDA. These methods are very similar in ﬂavor to ridge\n",
      "regression. The regularized covariance matrices have the f orm\n",
      "ˆΣk(α) =αˆΣk+(1−α)ˆΣ, (4.13)\n",
      "whereˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]\n",
      "allows a continuum of models between LDA and QDA, and needs to be\n",
      "speciﬁed. In practice αcan be chosen based on the performance of the\n",
      "model on validation data, or by cross-validation.\n",
      "Figure 4.7 shows the results of RDA applied to the vowel data. Both\n",
      "the training and test error improve with increasing α, although the test\n",
      "error increases sharply after α= 0.9. The large discrepancy between the\n",
      "training and test error is partly due to the fact that there ar e many repeat\n",
      "measurements on a small number of individuals, diﬀerent in t he training\n",
      "and test set.\n",
      "Similar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\n",
      "covariance,\n",
      "ˆΣ(γ) =γˆΣ+(1−γ)ˆσ2I (4.14)\n",
      "forγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\n",
      "of covariances ˆΣ(α,γ) indexed by a pair of parameters.\n",
      "In Chapter 12, we discuss other regularized versions of LDA, which are\n",
      "more suitable when the data arise from digitized analog sign als and images.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 113\n",
      "Inthesesituationsthefeaturesarehigh-dimensionalandc orrelated,andthe\n",
      "LDA coeﬃcients can be regularized to be smooth or sparse in th e original\n",
      "domain of the signal. This leads to better generalization an d allows for\n",
      "easier interpretation of the coeﬃcients. In Chapter 18 we al so deal with\n",
      "very high-dimensional problems, where for example the feat ures are gene-\n",
      "expression measurements in microarray studies. There the m ethods focus\n",
      "on the case γ= 0 in (4.14), and other severely regularized versions of LDA .\n",
      "4.3.2 Computations for LDA\n",
      "As a lead-in to the next topic, we brieﬂy digress on the comput ations\n",
      "required for LDA and especially QDA. Their computations are simpliﬁed\n",
      "by diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\n",
      "decomposition for each ˆΣk=UkDkUT\n",
      "k, whereUkisp×porthonormal,\n",
      "andDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\n",
      "forδk(x) (4.12) are\n",
      "•(x−ˆµk)TˆΣ−1\n",
      "k(x−ˆµk) = [UT\n",
      "k(x−ˆµk)]TD−1\n",
      "k[UT\n",
      "k(x−ˆµk)];\n",
      "•log|ˆΣk|=∑\n",
      "ℓlogdkℓ.\n",
      "In light of the computational steps outlined above, the LDA c lassiﬁer\n",
      "can be implemented by the following pair of steps:\n",
      "•Spherethe data with respect to the common covariance estimate ˆΣ:\n",
      "X∗←D−1\n",
      "2UTX, whereˆΣ=UDUT. The common covariance esti-\n",
      "mate ofX∗will now be the identity.\n",
      "•Classifytotheclosestclass centroidinthetransformedsp ace,modulo\n",
      "the eﬀect of the class prior probabilities πk.\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis\n",
      "So far we have discussed LDA as a restricted Gaussian classiﬁ er. Part of\n",
      "its popularity is due to an additional restriction that allo ws us to view\n",
      "informative low-dimensional projections of the data.\n",
      "TheKcentroids in p-dimensional input space lie in an aﬃne subspace\n",
      "of dimension≤K−1, and ifpis much larger than K, this will be a con-\n",
      "siderable drop in dimension. Moreover, in locating the clos est centroid, we\n",
      "can ignore distances orthogonal to this subspace, since the y will contribute\n",
      "equally to each class. Thus we might just as well project the X∗onto this\n",
      "centroid-spanning subspace HK−1, and make distance comparisons there.\n",
      "Thus there is a fundamental dimension reduction in LDA, name ly, that we\n",
      "need only consider the data in a subspace of dimension at most K−1.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"114 4. Linear Methods for Classiﬁcation\n",
      "IfK= 3, for instance, this could allow us to view the data in a two-\n",
      "dimensional plot, color-coding the classes. In doing so we w ould not have\n",
      "relinquished any of the information needed for LDA classiﬁc ation.\n",
      "What ifK >3? We might then ask for a L<K−1 dimensional subspace\n",
      "HL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\n",
      "mean that the projected centroids were spread out as much as p ossible in\n",
      "terms of variance. This amounts to ﬁnding principal compone nt subspaces\n",
      "of the centroids themselves (principal components are desc ribed brieﬂy in\n",
      "Section3.5.1,andinmoredetailinSection14.5.1).Figure 4.4showssuchan\n",
      "optimal two-dimensional subspace for the vowel data. Here t here are eleven\n",
      "classes, each a diﬀerent vowel sound, in a ten-dimensional i nput space. The\n",
      "centroids require the full space in this case, since K−1 =p, but we have\n",
      "shown an optimal two-dimensional subspace. The dimensions are ordered,\n",
      "sowecancomputeadditionaldimensionsinsequence.Figure 4.8showsfour\n",
      "additional pairs of coordinates, also known as canonical ordiscriminant\n",
      "variables. In summary then, ﬁnding the sequences of optimal subspaces\n",
      "for LDA involves the following steps:\n",
      "•compute the K×pmatrix of class centroids Mand the common\n",
      "covariance matrix W(forwithin-class covariance);\n",
      "•compute M∗=MW−1\n",
      "2using the eigen-decomposition of W;\n",
      "•compute B∗,thecovariancematrixof M∗(Bforbetween-class covari-\n",
      "ance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\n",
      "v∗\n",
      "ℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\n",
      "optimal subspaces.\n",
      "Combining all these operations the ℓthdiscriminant variable is given by\n",
      "Zℓ=vT\n",
      "ℓXwithvℓ=W−1\n",
      "2v∗\n",
      "ℓ.\n",
      "Fisher arrived at this decomposition via a diﬀerent route, w ithout refer-\n",
      "ring to Gaussian distributions at all. He posed the problem:\n",
      "Find the linear combination Z=aTXsuch that the between-\n",
      "class variance is maximized relative to the within-class va riance.\n",
      "Again, the between class variance is the variance of the clas s means of\n",
      "Z, and the within class variance is the pooled variance about t he means.\n",
      "Figure 4.9 shows why this criterion makes sense. Although th e direction\n",
      "joining the centroids separates the means as much as possibl e (i.e., max-\n",
      "imizes the between-class variance), there is considerable overlap between\n",
      "the projected classes due to the nature of the covariances. B y taking the\n",
      "covariance into account as well, a direction with minimum ov erlap can be\n",
      "found.\n",
      "The between-class variance of ZisaTBaand the within-class variance\n",
      "aTWa, whereWis deﬁned earlier, and Bis the covariance matrix of the\n",
      "class centroid matrix M. Note that B+W=T, where Tis thetotal\n",
      "covariance matrix of X, ignoring class information.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 115\n",
      "Coordinate 1 Coordinate 3 \n",
      "-4 -2 0 2 4-2 0 2o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooo ooooooo\n",
      "o\n",
      "o\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo o\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooo\n",
      "oo oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooooooo oooo ooooooo\n",
      "ooooooo\n",
      "••••••••\n",
      "••••••••••••••\n",
      "Coordinate 2 Coordinate 3 \n",
      "-6 -4 -2 0 2 4-2 0 2o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooo ooooooo\n",
      "o\n",
      "o\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo o\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooo\n",
      "oo oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooooooo ooooooooooo\n",
      "ooooooo\n",
      "••••••••\n",
      "••••••••••••••\n",
      "Coordinate 1 Coordinate 7 \n",
      "-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooooooooo\n",
      "oo oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooooo\n",
      "o\n",
      "ooooooooo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooooooo\n",
      "ooo o\n",
      "ooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "ooooooooooo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "oooo o\n",
      "ooooooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "ooooooooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "••••••••••••••••••••••\n",
      "Coordinate 9 Coordinate 10 \n",
      "-2 -1 0 1 2 3-2 -1 0 1 2oo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oooooooooooo o\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooooooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooooooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "oooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooooooooo\n",
      "oooo\n",
      "o\n",
      "oooooooooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooooooooo\n",
      "oooo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooooooooooo\n",
      "ooo\n",
      "o\n",
      "o ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o••••••••••••••••••••••Linear Discriminant Analysis\n",
      "FIGURE 4.8. Four projections onto pairs of canonical variates. Notice t hat as\n",
      "the rank of the canonical variates increases, the centroids become less spread out.\n",
      "In the lower right panel they appear to be superimposed, and th e classes most\n",
      "confused.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"116 4. Linear Methods for Classiﬁcation\n",
      "++\n",
      "++\n",
      "FIGURE 4.9. Although the line joining the centroids deﬁnes the direction o f\n",
      "greatest centroid spread, the projected data overlap becaus e of the covariance\n",
      "(left panel). The discriminant direction minimizes this over lap for Gaussian data\n",
      "(right panel).\n",
      "Fisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\n",
      "max\n",
      "aaTBa\n",
      "aTWa, (4.15)\n",
      "or equivalently\n",
      "max\n",
      "aaTBasubject toaTWa= 1. (4.16)\n",
      "This is a generalized eigenvalue problem, with agiven by the largest\n",
      "eigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\n",
      "a1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\n",
      "a2, orthogonal in Wtoa1, such that aT\n",
      "2Ba2/aT\n",
      "2Wa2is maximized; the\n",
      "solution is a2=v2, and so on. The aℓare referred to as discriminant\n",
      "coordinates , not to be confused with discriminant functions. They are al so\n",
      "referred to as canonical variates , since an alternative derivation of these\n",
      "results is through a canonical correlation analysis of the i ndicator response\n",
      "matrixYon the predictor matrix X. This line is pursued in Section 12.5.\n",
      "To summarize the developments so far:\n",
      "•Gaussian classiﬁcation with common covariances leads to li near deci-\n",
      "sion boundaries. Classiﬁcation can be achieved by sphering the data\n",
      "with respect to W, and classifying to the closest centroid (modulo\n",
      "logπk) in the sphered space.\n",
      "•Since only the relative distances to the centroids count, on e can con-\n",
      "ﬁne the data to the subspace spanned by the centroids in the sp hered\n",
      "space.\n",
      "•This subspace can be further decomposed into successively o ptimal\n",
      "subspaces in term of centroid separation. This decompositi on is iden-\n",
      "tical to the decomposition due to Fisher.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.3 Linear Discriminant Analysis 117\n",
      "DimensionMisclassification Rate\n",
      "2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\n",
      "•\n",
      "••••• • ••••\n",
      "•\n",
      "• •\n",
      "•\n",
      "•••••Test Data\n",
      "Train Data\n",
      "FIGURE 4.10. Training and test error rates for the vowel data, as a functio n\n",
      "of the dimension of the discriminant subspace. In this case t he best error rate is\n",
      "for dimension 2. Figure 4.11 shows the decision boundaries in this space.\n",
      "The reduced subspaces have been motivated as a data reductio n (for\n",
      "viewing) tool. Can they also be used for classiﬁcation, and w hat is the\n",
      "rationale? Clearly they can, as in our original derivation; we simply limit\n",
      "the distance-to-centroid calculations to the chosen subsp ace. One can show\n",
      "that this is a Gaussian classiﬁcation rule with the addition al restriction\n",
      "that the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\n",
      "Fitting such a model by maximum likelihood, and then constru cting the\n",
      "posterior probabilities using Bayes’ theorem amounts to th e classiﬁcation\n",
      "rule described above (Exercise 4.8).\n",
      "Gaussian classiﬁcation dictates the log πkcorrection factor in the dis-\n",
      "tance calculation. The reason for this correction can be see n in Figure 4.9.\n",
      "The misclassiﬁcation rate is based on the area of overlap bet ween the two\n",
      "densities. If the πkare equal (implicit in that ﬁgure), then the optimal\n",
      "cut-point is midway between the projected means. If the πkare not equal,\n",
      "moving the cut-point toward the smallerclass will improve the error rate.\n",
      "As mentioned earlier for two classes, one can derive the line ar rule using\n",
      "LDA (or any other method), and then choose the cut-point to mi nimize\n",
      "misclassiﬁcation error over the training data.\n",
      "As an example of the beneﬁt of the reduced-rank restriction, we return\n",
      "to the vowel data. There are 11 classes and 10 variables, and h ence 10\n",
      "possible dimensions for the classiﬁer. We can compute the tr aining and\n",
      "test error in each of these hierarchical subspaces; Figure 4 .10 shows the\n",
      "results. Figure 4.11 shows the decision boundaries for the c lassiﬁer based\n",
      "on the two-dimensional LDA solution.\n",
      "There is a close connection between Fisher’s reduced rank di scriminant\n",
      "analysis and regression of an indicator response matrix. It turns out that\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"118 4. Linear Methods for Classiﬁcation\n",
      "oooo\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "o o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o o\n",
      "oo\n",
      "o o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "Canonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\n",
      "••••••••••••••\n",
      "••\n",
      "••\n",
      "••••\n",
      "FIGURE 4.11. Decision boundaries for the vowel training data, in the two- di-\n",
      "mensional subspace spanned by the ﬁrst two canonical variat es. Note that in\n",
      "any higher-dimensional subspace, the decision boundaries are higher-dimensional\n",
      "aﬃne planes, and could not be represented as lines.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.4 Logistic Regression 119\n",
      "LDA amounts to the regression followed by an eigen-decompos ition of\n",
      "ˆYTY. In the case of two classes, there is a single discriminant va riable\n",
      "that is identical up to a scalar multiplication to either of t he columns of ˆY.\n",
      "These connections are developed in Chapter 12. A related fac t is that if one\n",
      "transforms the original predictors XtoˆY, then LDA using ˆYis identical\n",
      "to LDA in the original space (Exercise 4.3).\n",
      "4.4 Logistic Regression\n",
      "The logistic regression model arises from the desire to mode l the posterior\n",
      "probabilities of the Kclasses via linear functions in x, while at the same\n",
      "time ensuring that they sum to one and remain in [0 ,1]. The model has\n",
      "the form\n",
      "logPr(G= 1|X=x)\n",
      "Pr(G=K|X=x)=β10+βT\n",
      "1x\n",
      "logPr(G= 2|X=x)\n",
      "Pr(G=K|X=x)=β20+βT\n",
      "2x\n",
      "...\n",
      "logPr(G=K−1|X=x)\n",
      "Pr(G=K|X=x)=β(K−1)0+βT\n",
      "K−1x.(4.17)\n",
      "The model is speciﬁed in terms of K−1 log-odds or logit transformations\n",
      "(reﬂecting the constraint that the probabilities sum to one ). Although the\n",
      "model uses the last class as the denominator in the odds-rati os, the choice\n",
      "of denominator is arbitrary in that the estimates are equiva riant under this\n",
      "choice. A simple calculation shows that\n",
      "Pr(G=k|X=x) =exp(βk0+βT\n",
      "kx)\n",
      "1+∑K−1\n",
      "ℓ=1exp(βℓ0+βT\n",
      "ℓx), k= 1,...,K−1,\n",
      "Pr(G=K|X=x) =1\n",
      "1+∑K−1\n",
      "ℓ=1exp(βℓ0+βT\n",
      "ℓx), (4.18)\n",
      "and they clearly sum toone. Toemphasize the dependenceon th eentire pa-\n",
      "rameter set θ={β10,βT\n",
      "1,...,β (K−1)0,βT\n",
      "K−1}, we denote the probabilities\n",
      "Pr(G=k|X=x) =pk(x;θ).\n",
      "WhenK= 2, this model is especially simple, since there is only a sin gle\n",
      "linear function. It is widely used in biostatistical applic ations where binary\n",
      "responses(twoclasses)occurquitefrequently.Forexampl e,patientssurvive\n",
      "or die, have heart disease or not, or a condition is present or absent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"120 4. Linear Methods for Classiﬁcation\n",
      "4.4.1 Fitting Logistic Regression Models\n",
      "Logistic regression models are usually ﬁt by maximum likeli hood, using the\n",
      "conditional likelihood of GgivenX. Since Pr(G|X) completely speciﬁes the\n",
      "conditional distribution, the multinomial distribution is appropriate. The\n",
      "log-likelihood for Nobservations is\n",
      "ℓ(θ) =N∑\n",
      "i=1logpgi(xi;θ), (4.19)\n",
      "wherepk(xi;θ) = Pr(G=k|X=xi;θ).\n",
      "We discuss in detail the two-class case, since the algorithm s simplify\n",
      "considerably. It is convenient to code the two-class givia a 0/1 responseyi,\n",
      "whereyi= 1 whengi= 1, andyi= 0 whengi= 2. Letp1(x;θ) =p(x;θ),\n",
      "andp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\n",
      "ℓ(β) =N∑\n",
      "i=1{\n",
      "yilogp(xi;β)+(1−yi)log(1−p(xi;β))}\n",
      "=N∑\n",
      "i=1{\n",
      "yiβTxi−log(1+eβTxi)}\n",
      ". (4.20)\n",
      "Hereβ={β10,β1}, and we assume that the vector of inputs xiincludes\n",
      "the constant term 1 to accommodate the intercept.\n",
      "To maximize the log-likelihood, we set its derivatives to ze ro. These score\n",
      "equations are\n",
      "∂ℓ(β)\n",
      "∂β=N∑\n",
      "i=1xi(yi−p(xi;β)) = 0, (4.21)\n",
      "which arep+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\n",
      "nentofxiis1,theﬁrstscoreequationspeciﬁesthat∑N\n",
      "i=1yi=∑N\n",
      "i=1p(xi;β);\n",
      "theexpected number of class ones matches the observed number (and hence\n",
      "also class twos.)\n",
      "To solve the score equations (4.21), we use the Newton–Raphs on algo-\n",
      "rithm, which requires the second-derivative or Hessian mat rix\n",
      "∂2ℓ(β)\n",
      "∂β∂βT=−N∑\n",
      "i=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\n",
      "Starting with βold, a single Newton update is\n",
      "βnew=βold−(∂2ℓ(β)\n",
      "∂β∂βT)−1∂ℓ(β)\n",
      "∂β, (4.23)\n",
      "where the derivatives are evaluated at βold.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.4 Logistic Regression 121\n",
      "It is convenient to write the score and Hessian in matrix nota tion. Let\n",
      "ydenote the vector of yivalues,XtheN×(p+ 1) matrix of xivalues,\n",
      "pthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\n",
      "N×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\n",
      "p(xi;βold)). Then we have\n",
      "∂ℓ(β)\n",
      "∂β=XT(y−p) (4.24)\n",
      "∂2ℓ(β)\n",
      "∂β∂βT=−XTWX (4.25)\n",
      "The Newton step is thus\n",
      "βnew=βold+(XTWX)−1XT(y−p)\n",
      "= (XTWX)−1XTW(\n",
      "Xβold+W−1(y−p))\n",
      "= (XTWX)−1XTWz. (4.26)\n",
      "In the second and third line we have re-expressed the Newton s tep as a\n",
      "weighted least squares step, with the response\n",
      "z=Xβold+W−1(y−p), (4.27)\n",
      "sometimes known as the adjusted response . These equations get solved re-\n",
      "peatedly, since at each iteration pchanges, and hence so does Wandz.\n",
      "This algorithm is referred to as iteratively reweighted least squares or IRLS,\n",
      "since each iteration solves the weighted least squares prob lem:\n",
      "βnew←argmin\n",
      "β(z−Xβ)TW(z−Xβ). (4.28)\n",
      "It seems that β= 0 is a good starting value for the iterative procedure,\n",
      "although convergence is never guaranteed. Typically the al gorithm does\n",
      "converge, since the log-likelihood is concave, but oversho oting can occur.\n",
      "In the rare cases that the log-likelihood decreases, step si ze halving will\n",
      "guarantee convergence.\n",
      "For the multiclass case ( K≥3) the Newton algorithm can also be ex-\n",
      "pressed as an iteratively reweighted least squares algorit hm, but with a\n",
      "vectorofK−1 responses and a nondiagonal weight matrix per observation .\n",
      "The latter precludes any simpliﬁed algorithms, and in this c ase it is numer-\n",
      "ically more convenient to work with the expanded vector θdirectly (Ex-\n",
      "ercise 4.4). Alternatively coordinate-descent methods (S ection 3.8.6) can\n",
      "be used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\n",
      "(Friedman et al., 2010) can ﬁt very large logistic regressio n problems ef-\n",
      "ﬁciently, both in Nandp. Although designed to ﬁt regularized models,\n",
      "options allow for unregularized ﬁts.\n",
      "Logistic regression models are used mostly as a data analysi s and infer-\n",
      "ence tool, where the goal is to understand the role of the inpu t variables\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"122 4. Linear Methods for Classiﬁcation\n",
      "TABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\n",
      "disease data.\n",
      "Coeﬃcient Std. Error ZScore\n",
      "(Intercept) −4.130 0 .964−4.285\n",
      "sbp 0.006 0 .006 1.023\n",
      "tobacco 0.080 0 .026 3.034\n",
      "ldl 0.185 0 .057 3.219\n",
      "famhist 0.939 0 .225 4.178\n",
      "obesity -0.035 0 .029−1.187\n",
      "alcohol 0.001 0 .004 0.136\n",
      "age 0.043 0 .010 4.184\n",
      "inexplaining the outcome. Typically many models are ﬁt in a search for a\n",
      "parsimonious model involving a subset of the variables, pos sibly with some\n",
      "interactions terms. The following example illustrates som e of the issues\n",
      "involved.\n",
      "4.4.2 Example: South African Heart Disease\n",
      "Here we present an analysis of binary data to illustrate the t raditional\n",
      "statistical use of the logistic regression model. The data i n Figure 4.12 are a\n",
      "subset of the Coronary Risk-Factor Study (CORIS) baseline s urvey, carried\n",
      "out in three rural areas of the Western Cape, South Africa (Ro usseauw et\n",
      "al., 1983). The aim of the study was to establish the intensit y of ischemic\n",
      "heart disease risk factors in that high-incidence region. T he data represent\n",
      "white males between 15 and 64, and the response variable is th e presence or\n",
      "absence of myocardial infarction (MI) at the time of the surv ey (the overall\n",
      "prevalence of MI was 5.1% in this region). There are 160 cases in our data\n",
      "set, and a sample of 302 controls. These data are described in more detail\n",
      "in Hastie and Tibshirani (1987).\n",
      "We ﬁt a logistic-regression model by maximum likelihood, gi ving the\n",
      "results shown in Table 4.2. This summary includes Zscores for each of the\n",
      "coeﬃcients in the model (coeﬃcients divided by their standa rd errors); a\n",
      "nonsigniﬁcant Zscoresuggestsacoeﬃcientcanbedroppedfromthemodel.\n",
      "Each of these correspond formally to a test of the null hypoth esis that the\n",
      "coeﬃcient in question is zero, while all the others are not (a lso known as\n",
      "the Wald test). A Zscore greater than approximately 2 in absolute value\n",
      "is signiﬁcant at the 5% level.\n",
      "There are some surprises in this table of coeﬃcients, which m ust be in-\n",
      "terpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\n",
      "isobesity, and its sign is negative. This confusion is a result of the co rre-\n",
      "lation between the set of predictors. On their own, both sbpandobesity\n",
      "are signiﬁcant, and with positive sign. However, in the pres ence of many\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.4 Logistic Regression 123\n",
      "sbp0 10 20 30\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo oo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0.0 0.4 0.8\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo ooo\n",
      "ooo\n",
      "oo\n",
      "ooo oo\n",
      "oooo\n",
      "oooooo o ooo oo\n",
      "oooo o\n",
      "o ooo\n",
      "o oooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "o ooo\n",
      "oooooooo oo\n",
      "o oo\n",
      "oo\n",
      "oo ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo o\n",
      "oo oo\n",
      "oo ooooo\n",
      "oo o ooooooo\n",
      "ooo\n",
      "oo\n",
      "oo oo\n",
      "oo oo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "o ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "oooooooo\n",
      "oooo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "o oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo oo\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "ooo\n",
      "ooo oo oo\n",
      "o oooo\n",
      "oo\n",
      "ooo o\n",
      "o o oo\n",
      "o\n",
      "o o oo\n",
      "ooo\n",
      "ooo\n",
      "o ooo\n",
      "oooo\n",
      "oooo\n",
      "o oo ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooo o ooo\n",
      "ooo\n",
      "oo o ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0 50 100\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo ooo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "100 160 220o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo o\n",
      "oooooooooo ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "o ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo oo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo oooo\n",
      "oo ooooo ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oooo oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oo oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0 10 20 30o\n",
      "oooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooo oo\n",
      "ooooooooo\n",
      "o o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooooo\n",
      "oo\n",
      "o oooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooo oo\n",
      "ooooooooo\n",
      "o o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooooo\n",
      "oo\n",
      "o oooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooo oooooooo\n",
      "ooo\n",
      "oooooooooooooooo oo\n",
      "otobaccoo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo oooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo\n",
      "oooooooo oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooo oooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "ooooooo ooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "o oo\n",
      "oooooooooo\n",
      "oooooooo\n",
      "oo\n",
      "o ooo\n",
      "oo o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o o ooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o o o oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o ooo oo\n",
      "ooooo\n",
      "o oo\n",
      "oo ooooooo oooo\n",
      "o\n",
      "o o ooooo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo o\n",
      "o oooo\n",
      "ooooooo o oo\n",
      "o oooo oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooooo o oooooo\n",
      "o oooo\n",
      "oo\n",
      "oo\n",
      "o o oo oo o oo oooo ooo ooooo\n",
      "oooooo\n",
      "oooo\n",
      "o ooo oo oo\n",
      "oooo\n",
      "oooo\n",
      "o o\n",
      "ooo oooo\n",
      "o ooooo\n",
      "ooooo\n",
      "ooo\n",
      "oo oooo\n",
      "ooo\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooo o o oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo oo\n",
      "oo o oo\n",
      "ooooo\n",
      "ooo o\n",
      "ooooo\n",
      "oo oo o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oooo oo oo\n",
      "ooo\n",
      "oo\n",
      "o ooo oooo\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo oo\n",
      "ooooo o o o ooo\n",
      "o oo\n",
      "oo oo oooooo\n",
      "o ooo oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooooooo oooo\n",
      "ooo\n",
      "oooooooooo\n",
      "oooo oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "ooooooo ooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o oooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooooooo o\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oo oo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo o\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooooooooooo\n",
      "oooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooooooo\n",
      "oo\n",
      "o ooo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o ooooooo ooo\n",
      "ooo\n",
      "oooooooooooooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o o ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo oooo\n",
      "ooooo\n",
      "o oo\n",
      "ooo oooooo oooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oooo ooooo\n",
      "ooooo\n",
      "o ooooooo oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooo\n",
      "oooo oo\n",
      "oooo\n",
      "o ooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooooo\n",
      "ooooo ooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo oo\n",
      "o oooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oooooooooooooo\n",
      "oooooooooo\n",
      "ooo o oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo oo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "ooooo ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "ooooo ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "o oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo oooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oldl\n",
      "oo oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo o oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "oo ooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo oooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oooo\n",
      "ooo o\n",
      "o oo\n",
      "oo\n",
      "oooooo\n",
      "oo oo oo\n",
      "oo\n",
      "oooo oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo oo\n",
      "ooo o\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o ooooooo\n",
      "ooo ooo\n",
      "ooooo ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o oo ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "oo ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "ooooo\n",
      "o ooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo o oo\n",
      "oo\n",
      "o oo\n",
      "oo oo\n",
      "o ooo\n",
      "ooooo ooooooo\n",
      "ooo\n",
      "o oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "o o o\n",
      "ooo ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "ooo ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo oooooooo\n",
      "oo oo\n",
      "oooo\n",
      "o ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo ooooooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo oo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "ooo o\n",
      "oooooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "o oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo ooooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo oo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "2 6 10 14oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "o oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooo\n",
      "o ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo ooooo o\n",
      "oooooo\n",
      "o oooo oo\n",
      "ooo\n",
      "o ooo ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "o oooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooooooo\n",
      "oo o\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o0.0 0.4 0.8o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "ooooo o ooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo o oo\n",
      "oo\n",
      "ooo\n",
      "ooo o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "ooooo o\n",
      "oo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo o\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooo\n",
      "o oooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "o oooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo ooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo o\n",
      "oo o\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "o oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo ooooo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"o oooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo ooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo o\n",
      "oo o\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "o oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo ooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo oooo\n",
      "oooooooo\n",
      "ooooo o ooooooooo\n",
      "oo oooo\n",
      "ooooooooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "ooooo o\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o ooo\n",
      "oo\n",
      "o ooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo oo\n",
      "oo\n",
      "ooooo\n",
      "oo oo\n",
      "oo o ooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooooooo o\n",
      "ooooooooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o ooo oooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooo oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo ooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo oo\n",
      "famhisto\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "oo oooo\n",
      "oo o\n",
      "oooo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "oo ooooooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "ooo o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "oooooo ooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oo o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o oo\n",
      "ooooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooooooo o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo oooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "oo o oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "o ooooooo ooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "ooo o\n",
      "ooo o ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o ooooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo o\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o ooooooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo o\n",
      "oooo\n",
      "ooo\n",
      "ooooo o\n",
      "ooooo\n",
      "oo oo\n",
      "oo\n",
      "o ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooo\n",
      "o ooo\n",
      "o oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o o o ooooooo\n",
      "oo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o o ooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "o o ooo o ooo oooooo\n",
      "oo oooo\n",
      "oo o oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "o oo\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "o oo oooo o oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo ooooo\n",
      "oo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o o oooooo\n",
      "oo\n",
      "oo ooo\n",
      "oo oooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooo oo\n",
      "oooo ooo\n",
      "ooo\n",
      "oo o\n",
      "oooo\n",
      "oo\n",
      "o ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo o\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo oooooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooooooo oo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooo oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooo oooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo oo\n",
      "oo\n",
      "ooooo\n",
      "ooo oo oooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo ooo\n",
      "oooooo o ooo o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "o o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooo ooo\n",
      "oo\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo o ooo\n",
      "ooo\n",
      "oo oo o\n",
      "ooooooo oo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "o ooo\n",
      "ooo oo\n",
      "o oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo o ooooo\n",
      "ooooo\n",
      "oooo oo\n",
      "o\n",
      "o oo oo o\n",
      "oooooo\n",
      "o oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o o\n",
      "oooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "o oooo oo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo ooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "o oooooo oo\n",
      "o\n",
      "oo\n",
      "oooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oo ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooo oooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oobesity\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooooooo o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "15 25 35 45oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo oooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooo ooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooo oo\n",
      "oooo\n",
      "ooo ooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o0 50 100o\n",
      "oooo\n",
      "oooooo\n",
      "ooo ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo oooooooooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo oooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooo ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo oooo\n",
      "oo\n",
      "ooo\n",
      "o ooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooooo\n",
      "oooooooooooooooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "oooooooooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo ooooo oooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oo\n",
      "oo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo ooooo oooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooo oooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooooo\n",
      "oooooooooooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "o ooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooooooo ooo oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo o\n",
      "oo\n",
      "o ooo\n",
      "oo oooo\n",
      "o oo oo o\n",
      "oo\n",
      "ooo\n",
      "o o o o oo\n",
      "o oooooooo\n",
      "ooo\n",
      "o ooooo ooo\n",
      "o oo\n",
      "oooo\n",
      "ooo o o ooooo\n",
      "ooooooo\n",
      "o ooo ooo\n",
      "o o oooo\n",
      "o oo oooooooo\n",
      "oooo o oooo\n",
      "oo ooo o o oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo ooo ooooooooooo\n",
      "o oooo o ooooooo ooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooooooooooo ooo oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "o oo oo ooo\n",
      "oo oooo\n",
      "ooo\n",
      "oooo\n",
      "o ooooo\n",
      "o o ooo ooooooo\n",
      "ooo\n",
      "oo ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o oo o oooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo o ooooo\n",
      "ooo\n",
      "oo\n",
      "oo ooo\n",
      "oo\n",
      "ooo o oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo ooo\n",
      "oooo\n",
      "oo oo\n",
      "o\n",
      "ooooo o ooo\n",
      "o oooooooo\n",
      "oooo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o ooooo o oooooo oo\n",
      "oo\n",
      "o oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "oo oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo oooooooooooooo\n",
      "oooooo oooooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo ooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooooooooooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oalcoholo\n",
      "oooo\n",
      "oooooo\n",
      "ooo ooo\n",
      "oo\n",
      "ooo\n",
      "oo oooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo o ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oooo o oooooo\n",
      "ooooooooo\n",
      "ooooo oooo\n",
      "oo\n",
      "oo\n",
      "oo oooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooo o ooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo o oooooo ooooo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "o oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oooo o\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o ooooo ooooooo oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o ooo o\n",
      "o\n",
      "100 160 220oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "2 6 10 14oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "oooo o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o oo o\n",
      "ooo\n",
      "oooo o o\n",
      "oooo o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "oo o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooo oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo o\n",
      "oooo o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo o\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o\n",
      "ooooo ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "15 25 35 45oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oooo\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "20 40 60\n",
      "20 40 60age\n",
      "FIGURE 4.12. A scatterplot matrix of the South African heart disease data.\n",
      "Each plot shows a pair of risk factors, and the cases and contro ls are color coded\n",
      "(red is a case). The variable family history of heart disease ( famhist)is binary\n",
      "(yes or no).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"124 4. Linear Methods for Classiﬁcation\n",
      "TABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\n",
      "disease data.\n",
      "Coeﬃcient Std. Error Zscore\n",
      "(Intercept) −4.204 0 .498−8.45\n",
      "tobacco 0.081 0 .026 3.16\n",
      "ldl 0.168 0 .054 3.09\n",
      "famhist 0.924 0 .223 4.14\n",
      "age 0.044 0 .010 4.52\n",
      "other correlated variables, they are no longer needed (and c an even get a\n",
      "negative sign).\n",
      "At this stage the analyst might do some model selection; ﬁnd a subset\n",
      "of the variables that are suﬃcient for explaining their join t eﬀect on the\n",
      "prevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\n",
      "eﬃcient, and reﬁt the model. This is done repeatedly until no further terms\n",
      "can be dropped from the model. This gave the model shown in Tab le 4.3.\n",
      "A better but more time-consuming strategy is to reﬁt each of t he models\n",
      "with one variable removed, and then perform an analysis of deviance to\n",
      "decide which variable to exclude. The residual deviance of a ﬁtted model\n",
      "is minus twice its log-likelihood, and the deviance between two models is\n",
      "the diﬀerence of their individual residual deviances (in an alogy to sums-of-\n",
      "squares). This strategy gave the same ﬁnal model as above.\n",
      "How does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\n",
      "tobacco, for example? Tobacco is measured in total lifetime usage in kilo-\n",
      "grams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\n",
      "an increase of 1kg in lifetime tobacco usage accounts for an i ncrease in the\n",
      "odds of coronary heart disease of exp(0 .081) = 1.084 or 8.4%. Incorporat-\n",
      "ing the standard error we get an approximate 95% conﬁdence in terval of\n",
      "exp(0.081±2×0.026) = (1.03,1.14).\n",
      "We return to these data in Chapter 5, where we see that some of t he\n",
      "variables have nonlinear eﬀects, and when modeled appropri ately, are not\n",
      "excluded from the model.\n",
      "4.4.3 Quadratic Approximations and Inference\n",
      "The maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\n",
      "relationship: they are the coeﬃcients of a weighted least sq uares ﬁt, where\n",
      "the responses are\n",
      "zi=xT\n",
      "iˆβ+(yi−ˆpi)\n",
      "ˆpi(1−ˆpi), (4.29)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.4 Logistic Regression 125\n",
      "and the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\n",
      "providing a convenient algorithm, this connection with lea st squares has\n",
      "more to oﬀer:\n",
      "•The weighted residual sum-of-squares is the familiar Pears on chi-\n",
      "square statistic\n",
      "N∑\n",
      "i=1(yi−ˆpi)2\n",
      "ˆpi(1−ˆpi), (4.30)\n",
      "a quadratic approximation to the deviance.\n",
      "•Asymptotic likelihood theory says that if the model is corre ct, then\n",
      "ˆβis consistent (i.e., converges to the trueβ).\n",
      "•A central limit theorem then shows that the distribution of ˆβcon-\n",
      "verges toN(β,(XTWX)−1). This and other asymptotics can be de-\n",
      "riveddirectlyfromtheweightedleastsquaresﬁtbymimicki ngnormal\n",
      "theory inference.\n",
      "•Model building can be costly for logistic regression models , because\n",
      "each model ﬁtted requires iteration. Popular shortcuts are theRao\n",
      "score test which tests for inclusion of a term, and the Wald test which\n",
      "can be used to test for exclusion of a term. Neither of these re quire\n",
      "iterative ﬁtting, and are based on the maximum-likelihood ﬁ t of the\n",
      "current model. It turns out that both of these amount to addin g\n",
      "or dropping a term from the weighted least squares ﬁt, using t he\n",
      "sameweights. Such computations can be done eﬃciently, without\n",
      "recomputing the entire weighted least squares ﬁt.\n",
      "Software implementations can take advantage of these conne ctions. For\n",
      "example, the generalized linear modeling software in R (whi ch includes lo-\n",
      "gistic regression as part of the binomial family of models) e xploits them\n",
      "fully.GLM(generalizedlinearmodel)objectscanbetreate daslinearmodel\n",
      "objects, and all the tools available for linear models can be applied auto-\n",
      "matically.\n",
      "4.4.4L1Regularized Logistic Regression\n",
      "TheL1penalty used in the lasso (Section 3.4.2) can be used for vari able\n",
      "selection and shrinkage with any linear regression model. F or logistic re-\n",
      "gression, we would maximize a penalized version of (4.20):\n",
      "max\n",
      "β0,β\n",
      "\n",
      "N∑\n",
      "i=1[\n",
      "yi(β0+βTxi)−log(1+eβ0+βTxi)]\n",
      "−λp∑\n",
      "j=1|βj|\n",
      "\n",
      ".(4.31)\n",
      "As with the lasso, we typically do not penalize the intercept term, and stan-\n",
      "dardize the predictors for the penalty to be meaningful. Cri terion (4.31) is\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"126 4. Linear Methods for Classiﬁcation\n",
      "concave, and a solution can be found using nonlinear program ming meth-\n",
      "ods (Koh et al., 2007, for example). Alternatively, using th e same quadratic\n",
      "approximations that were used in the Newton algorithm in Sec tion 4.4.1,\n",
      "we can solve (4.31) by repeated application of a weighted las so algorithm.\n",
      "Interestingly, the score equations [see (4.24)] for the var iables with non-zero\n",
      "coeﬃcients have the form\n",
      "xT\n",
      "j(y−p) =λ·sign(βj), (4.32)\n",
      "which generalizes (3.58) in Section 3.4.4; the active varia bles are tied in\n",
      "theirgeneralized correlation with the residuals.\n",
      "Path algorithms such as LAR for lasso are more diﬃcult, becau se the\n",
      "coeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\n",
      "progress can be made using quadratic approximations.\n",
      "******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\n",
      "0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\n",
      "******************************* ************** * ****************************************************************************************************************************************************************************** *****************\n",
      "****************************** ********************************************************************************************************************************************************************************************** *****************\n",
      "******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\n",
      "obesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\n",
      "||β(λ)||1\n",
      "FIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\n",
      "African heart disease data, plotted as a function of the L1norm. The variables\n",
      "were all standardized to have unit variance. The proﬁles are c omputed exactly at\n",
      "each of the plotted points.\n",
      "Figure 4.13 shows the L1regularization path for the South African\n",
      "heart disease data of Section 4.4.2. This was produced using theRpackage\n",
      "glmpath (Park and Hastie, 2007), which uses predictor–corrector methods\n",
      "of convex optimization to identify the exact values of λat which the active\n",
      "set of non-zero coeﬃcients changes (vertical lines in the ﬁg ure). Here the\n",
      "proﬁles look almost linear; in other examples the curvature will be more\n",
      "visible.\n",
      "Coordinate descent methods (Section 3.8.6) are very eﬃcien t for comput-\n",
      "ing the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.4 Logistic Regression 127\n",
      "(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\n",
      "gression problems eﬃciently (large in Norp). Their algorithms can exploit\n",
      "sparsity in the predictor matrix X, which allows for even larger problems.\n",
      "See Section 18.4 for more details, and a discussion of L1-regularized multi-\n",
      "nomial models.\n",
      "4.4.5 Logistic Regression or LDA?\n",
      "In Section 4.3 we ﬁnd that the log-posterior odds between cla sskandK\n",
      "are linear functions of x(4.9):\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=K|X=x)= logπk\n",
      "πK−1\n",
      "2(µk+µK)TΣ−1(µk−µK)\n",
      "+xTΣ−1(µk−µK)\n",
      "=αk0+αT\n",
      "kx. (4.33)\n",
      "This linearity is a consequence of the Gaussian assumption f or the class\n",
      "densities, as well as the assumption of a common covariance m atrix. The\n",
      "linear logistic model (4.17) by construction has linear log its:\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=K|X=x)=βk0+βT\n",
      "kx. (4.34)\n",
      "Itseemsthatthemodelsarethesame.Althoughtheyhaveexac tlythesame\n",
      "form, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\n",
      "logistic regression model is more general, in that it makes l ess assumptions.\n",
      "We can write the joint density ofXandGas\n",
      "Pr(X,G=k) = Pr(X)Pr(G=k|X), (4.35)\n",
      "where Pr(X) denotes the marginal density of the inputs X. For both LDA\n",
      "and logistic regression, the second term on the right has the logit-linear\n",
      "form\n",
      "Pr(G=k|X=x) =eβk0+βT\n",
      "kx\n",
      "1+∑K−1\n",
      "ℓ=1eβℓ0+βT\n",
      "ℓx, (4.36)\n",
      "where we have again arbitrarily chosen the last class as the r eference.\n",
      "The logistic regression model leaves the marginal density o fXas an arbi-\n",
      "trary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\n",
      "imizing the conditional likelihood —the multinomial likelihood with proba-\n",
      "bilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\n",
      "of this marginal density as being estimated in a fully nonpar ametric and\n",
      "unrestricted fashion, using the empirical distribution fu nction which places\n",
      "mass 1/Nat each observation.\n",
      "With LDA we ﬁt the parameters by maximizing the full log-like lihood,\n",
      "based on the joint density\n",
      "Pr(X,G=k) =φ(X;µk,Σ)πk, (4.37)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"128 4. Linear Methods for Classiﬁcation\n",
      "whereφis the Gaussian density function. Standard normal theory le ads\n",
      "easily to the estimates ˆ µk,ˆΣ, and ˆπkgiven in Section 4.3. Since the linear\n",
      "parameters of the logistic form (4.33) are functions of the G aussian param-\n",
      "eters, we get their maximum-likelihood estimates by pluggi ng in the corre-\n",
      "sponding estimates. However, unlike in the conditional cas e, the marginal\n",
      "density Pr( X) does play a role here. It is a mixture density\n",
      "Pr(X) =K∑\n",
      "k=1πkφ(X;µk,Σ), (4.38)\n",
      "which also involves the parameters.\n",
      "What role can this additional component/restriction play? By relying\n",
      "on the additional model assumptions, we have more informati on about the\n",
      "parameters, and hence can estimate them more eﬃciently (low er variance).\n",
      "If in fact the true fk(x) are Gaussian, then in the worst case ignoring this\n",
      "marginal part of the likelihood constitutes a loss of eﬃcien cy of about 30%\n",
      "asymptotically in the error rate (Efron, 1975). Paraphrasi ng: with 30%\n",
      "more data, the conditional likelihood will do as well.\n",
      "For example, observations far from the decision boundary (w hich are\n",
      "down-weighted by logistic regression) play a role in estima ting the common\n",
      "covariance matrix. This is not all good news, because it also means that\n",
      "LDA is not robust to gross outliers.\n",
      "From the mixture formulation, it is clear that even observat ions without\n",
      "class labels have information about the parameters. Often i t is expensive\n",
      "to generate class labels, but unclassiﬁed observations com e cheaply. By\n",
      "relying on strong model assumptions, such as here, we can use both types\n",
      "of information.\n",
      "The marginal likelihood can be thought of as a regularizer, r equiring\n",
      "in some sense that class densities be visiblefrom this marginal view. For\n",
      "example, if the data in a two-class logistic regression mode l can be per-\n",
      "fectly separated by a hyperplane, the maximum likelihood es timates of the\n",
      "parameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\n",
      "cients for the same data will be well deﬁned, since the margin al likelihood\n",
      "will not permit these degeneracies.\n",
      "In practice these assumptions are never correct, and often s ome of the\n",
      "components of Xare qualitative variables. It is generally felt that logist ic\n",
      "regression is a safer, more robust bet than the LDA model, rel ying on fewer\n",
      "assumptions. It is our experience that the models give very s imilar results,\n",
      "even when LDA is used inappropriately, such as with qualitat ive predictors.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.5 Separating Hyperplanes 129\n",
      "FIGURE 4.14. A toy example with two classes separable by a hyperplane. The\n",
      "orange line is the least squares solution, which misclassiﬁes on e of the training\n",
      "points. Also shown are two blue separating hyperplanes found by theperceptron\n",
      "learning algorithm with diﬀerent random starts.\n",
      "4.5 Separating Hyperplanes\n",
      "We have seen that linear discriminant analysis and logistic regression both\n",
      "estimate linear decision boundaries in similar but slightl y diﬀerent ways.\n",
      "For the rest of this chapter we describe separating hyperpla ne classiﬁers.\n",
      "These procedures construct linear decision boundaries tha t explicitly try\n",
      "to separate the data into diﬀerent classes as well as possibl e. They provide\n",
      "the basis for support vector classiﬁers, discussed in Chapt er 12. The math-\n",
      "ematical level of this section is somewhat higher than that o f the previous\n",
      "sections.\n",
      "Figure 4.14 shows 20 data points in two classes in IR2. These data can be\n",
      "separated by a linear boundary. Included in the ﬁgure (blue l ines) are two\n",
      "of the inﬁnitely many possible separating hyperplanes . The orange line is\n",
      "the least squares solution to the problem, obtained by regre ssing the−1/1\n",
      "responseYonX(with intercept); the line is given by\n",
      "{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\n",
      "This least squares solution does not do a perfect job in separ ating the\n",
      "points, and makes one error. This is the same boundary found b y LDA,\n",
      "in light of its equivalence with linear regression in the two -class case (Sec-\n",
      "tion 4.3 and Exercise 4.2).\n",
      "Classiﬁers such as (4.39), that compute a linear combinatio n of the input\n",
      "featuresandreturnthesign,werecalled perceptrons intheengineeringliter-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"130 4. Linear Methods for Classiﬁcation\n",
      "x0x\n",
      "β∗β0+βTx= 0\n",
      "FIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\n",
      "ature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\n",
      "for the neural network models of the 1980s and 1990s.\n",
      "Beforewecontinue,letusdigressslightlyandreviewsomev ectoralgebra.\n",
      "Figure 4.15 depicts a hyperplane or aﬃne setLdeﬁned by the equation\n",
      "f(x) =β0+βTx= 0; since we are in IR2this is a line.\n",
      "Here we list some properties:\n",
      "1. For any two points x1andx2lying inL,βT(x1−x2) = 0, and hence\n",
      "β∗=β/||β||is the vector normal to the surface of L.\n",
      "2. For any point x0inL,βTx0=−β0.\n",
      "3. The signed distance of any point xtoLis given by\n",
      "β∗T(x−x0) =1\n",
      "∥β∥(βTx+β0)\n",
      "=1\n",
      "||f′(x)||f(x). (4.40)\n",
      "Hencef(x) is proportional to the signed distance from xto the hyperplane\n",
      "deﬁned byf(x) = 0.\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm\n",
      "Theperceptron learning algorithm tries to ﬁnd a separating hyperplane by\n",
      "minimizing the distance of misclassiﬁed points to the decis ion boundary. If\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.5 Separating Hyperplanes 131\n",
      "a responseyi= 1 is misclassiﬁed, then xT\n",
      "iβ+β0<0, and the opposite for\n",
      "a misclassiﬁed response with yi=−1. The goal is to minimize\n",
      "D(β,β0) =−∑\n",
      "i∈Myi(xT\n",
      "iβ+β0), (4.41)\n",
      "whereMindexes the set of misclassiﬁed points. The quantity is non-\n",
      "negative and proportional to the distance of the misclassiﬁ ed points to\n",
      "the decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\n",
      "Mis ﬁxed) is given by\n",
      "∂D(β,β0)\n",
      "∂β=−∑\n",
      "i∈Myixi, (4.42)\n",
      "∂D(β,β0)\n",
      "∂β0=−∑\n",
      "i∈Myi. (4.43)\n",
      "The algorithm in fact uses stochastic gradient descent to minimize this\n",
      "piecewise linear criterion. This means that rather than com puting the sum\n",
      "of the gradient contributions of each observation followed by a step in the\n",
      "negative gradient direction, a step is taken after each obse rvation is visited.\n",
      "Hence the misclassiﬁed observations are visited in some seq uence, and the\n",
      "parameters βare updated via\n",
      "(\n",
      "β\n",
      "β0)\n",
      "←(\n",
      "β\n",
      "β0)\n",
      "+ρ(\n",
      "yixi\n",
      "yi)\n",
      ". (4.44)\n",
      "Hereρis the learning rate, which in this case can be taken to be 1 wit hout\n",
      "loss in generality. If the classes are linearly separable, i t can be shown that\n",
      "the algorithm converges to a separating hyperplane in a ﬁnit e number of\n",
      "steps (Exercise 4.6). Figure 4.14 shows two solutions to a to y problem, each\n",
      "started at a diﬀerent random guess.\n",
      "There are a number of problems with this algorithm, summariz ed in\n",
      "Ripley (1996):\n",
      "•When the data are separable, there are many solutions, and wh ich\n",
      "one is found depends on the starting values.\n",
      "•The “ﬁnite” number of steps can be very large. The smaller the gap,\n",
      "the longer the time to ﬁnd it.\n",
      "•When the data are not separable, the algorithm will not conve rge,\n",
      "and cycles develop. The cycles can be long and therefore hard to\n",
      "detect.\n",
      "The second problem can often be eliminated by seeking a hyper plane not\n",
      "in the original space, but in a much enlarged space obtained b y creating\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"132 4. Linear Methods for Classiﬁcation\n",
      "many basis-function transformations of the original varia bles. This is anal-\n",
      "ogous to driving the residuals in a polynomial regression pr oblem down\n",
      "to zero by making the degree suﬃciently large. Perfect separ ation cannot\n",
      "always be achieved: for example, if observations from two di ﬀerent classes\n",
      "share the same input. It may not be desirable either, since th e resulting\n",
      "model is likely to be overﬁt and will not generalize well. We r eturn to this\n",
      "point at the end of the next section.\n",
      "A rather elegant solution to the ﬁrst problem is to add additi onal con-\n",
      "straints to the separating hyperplane.\n",
      "4.5.2 Optimal Separating Hyperplanes\n",
      "Theoptimal separating hyperplane separates the two classes and maximizes\n",
      "the distance to the closest point from either class (Vapnik, 1996). Not only\n",
      "does this provide a unique solution to the separating hyperp lane problem,\n",
      "butbymaximizingthemarginbetweenthetwoclassesonthetr ainingdata,\n",
      "this leads to better classiﬁcation performance on test data .\n",
      "Weneedtogeneralizecriterion(4.41).Considertheoptimi zationproblem\n",
      "max\n",
      "β,β0,||β||=1M\n",
      "subject toyi(xT\n",
      "iβ+β0)≥M, i= 1,...,N.(4.45)\n",
      "The set of conditions ensure that all the points are at least a signed\n",
      "distanceMfrom the decision boundary deﬁned by βandβ0, and we seek\n",
      "the largest such Mand associated parameters. We can get rid of the ||β||=\n",
      "1 constraint by replacing the conditions with\n",
      "1\n",
      "||β||yi(xT\n",
      "iβ+β0)≥M, (4.46)\n",
      "(which redeﬁnes β0) or equivalently\n",
      "yi(xT\n",
      "iβ+β0)≥M||β||. (4.47)\n",
      "Since for any βandβ0satisfying these inequalities, any positively scaled\n",
      "multiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\n",
      "is equivalent to\n",
      "min\n",
      "β,β01\n",
      "2||β||2\n",
      "subject toyi(xT\n",
      "iβ+β0)≥1, i= 1,...,N.(4.48)\n",
      "In light of (4.40), the constraints deﬁne an empty slab or mar gin around the\n",
      "linear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to\n",
      "maximize its thickness. This is a convex optimization probl em (quadratic\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"4.5 Separating Hyperplanes 133\n",
      "criterion with linear inequality constraints). The Lagran ge (primal) func-\n",
      "tion, to be minimized w.r.t. βandβ0, is\n",
      "LP=1\n",
      "2||β||2−N∑\n",
      "i=1αi[yi(xT\n",
      "iβ+β0)−1]. (4.49)\n",
      "Setting the derivatives to zero, we obtain:\n",
      "β=N∑\n",
      "i=1αiyixi, (4.50)\n",
      "0 =N∑\n",
      "i=1αiyi, (4.51)\n",
      "and substituting these in (4.49) we obtain the so-called Wol fe dual\n",
      "LD=N∑\n",
      "i=1αi−1\n",
      "2N∑\n",
      "i=1N∑\n",
      "k=1αiαkyiykxT\n",
      "ixk\n",
      "subject toαi≥0 andN∑\n",
      "i=1αiyi= 0. (4.52)\n",
      "The solution is obtained by maximizing LDin the positive orthant, a sim-\n",
      "pler convex optimization problem, for which standard softw are can be used.\n",
      "In addition the solution must satisfy the Karush–Kuhn–Tuck er conditions,\n",
      "which include (4.50), (4.51), (4.52) and\n",
      "αi[yi(xT\n",
      "iβ+β0)−1] = 0∀i. (4.53)\n",
      "From these we can see that\n",
      "•ifαi>0, thenyi(xT\n",
      "iβ+β0) = 1, or in other words, xiis on the\n",
      "boundary of the slab;\n",
      "•ifyi(xT\n",
      "iβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.\n",
      "From (4.50) we see that the solution vector βis deﬁned in terms of a linear\n",
      "combination of the support points xi—those points deﬁned to be on the\n",
      "boundary of the slab via αi>0. Figure 4.16 shows the optimal separating\n",
      "hyperplane for our toy example; there are three support poin ts. Likewise,\n",
      "β0is obtained by solving (4.53) for any of the support points.\n",
      "The optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\n",
      "for classifying new observations:\n",
      "ˆG(x) = signˆf(x). (4.54)\n",
      "Although none of the training observations fall in the margi n (by con-\n",
      "struction), this will not necessarily be the case for test ob servations. The\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"134 4. Linear Methods for Classiﬁcation\n",
      "FIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\n",
      "the maximum margin separating the two classes. There are thre e support points\n",
      "indicated, which lie on the boundary of the margin, and the opt imal separating\n",
      "hyperplane (blue line) bisects the slab. Included in the ﬁgure is t he boundary found\n",
      "using logistic regression (red line), which is very close to the optimal separating\n",
      "hyperplane (see Section 12.3.3).\n",
      "intuition is that a large margin on the training data will lea d to good\n",
      "separation on the test data.\n",
      "The description of the solution in terms of support points se ems to sug-\n",
      "gest that the optimal hyperplane focuses more on the points t hat count,\n",
      "and is more robust to model misspeciﬁcation. The LDA solutio n, on the\n",
      "other hand, depends on all of the data, even points far away fr om the de-\n",
      "cision boundary. Note, however, that the identiﬁcation of t hese support\n",
      "points required the use of all the data. Of course, if the clas ses are really\n",
      "Gaussian, then LDA is optimal, and separating hyperplanes w ill pay a price\n",
      "for focusing on the (noisier) data at the boundaries of the cl asses.\n",
      "Included in Figure 4.16 is the logistic regression solution to this prob-\n",
      "lem, ﬁt by maximum likelihood. Both solutions are similar in this case.\n",
      "When a separating hyperplane exists, logistic regression w ill always ﬁnd\n",
      "it, since the log-likelihood can be driven to 0 in this case (E xercise 4.5).\n",
      "The logistic regression solution shares some other qualita tive features with\n",
      "the separating hyperplane solution. The coeﬃcient vector i s deﬁned by a\n",
      "weighted least squares ﬁt of a zero-mean linearized respons e on the input\n",
      "features, and the weights are larger for points near the deci sion boundary\n",
      "than for those further away.\n",
      "When the data are not separable, there will be no feasible sol ution to\n",
      "this problem, and an alternative formulation is needed. Aga in one can en-\n",
      "large the space using basis transformations, but this can le ad to artiﬁcial\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 135\n",
      "separation through over-ﬁtting. In Chapter 12 we discuss a m ore attractive\n",
      "alternative known as the support vector machine , which allows for overlap,\n",
      "but minimizes a measure of the extent of this overlap.\n",
      "Bibliographic Notes\n",
      "Good general texts on classiﬁcation include Duda et al. (200 0), Hand\n",
      "(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1 979) have\n",
      "a concise discussion of linear discriminant analysis. Mich ie et al. (1994)\n",
      "compare a large number of popular classiﬁers on benchmark da tasets. Lin-\n",
      "ear separating hyperplanes are discussed in Vapnik (1996). Our account of\n",
      "the perceptron learning algorithm follows Ripley (1996).\n",
      "Exercises\n",
      "Ex. 4.1Show how to solve the generalized eigenvalue problem max aTBa\n",
      "subject toaTWa= 1 by transforming to a standard eigenvalue problem.\n",
      "Ex. 4.2Suppose we have features x∈IRp, a two-class response, with class\n",
      "sizesN1,N2, and the target coded as −N/N1,N/N2.\n",
      "(a) Show that the LDA rule classiﬁes to class 2 if\n",
      "xTˆΣ−1(ˆµ2−ˆµ1)>1\n",
      "2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),\n",
      "and class 1 otherwise.\n",
      "(b) Consider minimization of the least squares criterion\n",
      "N∑\n",
      "i=1(yi−β0−xT\n",
      "iβ)2. (4.55)\n",
      "Show that the solution ˆβsatisﬁes\n",
      "[\n",
      "(N−2)ˆΣ+NˆΣB]\n",
      "β=N(ˆµ2−ˆµ1) (4.56)\n",
      "(after simpliﬁcation), where ˆΣB=N1N2\n",
      "N2(ˆµ2−ˆµ1)(ˆµ2−ˆµ1)T.\n",
      "(c) Hence show that ˆΣBβis in the direction (ˆ µ2−ˆµ1) and thus\n",
      "ˆβ∝ˆΣ−1(ˆµ2−ˆµ1). (4.57)\n",
      "Therefore the least-squares regression coeﬃcient is ident ical to the\n",
      "LDA coeﬃcient, up to a scalar multiple.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"136 4. Linear Methods for Classiﬁcation\n",
      "(d) Show that this result holds for any (distinct) coding of t he two classes.\n",
      "(e) Find the solution ˆβ0(up to the same scalar multiple as in (c), and\n",
      "hence the predicted value ˆf(x) =ˆβ0+xTˆβ. Consider the following\n",
      "rule: classify to class 2 if ˆf(x)>0 and class 1 otherwise. Show this is\n",
      "not the same as the LDA rule unless the classes have equal numb ers\n",
      "of observations.\n",
      "(Fisher, 1936; Ripley, 1996)\n",
      "Ex. 4.3Suppose we transform the original predictors XtoˆYvia linear\n",
      "regression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\n",
      "indicator response matrix. Similarly for any input x∈IRp, we get a trans-\n",
      "formed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\n",
      "LDA in the original space.\n",
      "Ex. 4.4Consider the multilogit model with Kclasses (4.17). Let βbe the\n",
      "(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitabl y\n",
      "enlarged version of the input vector xto accommodate this vectorized co-\n",
      "eﬃcient matrix. Derive the Newton-Raphson algorithm for ma ximizing the\n",
      "multinomial log-likelihood, and describe how you would imp lement this\n",
      "algorithm.\n",
      "Ex. 4.5Consider a two-class logistic regression problem with x∈IR. Char-\n",
      "acterize the maximum-likelihood estimates of the slope and intercept pa-\n",
      "rameterifthesample xiforthetwoclassesareseparatedbyapoint x0∈IR.\n",
      "Generalize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\n",
      "two classes.\n",
      "Ex. 4.6Suppose we have Npointsxiin IRpin general position, with class\n",
      "labelsyi∈{−1,1}. Prove that the perceptron learning algorithm converges\n",
      "to a separating hyperplane in a ﬁnite number of steps:\n",
      "(a) Denote a hyperplane by f(x) =βT\n",
      "1x+β0= 0, or in more compact\n",
      "notationβTx∗= 0, where x∗= (x,1) andβ= (β1,β0). Letzi=\n",
      "x∗\n",
      "i/||x∗\n",
      "i||. Show that separability implies the existence of a βsepsuch\n",
      "thatyiβT\n",
      "sepzi≥1∀i\n",
      "(b)Givenacurrent βold,theperceptronalgorithmidentiﬁesapoint zithat\n",
      "is misclassiﬁed, and produces the update βnew←βold+yizi. Show\n",
      "that||βnew−βsep||2≤||βold−βsep||2−1, and hence that the algorithm\n",
      "converges to a separating hyperplane in no more than ||βstart−βsep||2\n",
      "steps (Ripley, 1996).\n",
      "Ex. 4.7Consider the criterion\n",
      "D∗(β,β0) =−N∑\n",
      "i=1yi(xT\n",
      "iβ+β0), (4.58)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 137\n",
      "a generalization of (4.41) where we sum over all the observat ions. Consider\n",
      "minimizing D∗subject to||β||= 1. Describe this criterion in words. Does\n",
      "it solve the optimal separating hyperplane problem?\n",
      "Ex. 4.8Consider the multivariate Gaussian model X|G=k∼N(µk,Σ),\n",
      "with the additional restriction that rank {µk}K\n",
      "1=L <max(K−1,p).\n",
      "Derive the constrained MLEs for the µkandΣ. Show that the Bayes clas-\n",
      "siﬁcation rule is equivalent to classifying in the reduced s ubspace computed\n",
      "by LDA (Hastie and Tibshirani, 1996b).\n",
      "Ex. 4.9Write a computer program to perform a quadratic discriminan t\n",
      "analysis by ﬁtting a separate Gaussian model per class. Try i t out on the\n",
      "vowel data, and compute the misclassiﬁcation error for the t est data. The\n",
      "datacanbefoundinthebookwebsite www-stat.stanford.edu/ElemStatLearn .\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"138 4. Linear Methods for Classiﬁcation\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 139\n",
      "Printer: Opaque this\n",
      "5\n",
      "Basis Expansions and Regularization\n",
      "5.1 Introduction\n",
      "We have already made use of models linear in the input feature s, both for\n",
      "regression and classiﬁcation. Linear regression, linear d iscriminant analysis,\n",
      "logistic regression and separating hyperplanes all rely on a linear model.\n",
      "It is extremely unlikely that the true function f(X) is actually linear in\n",
      "X. In regression problems, f(X) = E(Y|X) will typically be nonlinear and\n",
      "nonadditive in X, and representing f(X) by a linear model is usually a con-\n",
      "venient, and sometimes a necessary, approximation. Conven ient because a\n",
      "linear model is easy to interpret, and is the ﬁrst-order Tayl or approxima-\n",
      "tion tof(X). Sometimes necessary, because with Nsmall and/or plarge,\n",
      "a linear model might be all we are able to ﬁt to the data without overﬁt-\n",
      "ting. Likewise in classiﬁcation, a linear, Bayes-optimal d ecision boundary\n",
      "implies that some monotone transformation of Pr( Y= 1|X) is linear in X.\n",
      "This is inevitably an approximation.\n",
      "In this chapter and the next we discuss popular methods for mo ving\n",
      "beyond linearity. The core idea in this chapter is to augment /replace the\n",
      "vector of inputs Xwith additional variables, which are transformations of\n",
      "X, and then use linear models in this new space of derived input features.\n",
      "Denote by hm(X) : IRp↦→IR themth transformation of X,m=\n",
      "1,...,M. We then model\n",
      "f(X) =M∑\n",
      "m=1βmhm(X), (5.1)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"140 5. Basis Expansions and Regularization\n",
      "alinear basis expansion inX. The beauty of this approach is that once the\n",
      "basis functions hmhave been determined, the models are linear in these\n",
      "new variables, and the ﬁtting proceeds as before.\n",
      "Some simple and widely used examples of the hmare the following:\n",
      "•hm(X) =Xm, m= 1,...,precovers the original linear model.\n",
      "•hm(X) =X2\n",
      "jorhm(X) =XjXkallowsustoaugmenttheinputswith\n",
      "polynomial terms to achieve higher-order Taylor expansion s. Note,\n",
      "however, that the number of variables grows exponentially i n the de-\n",
      "gree of the polynomial. A full quadratic model in pvariables requires\n",
      "O(p2) square and cross-product terms, or more generally O(pd) for a\n",
      "degree-dpolynomial.\n",
      "•hm(X) = log(Xj),√\n",
      "Xj,...permits other nonlinear transformations\n",
      "of single inputs. More generally one can use similar functio ns involv-\n",
      "ing several inputs, such as hm(X) =||X||.\n",
      "•hm(X) =I(Lm≤Xk<Um), an indicator for a region of Xk. By\n",
      "breaking the range of Xkup intoMksuch nonoverlapping regions\n",
      "results in a model with a piecewise constant contribution fo rXk.\n",
      "Sometimestheproblemathandwillcallforparticularbasis functionshm,\n",
      "suchaslogarithmsorpowerfunctions.Moreoften,however, weusethebasis\n",
      "expansions as a device to achieve more ﬂexible representati ons forf(X).\n",
      "Polynomials are an example of the latter, although they are l imited by\n",
      "their global nature—tweaking the coeﬃcients to achieve a fun ctional form\n",
      "in one region can cause the function to ﬂap about madly in remo te regions.\n",
      "In this chapter we consider more useful families of piecewise-polynomials\n",
      "andsplinesthat allow for local polynomial representations. We also di scuss\n",
      "thewaveletbases, especially useful for modeling signals and images. T hese\n",
      "methods produce a dictionaryDconsisting of typically a very large number\n",
      "|D|of basis functions, far more than we can aﬀord to ﬁt to our data . Along\n",
      "with the dictionary we require a method for controlling the c omplexity\n",
      "of our model, using basis functions from the dictionary. The re are three\n",
      "common approaches:\n",
      "•Restriction methods, where we decide before-hand to limit t he class\n",
      "of functions. Additivity is an example, where we assume that our\n",
      "model has the form\n",
      "f(X) =p∑\n",
      "j=1fj(Xj)\n",
      "=p∑\n",
      "j=1Mj∑\n",
      "m=1βjmhjm(Xj). (5.2)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.2 Piecewise Polynomials and Splines 141\n",
      "The size of the model is limited by the number of basis functio nsMj\n",
      "used for each component function fj.\n",
      "•Selection methods, which adaptively scan the dictionary an d include\n",
      "only those basis functions hmthat contribute signiﬁcantly to the ﬁt of\n",
      "the model. Here the variable selection techniques discusse d in Chap-\n",
      "ter 3 are useful. The stagewise greedy approaches such as CAR T,\n",
      "MARS and boosting fall into this category as well.\n",
      "•Regularization methods where we use the entire dictionary b ut re-\n",
      "strict the coeﬃcients. Ridge regression is a simple example of a regu-\n",
      "larization approach, while the lasso is both a regularizati on and selec-\n",
      "tion method. Here we discuss these and more sophisticated me thods\n",
      "for regularization.\n",
      "5.2 Piecewise Polynomials and Splines\n",
      "We assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\n",
      "nomial function f(X) is obtained by dividing the domain of Xinto contigu-\n",
      "ous intervals, and representing fby a separate polynomial in each interval.\n",
      "Figure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\n",
      "constant, with three basis functions:\n",
      "h1(X) =I(X <ξ 1), h2(X) =I(ξ1≤X <ξ 2), h3(X) =I(ξ2≤X).\n",
      "Since these are positive over disjoint regions, the least sq uares estimate of\n",
      "the modelf(X) =∑3\n",
      "m=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\n",
      "in themth region.\n",
      "The top right panel shows a piecewise linear ﬁt. Three additi onal basis\n",
      "functions are needed: hm+3=hm(X)X, m= 1,...,3. Except in special\n",
      "cases, we would typically prefer the third panel, which is al so piecewise\n",
      "linear, but restricted to be continuous at the two knots. The se continu-\n",
      "ity restrictions lead to linear constraints on the paramete rs; for example,\n",
      "f(ξ−\n",
      "1) =f(ξ+\n",
      "1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\n",
      "are two restrictions, we expect to get backtwo parameters, leaving four free\n",
      "parameters.\n",
      "A more direct way to proceed in this case is to use a basis that i ncorpo-\n",
      "rates the constraints:\n",
      "h1(X) = 1, h2(X) =X, h 3(X) = (X−ξ1)+, h4(X) = (X−ξ2)+,\n",
      "wheret+denotes the positive part. The function h3is shown in the lower\n",
      "right panel of Figure 5.1. We often prefer smoother function s, and these\n",
      "can be achieved by increasing the order of the local polynomi al. Figure 5.2\n",
      "shows a series of piecewise-cubic polynomials ﬁt to the same data, with\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"142 5. Basis Expansions and Regularization\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOPiecewise Constant\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOPiecewise Linear\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOContinuous Piecewise Linear Piecewise-linear Basis Function\n",
      "•\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•• • ••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "• ••••\n",
      "• ••\n",
      "•\n",
      "••\n",
      "••\n",
      "ξ1 ξ1ξ1 ξ1\n",
      "ξ2 ξ2ξ2 ξ2\n",
      "(X−ξ1)+\n",
      "FIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to so me\n",
      "artiﬁcial data. The broken vertical lines indicate the posit ions of the two knots\n",
      "ξ1andξ2. The blue curve represents the true function, from which the d ata were\n",
      "generated with Gaussian noise. The remaining two panels show p iecewise lin-\n",
      "ear functions ﬁt to the same data—the top right unrestricted , and the lower left\n",
      "restricted to be continuous at the knots. The lower right pane l shows a piecewise–\n",
      "linear basis function, h3(X) = (X−ξ1)+, continuous at ξ1. The black points\n",
      "indicate the sample evaluations h3(xi), i= 1,...,N.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.2 Piecewise Polynomials and Splines 143\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOODiscontinuous\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous First Derivative\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous Second DerivativePiecewise Cubic Polynomials\n",
      "ξ1 ξ1ξ1 ξ1\n",
      "ξ2 ξ2ξ2 ξ2\n",
      "FIGURE 5.2. A series of piecewise-cubic polynomials, with increasing ord ers of\n",
      "continuity.\n",
      "increasing orders of continuity at the knots. The function i n the lower\n",
      "right panel is continuous, and has continuous ﬁrst and secon d derivatives\n",
      "at the knots. It is known as a cubic spline . Enforcing one more order of\n",
      "continuity would lead to a global cubic polynomial. It is not hard to show\n",
      "(Exercise 5.1) that the following basis represents a cubic s pline with knots\n",
      "atξ1andξ2:\n",
      "h1(X) = 1, h3(X) =X2, h5(X) = (X−ξ1)3\n",
      "+,\n",
      "h2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\n",
      "+.(5.3)\n",
      "Therearesixbasisfunctionscorrespondingtoasix-dimens ionallinearspace\n",
      "of functions. A quick check conﬁrms the parameter count: (3 r egions)×(4\n",
      "parameters per region) −(2 knots)×(3 constraints per knot)= 6.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"144 5. Basis Expansions and Regularization\n",
      "More generally, an order- Mspline with knots ξj, j= 1,...,Kis a\n",
      "piecewise-polynomial of order M, and has continuous derivatives up to\n",
      "orderM−2. A cubic spline has M= 4. In fact the piecewise-constant\n",
      "function in Figure 5.1 is an order-1 spline, while the contin uous piece-\n",
      "wise linear function is an order-2 spline. Likewise the gene ral form for the\n",
      "truncated-power basis set would be\n",
      "hj(X) =Xj−1, j= 1,...,M,\n",
      "hM+ℓ(X) = (X−ξℓ)M−1\n",
      "+, ℓ= 1,...,K.\n",
      "It is claimed that cubic splines are the lowest-order spline for which the\n",
      "knot-discontinuity is not visible to the human eye. There is seldom any\n",
      "good reason to go beyond cubic-splines, unless one is intere sted in smooth\n",
      "derivatives. In practice the most widely used orders are M= 1,2 and 4.\n",
      "These ﬁxed-knot splines are also known as regression splines . One needs\n",
      "to select the order of the spline, the number of knots and thei r placement.\n",
      "One simple approach is to parameterize a family of splines by the number\n",
      "of basis functions or degrees of freedom, and have the observ ationsxide-\n",
      "termine the positions of the knots. For example, the express ionbs(x,df=7)\n",
      "inRgenerates a basis matrix of cubic-spline functions evaluat ed at theN\n",
      "observations in x, with the 7−3 = 41interior knots at the appropriate per-\n",
      "centilesof x(20,40,60and80th.)Onecanbemoreexplicit,however; bs(x,\n",
      "degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\n",
      "with three interior knots, and returns an N×4 matrix.\n",
      "Sincethespaceofsplinefunctionsofaparticularorderand knotsequence\n",
      "is a vector space, there are many equivalent bases for repres enting them\n",
      "(just as there are for ordinary polynomials.) While the trun cated power\n",
      "basis is conceptually simple, it is not too attractive numer ically: powers of\n",
      "large numbers can lead to severe rounding problems. The B-spline basis,\n",
      "described in the Appendix to this chapter, allows for eﬃcien t computations\n",
      "even when the number of knots Kis large.\n",
      "5.2.1 Natural Cubic Splines\n",
      "We know that the behavior of polynomials ﬁt to data tends to be erratic\n",
      "near the boundaries, and extrapolation can be dangerous. Th ese problems\n",
      "are exacerbated with splines. The polynomials ﬁt beyond the boundary\n",
      "knots behave even more wildly than the corresponding global polynomials\n",
      "in that region. This can be conveniently summarized in terms of the point-\n",
      "wise variance of spline functions ﬁt by least squares (see th e example in the\n",
      "next section for details on these variance calculations). F igure 5.3 compares\n",
      "1A cubic spline with four knots is eight-dimensional. The bs()function omits by\n",
      "default the constant term in the basis, since terms like this are typical ly included with\n",
      "other terms in the model.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.2 Piecewise Polynomials and Splines 145\n",
      "XPointwise Variances\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••••••••••••• •••• ••••••••••••••••••••••••••\n",
      "•••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••••••••••••••••••••••••\n",
      "•\n",
      "••••••••••••••••••\n",
      "•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\n",
      "Global Cubic Polynomial\n",
      "Cubic Spline - 2 knots\n",
      "Natural Cubic Spline - 6 knots\n",
      "FIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-\n",
      "sisting of 50points drawn at random from U[0,1], and an assumed error model\n",
      "with constant variance. The linear and cubic polynomial ﬁts ha ve two and four\n",
      "degrees of freedom, respectively, while the cubic spline and na tural cubic spline\n",
      "each have six degrees of freedom. The cubic spline has two knot s at0.33and0.66,\n",
      "while the natural spline has boundary knots at 0.1and0.9, and four interior knots\n",
      "uniformly spaced between them.\n",
      "the pointwise variances for a variety of diﬀerent models. Th e explosion of\n",
      "the variance near the boundaries is clear, and inevitably is worst for cubic\n",
      "splines.\n",
      "Anatural cubic spline adds additional constraints, namely that the func-\n",
      "tion is linear beyond the boundary knots. This frees up four d egrees of\n",
      "freedom (two constraints each in both boundary regions), wh ich can be\n",
      "spent more proﬁtably by sprinkling more knots in the interio r region. This\n",
      "tradeoﬀ is illustrated in terms of variance in Figure 5.3. Th ere will be a\n",
      "price paid in bias near the boundaries, but assuming the func tion is lin-\n",
      "ear near the boundaries (where we have less information anyw ay) is often\n",
      "considered reasonable.\n",
      "A natural cubic spline with Kknots is represented by Kbasis functions.\n",
      "One can start from a basis for cubic splines, and derive the re duced ba-\n",
      "sis by imposing the boundary constraints. For example, star ting from the\n",
      "truncated power series basis described in Section 5.2, we ar rive at (Exer-\n",
      "cise 5.4):\n",
      "N1(X) = 1, N2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"146 5. Basis Expansions and Regularization\n",
      "where\n",
      "dk(X) =(X−ξk)3\n",
      "+−(X−ξK)3\n",
      "+\n",
      "ξK−ξk. (5.5)\n",
      "Each of these basis functions can be seen to have zero second a nd third\n",
      "derivative for X≥ξK.\n",
      "5.2.2 Example: South African Heart Disease (Continued)\n",
      "In Section 4.4.2 we ﬁt linear logistic regression models to t he South African\n",
      "heart disease data. Here we explore nonlinearities in the fu nctions using\n",
      "natural splines. The functional form of the model is\n",
      "logit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+···+hp(Xp)Tθp,(5.6)\n",
      "where each of the θjare vectors of coeﬃcients multiplying their associated\n",
      "vector of natural spline basis functions hj.\n",
      "We use four natural spline bases for each term in the model. Fo r example,\n",
      "withX1representing sbp,h1(X1) is a basis consisting of four basis func-\n",
      "tions. This actually implies three rather than two interior knots (chosen at\n",
      "uniform quantiles of sbp), plus two boundary knots at the extremes of the\n",
      "data, since we exclude the constant term from each of the hj.\n",
      "Sincefamhist is a two-level factor, it is coded by a simple binary or\n",
      "dummy variable, and is associated with a single coeﬃcient in the ﬁt of the\n",
      "model.\n",
      "More compactly we can combine all pvectors of basis functions (and\n",
      "the constant term) into one big vector h(X), and then the model is simply\n",
      "h(X)Tθ, with total number of parameters df = 1 +∑p\n",
      "j=1dfj, the sum of\n",
      "the parameters in each component term. Each basis function i s evaluated\n",
      "at each of the Nsamples, resulting in a N×df basis matrix H. At this\n",
      "point the model is like any other linear logistic model, and t he algorithms\n",
      "described in Section 4.4.1 apply.\n",
      "We carried out a backward stepwise deletion process, droppi ng terms\n",
      "from this model while preserving the group structure of each term, rather\n",
      "than dropping one coeﬃcient at a time. The AIC statistic (Sec tion 7.5) was\n",
      "used to drop terms, and all the terms remaining in the ﬁnal mod el would\n",
      "cause AIC to increase if deleted from the model (see Table 5.1 ). Figure 5.4\n",
      "shows a plot of the ﬁnal model selected by the stepwise regres sion. The\n",
      "functions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\n",
      "covariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, whereW\n",
      "is the diagonal weight matrix from the logistic regression. Hencevj(Xj) =\n",
      "Var[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,\n",
      "whereCov( ˆθj) =ˆΣjjistheappropriatesub-matrixof ˆΣ.Theshadedregion\n",
      "in each panel is deﬁned by ˆfj(Xj)±2√\n",
      "vj(Xj).\n",
      "The AIC statistic is slightly more generous than the likelih ood-ratio test\n",
      "(deviance test). Both sbpandobesityare included in this model, while\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.2 Piecewise Polynomials and Splines 147\n",
      "100 120 140 160 180 200 220-2 0 2 4\n",
      "0 5 10 15 20 25 300 2 4 6 8\n",
      "2 4 6 8 10 12 14-4 -2 0 2 4\n",
      "-4 -2 0 2 4\n",
      "Absent Present\n",
      "15 20 25 30 35 40 45-2 0 2 4 6\n",
      "20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\n",
      "sbp\n",
      "ˆf(tobacco)\n",
      "tobaccoˆf(ldl)\n",
      "ldlˆf(obesity)\n",
      "obesity\n",
      "ˆf(age)\n",
      "ageˆf(famhist)\n",
      "famhist\n",
      "FIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁn al\n",
      "model selected by the stepwise procedure. Included are pointw ise standard-error\n",
      "bands. The rug plot at the base of each ﬁgure indicates the location of each of the\n",
      "sample values for that variable (jittered to break ties).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"148 5. Basis Expansions and Regularization\n",
      "TABLE 5.1. Final logistic regression model, after stepwise deletion of na tural\n",
      "splines terms. The column labeled “LRT” is the likelihood-ratio te st statistic when\n",
      "that term is deleted from the model, and is the change in devianc e from the full\n",
      "model (labeled “none”).\n",
      "Terms Df Deviance AIC LRT P-value\n",
      "none 458.09 502.09\n",
      "sbp4 467.16 503.16 9.076 0.059\n",
      "tobacco 4 470.48 506.48 12.387 0.015\n",
      "ldl4 472.39 508.39 14.307 0.006\n",
      "famhist 1 479.44 521.44 21.356 0.000\n",
      "obesity 4 466.24 502.24 8.147 0.086\n",
      "age4 481.86 517.86 23.768 0.000\n",
      "they were not in the linear model. The ﬁgure explains why, sin ce their\n",
      "contributions are inherently nonlinear. These eﬀects at ﬁr st may come as\n",
      "a surprise, but an explanation lies in the nature of the retro spective data.\n",
      "These measurements were made sometime after the patients su ﬀered a\n",
      "heart attack, and in many cases they had already beneﬁted fro m a healthier\n",
      "diet and lifestyle, hence the apparent increase in risk at low values for\n",
      "obesityandsbp. Table 5.1 shows a summary of the selected model.\n",
      "5.2.3 Example: Phoneme Recognition\n",
      "In this example we use splines to reduce ﬂexibility rather th an increase it;\n",
      "the application comes under the general heading of functional modeling. In\n",
      "the top panel of Figure 5.5 are displayed a sample of 15 log-pe riodograms\n",
      "for each of the two phonemes “aa” and “ao” measured at 256 freq uencies.\n",
      "The goal is to use such data to classify a spoken phoneme. Thes e two\n",
      "phonemes were chosen because they are diﬃcult to separate.\n",
      "The input feature is a vector xof length 256, which we can think of as\n",
      "a vector of evaluations of a function X(f) over a grid of frequencies f. In\n",
      "reality there is a continuous analog signal which is a functi on of frequency,\n",
      "and we have a sampled version of it.\n",
      "The gray lines in the lower panel of Figure 5.5 show the coeﬃci ents of\n",
      "a linear logistic regression model ﬁt by maximum likelihood to a training\n",
      "sample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s . The\n",
      "coeﬃcients are also plotted as a function of frequency, and i n fact we can\n",
      "think of the model in terms of its continuous counterpart\n",
      "logPr(aa|X)\n",
      "Pr(ao|X)=∫\n",
      "X(f)β(f)df, (5.7)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.2 Piecewise Polynomials and Splines 149\n",
      "FrequencyLog-periodogram\n",
      "0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\n",
      "aa\n",
      "ao\n",
      "FrequencyLogistic Regression Coefficients\n",
      "0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\n",
      "FIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\n",
      "quency for 15examples each of the phonemes “aa” and “ao” sampled from a total\n",
      "of695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\n",
      "spaced frequencies. The lower panel shows the coeﬃcients (as a function of fre-\n",
      "quency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\n",
      "256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\n",
      "in the red curve, and are unrestricted in the jagged gray curv e.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"150 5. Basis Expansions and Regularization\n",
      "which we approximate by\n",
      "256∑\n",
      "j=1X(fj)β(fj) =256∑\n",
      "j=1xjβj. (5.8)\n",
      "The coeﬃcients compute a contrast functional, and will have appreciable\n",
      "values in regions of frequency where the log-periodograms d iﬀer between\n",
      "the two classes.\n",
      "The gray curves are very rough. Since the input signals have f airly strong\n",
      "positive autocorrelation, this results in negative autoco rrelation in the co-\n",
      "eﬃcients. In addition the sample size eﬀectively provides o nly four obser-\n",
      "vations per coeﬃcient.\n",
      "Applications such as this permit a natural regularization. We force the\n",
      "coeﬃcientstovarysmoothlyasafunctionoffrequency.Ther edcurveinthe\n",
      "lower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\n",
      "data.Weseethatthelowerfrequenciesoﬀerthemostdiscrim inatorypower.\n",
      "Not only does the smoothing allow easier interpretation of t he contrast, it\n",
      "also produces a more accurate classiﬁer:\n",
      "RawRegularized\n",
      "Training error 0.080 0.185\n",
      "Test error 0.255 0.158\n",
      "The smooth red curve was obtained through a very simple use of natural\n",
      "cubic splines. We can represent the coeﬃcient function as an expansion of\n",
      "splinesβ(f) =∑M\n",
      "m=1hm(f)θm. In practice this means that β=Hθwhere,\n",
      "His ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\n",
      "frequencies. Here we used M= 12 basis functions, with knots uniformly\n",
      "placed over the integers 1 ,2,...,256 representing the frequencies. Since\n",
      "xTβ=xTHθ, we can simply replace the input features xby their ﬁltered\n",
      "versionsx∗=HTx, and ﬁtθby linear logistic regression on the x∗. The\n",
      "red curve is thus ˆβ(f) =h(f)Tˆθ.\n",
      "5.3 Filtering and Feature Extraction\n",
      "In the previous example, we constructed a p×Mbasis matrix H, and then\n",
      "transformed our features xinto new features x∗=HTx. These ﬁltered\n",
      "versions of the features were then used as inputs into a learn ing procedure:\n",
      "in the previous example, this was linear logistic regressio n.\n",
      "Preprocessing of high-dimensional features is a very gener al and pow-\n",
      "erful method for improving the performance of a learning alg orithm. The\n",
      "preprocessing need not be linear as it was above, but can be a g eneral\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.4 Smoothing Splines 151\n",
      "(nonlinear) function of the form x∗=g(x). The derived features x∗can\n",
      "then be used as inputs into any (linear or nonlinear) learnin g procedure.\n",
      "Forexample,forsignalorimagerecognitionapopularappro achistoﬁrst\n",
      "transform the raw features via a wavelet transform x∗=HTx(Section 5.9)\n",
      "and then use the features x∗as inputs into a neural network (Chapter 11).\n",
      "Wavelets are eﬀective in capturing discrete jumps or edges, and the neural\n",
      "network is a powerful tool for constructing nonlinear funct ions of these\n",
      "features for predicting the target variable. By using domai n knowledge\n",
      "to construct appropriate features, one can often improve up on a learning\n",
      "method that has only the raw features xat its disposal.\n",
      "5.4 Smoothing Splines\n",
      "Here we discuss a spline basis method that avoids the knot sel ection prob-\n",
      "lem completely by using a maximal set of knots. The complexit y of the ﬁt\n",
      "is controlled by regularization. Consider the following pr oblem: among all\n",
      "functionsf(x) with two continuous derivatives, ﬁnd one that minimizes th e\n",
      "penalized residual sum of squares\n",
      "RSS(f,λ) =N∑\n",
      "i=1{yi−f(xi)}2+λ∫\n",
      "{f′′(t)}2dt, (5.9)\n",
      "whereλis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\n",
      "to the data, while the second term penalizes curvature in the function, and\n",
      "λestablishes a tradeoﬀ between the two. Two special cases are :\n",
      "λ= 0 :fcan be any function that interpolates the data.\n",
      "λ=∞:the simple least squares line ﬁt, since no second derivative can\n",
      "be tolerated.\n",
      "Thesevaryfromveryroughtoverysmooth,andthehopeisthat λ∈(0,∞)\n",
      "indexes an interesting class of functions in between.\n",
      "The criterion (5.9) is deﬁned on an inﬁnite-dimensional fun ction space—\n",
      "in fact, a Sobolev space of functions for which the second ter m is deﬁned.\n",
      "Remarkably, it can be shown that (5.9) has an explicit, ﬁnite -dimensional,\n",
      "unique minimizer which is a natural cubic spline with knots a t the unique\n",
      "values of the xi, i= 1,...,N(Exercise 5.7). At face value it seems that\n",
      "the family is still over-parametrized, since there are as ma ny asNknots,\n",
      "which implies Ndegrees of freedom. However, the penalty term translates\n",
      "to a penalty on the spline coeﬃcients, which are shrunk some o f the way\n",
      "toward the linear ﬁt.\n",
      "Since the solution is a natural spline, we can write it as\n",
      "f(x) =N∑\n",
      "j=1Nj(x)θj, (5.10)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"152 5. Basis Expansions and Regularization\n",
      "AgeRelative Change in Spinal BMD\n",
      "10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "• ••• ••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "• •••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••••\n",
      "• •\n",
      "••••\n",
      "••• •\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "• •••\n",
      "•\n",
      "•\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "• •••\n",
      "• ••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "••\n",
      "•\n",
      "••••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "• ••\n",
      "•••••••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•• ••\n",
      "•\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "• •\n",
      "•••\n",
      "•\n",
      "•••\n",
      "• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••\n",
      "• ••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "• ••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•Male\n",
      "Female\n",
      "FIGURE 5.6. The response is the relative change in bone mineral density me a-\n",
      "sured at the spine in adolescents, as a function of age. A separ ate smoothing spline\n",
      "was ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\n",
      "about12degrees of freedom.\n",
      "where the Nj(x) are anN-dimensional set of basis functions for repre-\n",
      "senting this family of natural splines (Section 5.2.1 and Ex ercise 5.4). The\n",
      "criterion thus reduces to\n",
      "RSS(θ,λ) = (y−Nθ)T(y−Nθ)+λθTΩNθ, (5.11)\n",
      "where{N}ij=Nj(xi) and{ΩN}jk=∫\n",
      "N′′\n",
      "j(t)N′′\n",
      "k(t)dt. The solution is\n",
      "easily seen to be\n",
      "ˆθ= (NTN+λΩN)−1NTy, (5.12)\n",
      "a generalized ridge regression. The ﬁtted smoothing spline is given by\n",
      "ˆf(x) =N∑\n",
      "j=1Nj(x)ˆθj. (5.13)\n",
      "Eﬃcient computational techniques for smoothing splines ar e discussed in\n",
      "the Appendix to this chapter.\n",
      "Figure 5.6 shows a smoothing spline ﬁt to some data on bone min eral\n",
      "density (BMD) in adolescents. The response is relative chan ge in spinal\n",
      "BMD over two consecutive visits, typically about one year ap art. The data\n",
      "are color coded by gender, and two separate curves were ﬁt. Th is simple\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.4 Smoothing Splines 153\n",
      "summary reinforces the evidence in the data that the growth s purt for\n",
      "females precedes that for males by about two years. In both ca ses the\n",
      "smoothing parameter λwas approximately 0 .00022; this choice is discussed\n",
      "in the next section.\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices\n",
      "We have not yet indicated how λis chosen for the smoothing spline. Later\n",
      "in this chapter we describe automatic methods using techniq ues such as\n",
      "cross-validation. In this section we discuss intuitive way s of prespecifying\n",
      "the amount of smoothing.\n",
      "A smoothing spline with prechosen λis an example of a linear smoother\n",
      "(as in linear operator). This is because the estimated param eters in (5.12)\n",
      "are a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\n",
      "ˆf(xi) at the training predictors xi. Then\n",
      "ˆf=N(NTN+λΩN)−1NTy\n",
      "=Sλy. (5.14)\n",
      "Again the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as\n",
      "thesmoother matrix . One consequence of this linearity is that the recipe\n",
      "for producing ˆffromydoes not depend on yitself;Sλdepends only on\n",
      "thexiandλ.\n",
      "Linear operators are familiar in more traditional least squ ares ﬁtting as\n",
      "well. Suppose Bξis aN×Mmatrix ofMcubic-spline basis functions\n",
      "evaluated at the Ntraining points xi, with knot sequence ξ, andM≪N.\n",
      "Then the vector of ﬁtted spline values is given by\n",
      "ˆf=Bξ(BT\n",
      "ξBξ)−1BT\n",
      "ξy\n",
      "=Hξy. (5.15)\n",
      "Here the linear operator Hξis a projection operator, also known as the hat\n",
      "matrix in statistics. There are some important similaritie s and diﬀerences\n",
      "between HξandSλ:\n",
      "•Both are symmetric, positive semideﬁnite matrices.\n",
      "•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\n",
      "hand side exceeds the left-hand side by a positive semideﬁni te matrix.\n",
      "This is a consequence of the shrinking nature of Sλ, which we discuss\n",
      "further below.\n",
      "•Hξhas rankM, whileSλhas rankN.\n",
      "The expression M= trace(Hξ) gives the dimension of the projection space,\n",
      "which is also the number of basis functions, and hence the num ber of pa-\n",
      "rameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"154 5. Basis Expansions and Regularization\n",
      "freedomof a smoothing spline to be\n",
      "dfλ= trace(Sλ), (5.16)\n",
      "the sum of the diagonal elements of Sλ. This very useful deﬁnition allows\n",
      "us a more intuitive way to parameterize the smoothing spline , and indeed\n",
      "many other smoothers as well, in a consistent fashion. For ex ample, in Fig-\n",
      "ure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\n",
      "λ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are\n",
      "many arguments supporting this deﬁnition of degrees of free dom, and we\n",
      "cover some of them here.\n",
      "SinceSλis symmetric (and positive semideﬁnite), it has a real eigen -\n",
      "decomposition. Before we proceed, it is convenient to rewri teSλin the\n",
      "Reinschform\n",
      "Sλ= (I+λK)−1, (5.17)\n",
      "whereKdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\n",
      "min\n",
      "f(y−f)T(y−f)+λfTKf, (5.18)\n",
      "Kis known as the penalty matrix , and indeed a quadratic form in Khas\n",
      "a representation in terms of a weighted sum of squared (divid ed) second\n",
      "diﬀerences. The eigen-decomposition of Sλis\n",
      "Sλ=N∑\n",
      "k=1ρk(λ)ukuT\n",
      "k (5.19)\n",
      "with\n",
      "ρk(λ) =1\n",
      "1+λdk, (5.20)\n",
      "anddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\n",
      "sults of applying a cubic smoothing spline to some air pollut ion data (128\n",
      "observations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\n",
      "penaltyλand arougherﬁt for a smaller penalty. The lower panels repre-\n",
      "sent the eigenvalues (lower left) and some eigenvectors (lo wer right) of the\n",
      "corresponding smoother matrices. Some of the highlights of the eigenrep-\n",
      "resentation are the following:\n",
      "•Theeigenvectorsarenotaﬀectedbychangesin λ,andhencethewhole\n",
      "family of smoothing splines (for a particular sequence x) indexed by\n",
      "λhave the same eigenvectors.\n",
      "•Sλy=∑N\n",
      "k=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\n",
      "ates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-\n",
      "entially shrinking the contributions using ρk(λ). This is to be con-\n",
      "trasted with a basis-regression method, where the componen ts are\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"5.4 Smoothing Splines 155\n",
      "Daggot Pressure GradientOzone Concentration\n",
      "-50 0 50 1000 10 20 30•••\n",
      "••••••\n",
      "••\n",
      "••••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•••••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "OrderEigenvalues\n",
      "5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••••••••• •• • • •• • ••••••••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•••••••••••••df=5\n",
      "df=11\n",
      "-50 0 50 100 -50 0 50 100\n",
      "FIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Dag got\n",
      "pressure gradient. The two ﬁts correspond to diﬀerent values of the smoothing\n",
      "parameter, chosen to achieve ﬁve and eleven eﬀective degrees of freedom, deﬁned\n",
      "by dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\n",
      "matrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\n",
      "sixth eigenvectors of the spline smoother matrices. In each c ase,ukis plotted\n",
      "againstx, and as such is viewed as a function of x. Therugat the base of the\n",
      "plots indicate the occurrence of data points. The damped func tions represent the\n",
      "smoothed versions of these functions (using the 5df smoother).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The book \"The Elements of Statistical Learning\" is a comprehensive resource that covers various topics in statistics, data mining, and machine learning. Written by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, who are renowned professors in the field, the book provides a conceptual framework for understanding these areas and includes many examples and color graphics. The second edition of the book includes additional topics such as graphical models, random forests, and ensemble methods. It is a valuable resource for statisticians and anyone interested in data mining in science or industry.\n",
      "\n",
      "The page is dedicated to the parents and families of the individuals named.\n",
      "\n",
      "\"vi\" is a text editor that is commonly used in Unix-like operating systems. It is a command-line based editor that allows users to create, edit, and view text files.\n",
      "\n",
      "The second edition of \"The Elements of Statistical Learning\" has been released, with four new chapters and updates to existing chapters. The layout of the book has been kept largely the same to maintain familiarity for readers.\n",
      "\n",
      "The second edition of the book includes new chapters and updates to existing chapters on topics such as supervised learning, regression, classification, kernel smoothing methods, model assessment and selection, neural networks, support vector machines, unsupervised learning, random forests, ensemble learning, undirected graphical models, and high-dimensional problems. The edition also addresses concerns from colorblind readers and clarifies the discussion of error-rate estimation from the first edition.\n",
      "\n",
      "The preface to the second edition of a book suggests that chapters 15 and 16 should be read after chapter 10. Chapter 17 only covers undirected graphical models and does not include directed models. Chapter 18 explores the problem of learning in high-dimensional feature spaces. The authors apologize for errors in the first edition and thank those who provided comments and guidance for the new edition. The edition is dedicated to the memory of Anna McPhee.\n",
      "\n",
      "The \"Preface to the Second Edition\" introduces the second edition of a book or publication. It provides a brief overview of any updates or changes made to the content since the first edition.\n",
      "\n",
      "The field of Statistics is facing challenges due to the increasing size and complexity of statistical problems in various industries. The advent of computers and the information age has led to the emergence of new fields such as data mining and bioinformatics. The statistician's role is to extract important patterns and trends from vast amounts of data and learn from it. The challenges in learning from data have led to a revolution in the statistical sciences, with a significant contribution from researchers in computer science and engineering. Learning problems can be categorized as either supervised or unsupervised, with the goal being to predict outcomes or describe associations and patterns among input measures.\n",
      "\n",
      "The preface to the first edition of the book emphasizes the importance of learning and explains that the book will present new ideas in learning within a statistical framework. The authors hope that the book will be useful to a wide range of researchers and practitioners. They acknowledge the contributions of various individuals to the completion of the book and express their gratitude to their families and parents for their support. The preface also mentions the impact of statisticians in changing the way we reason, experiment, and form opinions.\n",
      "\n",
      "This is a table of contents for a book on supervised learning. It includes a preface to the second edition and first edition, as well as an introduction and an overview of supervised learning techniques such as least squares and nearest neighbors. It also covers statistical decision theory, local methods in high dimensions, statistical models, supervised learning, function approximation, and structured regression models.\n",
      "\n",
      "This section of the book discusses classes of restricted estimators, including roughness penalty and Bayesian methods, kernel methods and local regression, and basis functions and dictionary methods. It also covers model selection and the bias-variance tradeoff. The next section focuses on linear methods for regression, such as linear regression models and least squares, subset selection, shrinkage methods, methods using derived input directions, and more. It also explores computational considerations.\n",
      "\n",
      "This section of the book covers linear methods for classification. It includes topics such as linear regression of an indicator matrix, linear discriminant analysis, logistic regression, separating hyperplanes, basis expansions and regularization. The chapter also discusses topics such as splines, filtering and feature extraction, smoothing splines, nonparametric logistic regression, multidimensional splines, regularization and reproducing kernel Hilbert spaces, and wavelet smoothing.\n",
      "\n",
      "This section of the book discusses kernel smoothing methods for regression and density estimation. It covers topics such as one-dimensional kernel smoothers, selecting the width of the kernel, local regression in multiple dimensions, structured local regression models, kernel density estimation and classification, radial basis functions and kernels, and mixture models for density estimation and classification. It also discusses model assessment and selection techniques, including bias, variance, and model complexity, the bias-variance decomposition, estimates of in-sample prediction error, the effective number of parameters, the Bayesian approach and BIC, minimum description length, Vapnik-Chervonenkis dimension, cross-validation, bootstrap methods, and conditional or expected test error. Finally, it explores model inference and averaging.\n",
      "\n",
      "This section of the book covers various statistical methods such as the Bootstrap and Maximum Likelihood Methods, Bayesian Methods, the EM Algorithm, MCMC for Sampling from the Posterior, Bagging, Model Averaging and Stacking, Stochastic Search, Additive Models, Trees and Related Methods, Boosting Methods, and Additive Trees. Each method is explained with examples and exercises provided for further practice.\n",
      "\n",
      "This section of the book covers the topic of boosting, which involves fitting an additive model and using exponential loss and AdaBoost. It also discusses the use of off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, regularization techniques, and interpretation methods. The section includes various illustrations and exercises. The next section of the book focuses on neural networks, including the fitting of neural networks, training issues, and examples using simulated and ZIP code data. It also discusses Bayesian neural nets and computational considerations.\n",
      "\n",
      "The given text is an outline of the contents of a book or document, specifically focusing on chapters 12 and 13. Chapter 12 discusses support vector machines, flexible discriminants, and various methods of classification and regression. Chapter 13 covers prototype methods, nearest-neighbor classifiers, and adaptive nearest-neighbor methods. Each chapter includes exercises for practice.\n",
      "\n",
      "This section of the book discusses various topics related to unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components, non-negative matrix factorization, independent component analysis, multidimensional scaling, nonlinear dimension reduction, and the Google PageRank algorithm. The chapter provides an introduction to each topic and includes examples and practical issues.\n",
      "\n",
      "This section of the book covers topics such as Random Forests, Ensemble Learning, Undirected Graphical Models, and High-Dimensional Problems. It provides an introduction to each topic and explores various details, analysis, and exercises related to each.\n",
      "\n",
      "This section discusses various techniques for linear classifiers and regularization in high-dimensional data analysis. It covers topics such as linear discriminant analysis, nearest shrunken centroids, regularized discriminant analysis, logistic regression with quadratic regularization, support vector classifiers, feature selection, L1 regularization, classification when features are unavailable, high-dimensional regression using supervised principal components, feature assessment, and the multiple-testing problem. The section also includes exercises and references for further reading.\n",
      "\n",
      "This passage discusses the importance of statistical learning in various fields and provides examples of learning problems. It mentions that learning from data is the focus of the book, where the goal is to predict an outcome based on a set of features using a training set of data.\n",
      "\n",
      "The summary describes a study that aimed to predict whether an email was spam or not. The study analyzed the relative frequencies of words and characters in 4601 email messages. The objective was to develop a spam detector. The study focused on supervised learning, where the outcome variable was known, and used classification methods to determine if an email was spam or not. The study also mentioned the importance of feature selection in creating rules for classification.\n",
      "\n",
      "The given text appears to be a mixture of alphanumeric characters and symbols. It is difficult to determine the exact nature or meaning of the content without further context or clarification.\n",
      "\n",
      "The given text is a series of repeating patterns made up of the characters \"o\" and \" \". The patterns do not form any coherent words or sentences.\n",
      "\n",
      "The text discusses two examples: email spam filtering and prostate cancer data analysis. For email spam filtering, the goal is to accurately classify emails as spam or not spam. Various methods are discussed in the book to address this problem. The prostate cancer data example involves examining the correlation between the level of prostate-specific antigen (PSA) and other predictors in predicting the presence of prostate cancer. The data is displayed in a scatterplot matrix.\n",
      "\n",
      "The study by Stamey et al. (1989) that investigated the correlation between the level of something had an error in the data in the first edition of the book. Subject 32 had a value of 6.1 for lweight, which was incorrect. The correct value is 44.9 gm. Prof. Stephen W. Link alerted the authors to this error.\n",
      "\n",
      "The first example involves predicting the log of PSA from various measurements in men about to undergo prostate surgery. The second example involves recognizing handwritten digits from postal envelopes using image data. Both examples are supervised learning problems, with the first being a regression problem and the second being a classification problem.\n",
      "\n",
      "This text introduces the concept of DNA microarrays and gene expression datasets. It explains how DNA microarrays work by measuring the expression of genes in cells. The text also discusses the challenges of understanding how genes and samples are organized in gene expression datasets and suggests viewing it as an unsupervised learning problem.\n",
      "\n",
      "The given text describes a DNA microarray data set consisting of 6830 genes and 64 samples from human tumors. The data is presented in a heat map format, with bright green indicating underexpression, bright red indicating overexpression, and gray indicating missing values. The order of the rows and columns in the display is randomized.\n",
      "\n",
      "This book is intended for researchers and students in various fields, such as statistics, artificial intelligence, engineering, and finance. It provides an overview of important learning methods and concepts, with an emphasis on intuitive understanding rather than mathematical details. The book is organized in a way that starts with simple methods and gradually progresses to more complex ones. It covers topics such as linear regression, splines, wavelets, kernel methods, model assessment and selection, model inference and averaging, structured methods for supervised learning, unsupervised learning, random forests, ensemble learning, undirected graphical models, and high-dimensional problems. Each chapter also includes computational considerations and bibliographic notes for further reading.\n",
      "\n",
      "The book recommends reading Chapters 1-4 and Chapter 7 in sequence, with Chapter 7 being mandatory. The rest of the book can be read sequentially or sampled based on the reader's interest. The book's website provides additional resources and data sets. Instructors can use the book for a two or three-quarter course, with exercises provided at the end of each chapter. Good software tools, such as R and S-PLUS, are important for students studying these topics.\n",
      "\n",
      "This section discusses supervised learning, which involves using inputs to predict outputs. The inputs are often referred to as predictors or independent variables, while the outputs are referred to as responses or dependent variables. The outputs can vary in nature, such as quantitative measurements or qualitative categories.\n",
      "\n",
      "This passage discusses the two types of outputs in supervised learning: quantitative (regression) and qualitative (classification). It also mentions the different types of input variables: qualitative, quantitative, and ordered categorical. The use of numeric codes to represent qualitative variables is explained, with the most commonly used method being dummy variables. The passage concludes by introducing the notation used for input and output variables in supervised learning.\n",
      "\n",
      "This section discusses two methods for prediction: least squares and nearest neighbors. The linear model, fitted by least squares, assumes a linear relationship between the inputs and the output. The k-nearest-neighbor prediction rule, on the other hand, makes minimal assumptions and can provide accurate but potentially unstable predictions. The linear model is represented by a linear equation, while the k-nearest-neighbor rule is based on finding the nearest neighbors in the training data.\n",
      "\n",
      "This section discusses supervised learning and the use of linear models. The linear model is represented by the equation f(X) = XTβ, where f(X) is a linear function and β is a vector of coefficients. The most popular method for fitting the linear model is the method of least squares, which minimizes the residual sum of squares. The solution to the least squares problem is given by the normal equations, and the fitted values are obtained using the coefficients. An example of the linear model in a classification context is also provided.\n",
      "\n",
      "This section discusses the use of least squares and nearest neighbors in linear regression of 0/1 response variables.\n",
      "\n",
      "The summary provided does not contain any information.\n",
      "\n",
      "The given text is too long to provide a concise summary.\n",
      "\n",
      "The provided text does not contain any readable content.\n",
      "\n",
      "The text provided does not contain any information or context to summarize.\n",
      "\n",
      "The given text is describing a classification example in two dimensions using linear regression. The classes are coded as binary variables (BLUE=0, ORANGE=1) and a decision boundary is defined by xTβ=0.5. The text discusses the misclassifications on both sides of the decision boundary and presents two possible scenarios for the training data generation: Scenario 1 where the data is generated from bivariate Gaussian distributions with different means, and Scenario 2 where the data is generated from a mixture of 10 low-variance Gaussian distributions with individual means distributed as Gaussian. The concept of a mixture of Gaussians is briefly explained.\n",
      "\n",
      "This section discusses supervised learning methods, specifically focusing on the use of Gaussian distributions and nearest-neighbor methods. It explains that in cases where there is one Gaussian per class, a linear decision boundary is the best option, but in cases with mixtures of tightly clustered Gaussians, a nonlinear and disjoint decision boundary is more optimal. Nearest-neighbor methods use the observations closest to a given input to form a prediction, and the k-nearest neighbor fit is defined as averaging the responses of the k closest points. The results of using 15-nearest-neighbor averaging and 1-nearest-neighbor classification are shown, demonstrating the irregularity of decision boundaries in these methods. The same method can also be applied for regression tasks.\n",
      "\n",
      "The summary states that the topic is about Least Squares and Nearest Neighbors, specifically the 15-Nearest Neighbor Classifier.\n",
      "\n",
      "The provided text does not contain any information or context that would allow for a concise summary to be generated.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The given text is a long string of dots, which does not provide any meaningful content or information.\n",
      "\n",
      "There is no information provided to generate a concise summary.\n",
      "\n",
      "The given text is a random sequence of dots and dashes and does not provide any meaningful information or context.\n",
      "\n",
      "The given text consists of a series of dots and spaces arranged in a pattern.\n",
      "\n",
      "The text discusses the use of k-nearest neighbor fits in classification problems. It mentions that the error on the training data increases with larger values of k, and that k-nearest neighbors have a larger effective number of parameters compared to least-squares fits. It also suggests that k-nearest neighbor methods may be more appropriate for mixture scenarios, while Gaussian data may result in noisy decision boundaries.\n",
      "\n",
      "This section discusses the 1-nearest neighbor classifier and compares it to the least squares method. The 1-nearest neighbor classifier does not rely on strict assumptions about the data and can adapt to any situation, but it has high variance and low bias. The least squares method, on the other hand, has a smooth and stable decision boundary but relies on the assumption of a linear decision boundary. Each method has its own strengths and is more suitable for different scenarios. The data used for the comparison comes from a model that is closer to the scenario where the nearest neighbors method is more appropriate.\n",
      "\n",
      "The summary discusses the comparison between least squares and k-nearest neighbors for classifying new observations. It mentions that these two simple procedures are commonly used in various techniques, with 1-nearest-neighbor being popular for low-dimensional problems. The summary also mentions the enhancements made to these procedures, including the use of kernel methods and modified distance kernels in high-dimensional spaces.\n",
      "\n",
      "This section provides an overview of supervised learning, including local regression, linear models with basis expansion, and projection pursuit and neural network models. The section also introduces statistical decision theory, which involves the use of random variables and probability spaces to develop models. The theory utilizes a loss function to penalize prediction errors, with squared error loss being the most common. The goal is to minimize the expected (squared) prediction error, and the solution is found by calculating the conditional expectation, also known as the regression function. Nearest-neighbor methods aim to implement this approach using training data.\n",
      "\n",
      "This section discusses statistical decision theory and how it relates to the estimation of the regression function. The k-nearest neighbors method is introduced as a way to estimate the regression function by averaging the observed values in a neighborhood of the target point. However, this method may not be suitable for large sample sizes or high-dimensional data. Linear regression is also discussed as a model-based approach, where the regression function is assumed to be approximately linear. The least squares solution is derived as a way to estimate the regression coefficients in linear regression. Both k-nearest neighbors and least squares methods approximate conditional expectations but differ in their model assumptions.\n",
      "\n",
      "This section discusses supervised learning and different approaches to approximating the function f(x). It introduces the concept of k-nearest neighbors and discusses its limitations. The section also mentions model-based techniques like additive models, which assume additivity of coordinate functions. The use of different loss functions, such as L2 and L1, is discussed and the concept of estimating conditional median is introduced. The section also touches upon handling categorical variables and the use of different loss functions for penalizing prediction errors. The expected prediction error is defined as the expectation of the loss function.\n",
      "\n",
      "Statistical Decision Theory and Bayes Optimal Classifier are discussed in Section 2.4.\n",
      "\n",
      "The given text does not contain any meaningful information or message. It is a random sequence of dots and dashes without any clear meaning or pattern.\n",
      "\n",
      "The provided text does not contain any information or content that can be summarized.\n",
      "\n",
      "The given text is a repetition of the ellipsis symbol (\".\") and the space (\" \"). There is no meaningful content in the summary.\n",
      "\n",
      "The given text is a series of dots, indicating that it does not contain any meaningful information.\n",
      "\n",
      "The passage discusses the optimal Bayes decision boundary for a simulation example. The Bayes classiﬁer is the most probable class based on the conditional distribution, and the error rate of the Bayes classiﬁer is known as the Bayes rate. Figure 2.5 shows the Bayes-optimal decision boundary for the example.\n",
      "\n",
      "This passage discusses the k-nearest neighbor classifier as an approximation of the Bayes classifier. It also explores the limitations of local methods in high dimensions, known as the curse of dimensionality. It explains that as the number of dimensions increases, the effectiveness of k-nearest neighbor averaging decreases, and the sparsity of data points near the edges becomes a problem.\n",
      "\n",
      "The concept of the curse of dimensionality is discussed, which refers to the challenges that arise when dealing with high-dimensional data. It is shown that as the dimensionality increases, the volume of the data becomes sparser, making prediction more difficult. The sampling density is also shown to decrease exponentially with the dimensionality. An example is provided to illustrate these concepts.\n",
      "\n",
      "In supervised learning, the expected prediction error is computed to estimate the accuracy of the model. The mean squared error (MSE) is used to measure the accuracy of estimating the target variable. The MSE is decomposed into variance and squared bias. In a contrived example, as the dimension increases, both bias and variance increase. The complexity of functions in high dimensions requires a larger training set for accurate estimation. Linear relationships between variables can be modeled using least squares, where the predictions are based on the linear combination of the input variables and an error term.\n",
      "\n",
      "This summary discusses the curse of dimensionality and its impact on mean squared error (MSE), bias, and variance. It includes a simulation example with uniformly distributed input features in the range of [-1,1] for dimensions 1 to 10. The example demonstrates the error of the 1-nearest neighbor method in estimating the target function and shows how the radius of the 1-nearest neighborhood increases with dimension. The lower panels show the average radius of the neighborhoods and the curves for MSE, squared bias, and variance as a function of dimension.\n",
      "\n",
      "This passage discusses the concept of supervised learning and provides an example of a simulation with one-dimensional data. It also introduces the concepts of mean squared error (MSE), variance, and squared bias. The passage concludes with a comparison of 1-nearest neighbor and least squares methods in two different scenarios.\n",
      "\n",
      "This passage discusses the expected prediction error (EPE) of 1-nearest neighbor and least squares methods in high dimensions. It shows that the EPE of 1-nearest neighbor is always above 2 and increases with dimension, while least squares is unbiased but has a slightly higher EPE. The passage also mentions that the choice between these methods depends on the assumptions made and that there is a range of models with different assumptions and biases. The passage concludes by mentioning that statistical models play a role in the prediction framework.\n",
      "\n",
      "This section discusses the goal of finding an approximation to the underlying function that relates inputs and outputs in supervised learning. It mentions that nearest-neighbor methods can fail in high-dimensional spaces and if special structure is not considered. It introduces the concept of a statistical model for the joint distribution of the input and output variables. The additive error model is mentioned as a useful approximation, as it captures departures from a deterministic relationship. The assumption of independent and identically distributed errors is discussed.\n",
      "\n",
      "This section discusses statistical models, supervised learning, and function approximation. It explains the use of least squares as a data criterion for model estimation. It also discusses the use of additive error models for quantitative responses and conditional density models for qualitative outputs. The concept of supervised learning is introduced, where a learning algorithm is used to modify the input/output relationship based on observed data. The section also discusses function approximation and estimation in the context of supervised learning.\n",
      "\n",
      "This section discusses supervised learning and the goal of obtaining a useful approximation of a function using representations. It introduces the concept of parameterized functions and linear basis expansions. The least squares method is used to estimate the parameters, and a visual representation of the fitting process is provided. The linear model has a closed form solution, but for basis function methods with hidden parameters, iterative or numerical optimization methods are required.\n",
      "\n",
      "The given text discusses statistical models, supervised learning, and function approximation. It mentions the principle of maximum likelihood estimation and how it is used to estimate parameters in a random sample. The text also explains how least squares fitting and maximum likelihood are equivalent in the context of additive error models. Additionally, it introduces the multinomial likelihood for regression functions with qualitative outputs.\n",
      "\n",
      "This section discusses the concept of log-likelihood and how it is used in supervised learning to find values of θ that best fit the data. It also introduces structured regression models as an alternative to local methods, which may face problems in high dimensions. The section highlights the difficulty of the problem by explaining that minimizing the RSS criterion for an arbitrary function leads to infinitely many solutions. To obtain useful results for finite data, restrictions must be placed on the eligible solutions. These restrictions can be encoded through the parameter representation or built into the learning method itself. The book focuses on these restricted classes of solutions. However, it is noted that any restrictions imposed on the function that lead to a unique solution do not truly eliminate ambiguity.\n",
      "\n",
      "The text discusses the concept of restricted estimators in nonparametric regression. It explains that constraints imposed by learning methods are usually complexity restrictions, which result in special structures in small neighborhoods of the input space. The strength of the constraint is determined by the size of the neighborhood. Different methods use different metrics and neighborhood sizes to define the constraints. The text also mentions that methods that produce locally varying functions in small isotropic neighborhoods face problems in high dimensions. It concludes by mentioning that there are various classes of restricted estimators in nonparametric regression, with some methods falling into multiple classes.\n",
      "\n",
      "The passage discusses two broad classes of supervised learning methods: roughness penalty and Bayesian methods, and kernel methods and local regression. The roughness penalty approach penalizes the sum of squared residuals with a roughness penalty to control the smoothness of the function. This penalty can be constructed for functions in any dimension and can impose special structures. Kernel methods and local regression, on the other hand, provide estimates of the regression function or conditional expectation by specifying the local neighborhood and the class of regular functions fitted locally. The local neighborhood is specified by a kernel function.\n",
      "\n",
      "In this section, the authors discuss different classes of restricted estimators. They introduce the concept of kernel estimation, which assigns weights to points based on their distance from a specific point. They also discuss local regression estimation, where a parameterized function is used to minimize the residual sum of squares. They mention that nearest-neighbor methods can be seen as a variation of kernel methods. Lastly, they mention the use of basis functions and dictionary methods, which allow for more flexible models.\n",
      "\n",
      "This passage discusses various methods of supervised learning, including polynomial splines, radial basis functions, and single-layer feed-forward neural networks. These methods involve the use of basis functions that are determined by parameters or knots. The passage also mentions the use of greedy algorithms or two-stage processes to determine these parameters. These methods are considered adaptive basis function methods or dictionary methods, where a set of candidate basis functions is available to choose from.\n",
      "\n",
      "Model selection involves determining the parameter values that optimize the performance of a model. This is necessary for models like smoothing splines and local polynomial models that have complexity parameters. Using residual sum-of-squares on the training data is not suitable for selecting these parameters as it would always result in interpolating fits with zero residuals. The k-nearest-neighbor regression fit demonstrates the trade-off between bias and variance in predictive ability. The expected prediction error can be decomposed into reducible error, bias, and variance. The reducible error is beyond our control, while the bias and variance components can be managed. The bias term increases with k and represents the difference between the true mean and the expected value of the estimate.\n",
      "\n",
      "The text discusses the tradeoff between bias and variance in supervised learning. It explains that as model complexity increases, variance tends to increase and bias tends to decrease. However, if the model complexity is too high, it may lead to overfitting and poor generalization. On the other hand, if the model complexity is too low, it may result in underfitting and large bias. The goal is to choose the right level of model complexity to minimize test error. The text also mentions that training error is not a good estimate of test error and discusses methods for estimating test error and optimal model complexity.\n",
      "\n",
      "This chapter provides bibliographic notes and exercises related to the learning problem. It mentions several books on the topic and states that parts of the chapter are based on Friedman (1994b). The exercises include topics such as classifying to the closest target, computing the Bayes decision boundary, deriving equations, discussing the edge effect problem, and solving a regression problem using weighted least squares.\n",
      "\n",
      "The summary is about supervised learning and the construction of estimators for regression functions. It discusses linear regression, k-nearest neighbor regression, and their weights. It also discusses decomposing mean-squared errors into bias and variance components. Additionally, it compares the classification performance of linear regression and k-nearest neighbor classification on zip code data. Lastly, it proves a relationship between the squared biases and variances in linear regression models.\n",
      "\n",
      "Exercise 41 involves calculating expectations for random expressions, and it was brought to attention by Ryan Tibshirani from a homework assignment given by Andrew Ng.\n",
      "\n",
      "\"42 2. Overview of Supervised Learning\" provides an overview of the concept of supervised learning.\n",
      "\n",
      "This section introduces linear regression models, which assume that the regression function is linear in the inputs. Linear models are simple and often provide an adequate description of how the inputs affect the output. They can sometimes outperform nonlinear models in certain situations. Linear methods can also be applied to transformations of the inputs. The next chapter will discuss linear methods for classification, and an understanding of linear methods is essential for understanding nonlinear techniques.\n",
      "\n",
      "This section discusses linear regression models and least squares as a method for estimating the parameters in the model. The linear regression model assumes that the regression function is linear or a reasonable approximation. The parameters in the model can come from different sources such as quantitative inputs, transformations of inputs, basis expansions, dummy coding of qualitative inputs, and interactions between variables. The least squares method is used to estimate the parameters by minimizing the residual sum of squares. This method is valid if the training observations are independent random draws from their population.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This section discusses linear regression models and least squares fitting. The goal is to find the linear function that minimizes the sum of squared residuals from the data. The method involves minimizing the residual sum-of-squares, which is a quadratic function in the parameters. By differentiating and setting the derivative to zero, the unique solution for the parameters is found.\n",
      "\n",
      "This section discusses linear methods for regression, specifically least squares regression. It explains the geometry of least squares regression and how the predicted values are calculated. The hat matrix is introduced as a way to compute the orthogonal projection. It also mentions that in some cases, the columns of the input matrix may not be linearly independent, resulting in non-uniquely defined coefficients, but there are ways to resolve this issue.\n",
      "\n",
      "This section discusses linear regression models and least squares. It explains how rank deficiencies can occur and how to address them. The assumptions for the data distribution are also discussed, including uncorrelated and constant variance observations. The variance-covariance matrix of the least squares parameter estimates is derived. Additional assumptions are needed to draw inferences about the parameters and the model. The distribution properties of the parameter estimates and the variance estimate are also discussed, which are used for hypothesis tests and confidence intervals.\n",
      "\n",
      "This passage discusses the use of tail probabilities, Z-scores, and F-statistics in regression analysis. It explains that the standardized coeﬃcient or Z-score is used to test the hypothesis that a particular coeﬃcient is equal to zero. The passage also mentions that the F-statistic is used to test the significance of groups of coefficients simultaneously. It concludes by noting that the difference between the tail quantiles of a t-distribution and a standard normal becomes negligible as the sample size increases.\n",
      "\n",
      "This passage discusses linear regression models and least squares. It explains that the F statistic is used to measure the change in residual sum-of-squares for additional parameters in a model. It also discusses how to calculate confidence intervals for individual coefficients and the entire parameter vector. An example of applying these concepts to prostate cancer data is provided.\n",
      "\n",
      "The summary states that a linear model was fitted to the prostate cancer data, with the log of prostate-specific antigen as the response variable. The predictors were standardized and split into a training set and a test set. Least squares estimation was applied to the training set, resulting in estimates, standard errors, and Z-scores. The Z-scores measure the effect of dropping a variable from the model, with a Z-score greater than 2 indicating significance at the 5% level. The predictor \"lcavol\" showed the strongest effect, while \"lcp\" was not significant when \"lcavol\" was included in the model. The F-statistic was also used to test for the exclusion of non-significant terms.\n",
      "\n",
      "This passage discusses linear regression models and least squares. It mentions the calculation of F-statistics and p-values to determine the significance of the model. The passage also introduces the Gauss-Markov theorem, which states that the least squares estimates of the parameters have the smallest variance among all linear unbiased estimates. It concludes by mentioning the mean squared error of an estimator in estimating a parameter.\n",
      "\n",
      "This section discusses linear methods for regression, including the variance and squared bias. The Gauss-Markov theorem states that the least squares estimator has the smallest mean squared error among all linear estimators with no bias. However, there may be biased estimators with smaller mean squared error. Biased estimators are commonly used, and methods such as variable subset selection and ridge regression can result in biased estimates. The balance between bias and variance is important in selecting the right model. Mean squared error is related to prediction accuracy, and the expected prediction error and mean squared error differ only by a constant representing the variance of the new observation. The section also discusses multiple regression from simple univariate regression and the least squares estimates for the univariate linear model.\n",
      "\n",
      "The summary discusses linear regression models and least squares. It explains that univariate regression provides the foundation for multiple linear regression. When the inputs are orthogonal, they do not affect each other's parameter estimates in the model. However, orthogonal inputs are rare in observational data, so they need to be orthogonalized. The summary also outlines the steps for orthogonalizing the inputs and shows how this process can be generalized to multiple inputs.\n",
      "\n",
      "The given text describes the process of least squares regression by orthogonalization of the inputs. It explains the algorithm for regression by successive orthogonalization and shows how the multiple regression coefficient is calculated. The text also discusses the effect of correlated inputs in multiple regression. Overall, the text provides a concise summary of the method and its key results.\n",
      "\n",
      "This section discusses linear regression models and least squares. It explains that the coefficient represents the additional contribution of a variable after adjusting for other variables. If a variable is highly correlated with others, the coefficient will be unstable. The section also introduces the Gram-Schmidt procedure and the QR decomposition as numerical strategies for computing estimates. It concludes by stating that the least squares solution can be easily obtained using the QR decomposition.\n",
      "\n",
      "This section discusses linear methods for regression with multiple outputs. The linear model for each output is assumed, and the least squares estimates for the coefficients are the same as in the univariate case. If the errors are correlated, a multivariate weighted criterion can be used, but the solution is still given by the least squares estimates. However, if the errors vary among observations, the solution for the coefficients is no longer decoupled. The next section explores situations where it is beneficial to combine the regressions for multiple outcomes.\n",
      "\n",
      "Subset selection in linear regression is used to improve prediction accuracy and interpretation by choosing a smaller subset of variables with the strongest effects. This section describes different approaches to subset selection, starting with best-subset selection. Best-subset regression finds the subset of variables of each size that gives the smallest residual sum of squares. There are various criteria for choosing the subset size, typically selecting the smallest model that minimizes the expected prediction error. Other approaches discussed in this chapter are similar, producing a sequence of models of varying complexity indexed by a single parameter.\n",
      "\n",
      "The excerpt discusses linear methods for regression and the use of subset selection to determine the best predictors for the model. It explains that forward-stepwise selection is a more efficient approach than searching through all possible subsets, and it produces a nested sequence of models. The advantages of forward-stepwise selection are also mentioned.\n",
      "\n",
      "The summary discusses subset selection techniques in regression analysis. It mentions that for large values of p (the number of predictors), it may not be possible to compute the best subset sequence, but the forward stepwise sequence can always be computed. The forward stepwise method is a more constrained search and will have lower variance but possibly more bias. The backward-stepwise selection starts with the full model and sequentially deletes the predictor with the least impact on the fit. The summary also includes the results of a simulation study comparing best-subset regression with forward and backward selection, showing that their performance is similar. Additionally, it mentions the inclusion of forward stagewise regression, which takes longer to reach minimum error.\n",
      "\n",
      "The passage discusses different methods for selecting variables in linear regression models. It mentions that best-subset, forward, and backward selection gave the same sequence of terms in the example of prostate cancer. Some software packages use hybrid stepwise-selection strategies that consider both forward and backward moves and select the best option based on criteria such as AIC or F-statistics. However, using F-statistics is considered outdated because it does not account for multiple testing issues. It is also mentioned that standard errors in the chosen model are not valid due to the search process, and the bootstrap method can be useful in such cases. The passage also introduces forward-stagewise regression, which is more constrained than forward-stepwise regression. It starts with an intercept and adds variables one at a time based on their correlation with the current residual. This method may take more steps to reach the least squares fit, but it can be effective in high-dimensional problems.\n",
      "\n",
      "This section discusses various shrinkage methods, including best-subset selection, ridge regression, the lasso, principal components regression, and partial least squares. The methods are compared based on their complexity parameter and estimated prediction error. Cross-validation is used to select the shrinkage parameter, and the test set is used to evaluate the selected model. The estimated prediction error curves show that many of the methods have flat curves near their minimum. Best-subset selection chose two predictors. Shrinkage methods, such as ridge regression, shrink the regression coefficients by imposing a penalty on their size.\n",
      "\n",
      "The summary discusses the estimated prediction error curves and their standard errors for different selection and shrinkage methods in linear regression. The curves represent the model complexity parameter, and the least complex model within one standard error of the best is chosen.\n",
      "\n",
      "The table shows the estimated coefficients and test error results for different subset and shrinkage methods applied to the prostate data. The shrinkage method involves shrinking the coefficients towards zero and each other, controlled by a complexity parameter. The ridge problem is equivalent to minimizing the sum-of-squares of the residuals subject to a constraint on the size of the coefficients. This helps alleviate the issue of poorly determined coefficients and high variance in the presence of correlated variables. Standardizing the inputs is usually done before solving the ridge problem.\n",
      "\n",
      "The excerpt discusses linear methods for regression, specifically ridge regression. It explains that the intercept is excluded from the penalty term to prevent the procedure from depending on the origin of the target variable. The criterion for ridge regression is written in matrix form, and the ridge regression solutions are derived. The excerpt also mentions that ridge regression can be derived as the mean or mode of a posterior distribution. Additionally, the singular value decomposition of the input matrix is discussed as a useful tool in the analysis of ridge regression.\n",
      "\n",
      "The summary mentions that the figure shows the profiles of ridge coefficients for the example of prostate cancer as the tuning parameter λ is varied. The coefficients are plotted against the effective degrees of freedom (df(λ)). A vertical line is drawn at df = 5.0, which is the value chosen through cross-validation.\n",
      "\n",
      "The text discusses linear methods for regression, specifically the singular value decomposition (SVD) and ridge regression. The SVD is used to compute the least squares fitted vector and the ridge solutions. Ridge regression shrinks the coordinates of the orthonormal basis U by a factor determined by the singular values of X. A small value of d2j in the SVD indicates a smaller contribution to the principal components of X. The eigenvectors of X are also known as the principal components or Karhunen-Loeve directions. The first principal component has the largest sample variance among all normalized linear combinations of the columns of X.\n",
      "\n",
      "Shrinkage methods, such as ridge regression, are used to reduce the variability of coefficient estimates in linear regression. Ridge regression calculates principal components, which are directions that maximize the variance of the projected data. The method then shrinks the coefficients of low-variance components more than high-variance components. This helps protect against the potentially high variance of gradients estimated in certain directions and assumes that the response variable tends to vary most in the directions of high variance of the inputs.\n",
      "\n",
      "The summary discusses linear methods for regression, specifically ridge regression and the lasso. It mentions that the effective degrees of freedom in ridge regression is controlled by lambda and is a decreasing function. The lasso estimate is defined by minimizing the sum of squared errors subject to a constraint on the sum of absolute values of the coefficients. The summary also mentions that the lasso problem can be written in Lagrangian form.\n",
      "\n",
      "This section discusses shrinkage methods in linear regression, specifically the lasso method. The lasso is a quadratic programming problem that can be efficiently solved to find the optimal solution path. By choosing an appropriate value for the tuning parameter, some coefficients can be set exactly to zero, resulting in a kind of continuous subset selection. The shrinkage effect of the lasso is not obvious and further investigation is needed. The lasso coefficients decrease as the tuning parameter decreases, but the decrease is not always strictly monotonic. The lasso, ridge regression, and subset selection are compared in terms of their approaches to restricting the linear regression model. In the case of an orthonormal input matrix, all three methods have explicit solutions. Ridge regression does proportional shrinkage, the lasso applies soft thresholding, and subset selection uses hard thresholding.\n",
      "\n",
      "The summary states that the figure shows the profiles of lasso coefficients as the tuning parameter is varied. The coefficients are plotted against the shrinkage factor, and a vertical line is drawn at a specific value chosen through cross-validation. The lasso profiles hit zero, unlike the profiles for ridge. The profiles are piece-wise linear and are only computed at the displayed points.\n",
      "\n",
      "This section discusses shrinkage methods for estimating βj, where βj is a coefficient in a linear regression model. The table provides formulas for different estimators, including best subset, ridge, and lasso. The estimators are shown graphically in a figure, with blue areas representing constraint regions and red ellipses representing the least squares error function.\n",
      "\n",
      "This section discusses the region for ridge regression and lasso methods in linear regression. It explains that ridge regression uses a disk constraint region while lasso uses a diamond constraint region. The methods find the first point where the elliptical contours hit the constraint region. The diamond has corners, and if the solution occurs at a corner, one parameter is equal to zero. When p>2, the diamond becomes a rhomboid with more opportunities for parameters to be zero. The section also discusses the generalization of ridge regression and lasso as Bayes estimates, where different values of q represent different priors. The lasso, ridge regression, and best subset selection are derived as posterior modes, but it is more common to use the mean of the posterior as the Bayes estimate. The section suggests that values of q between 1 and 2 provide a compromise between lasso and ridge regression.\n",
      "\n",
      "The summary is about shrinkage methods in regression analysis. It discusses the elastic-net penalty, which is a compromise between ridge and lasso penalties. It also introduces least angle regression (LAR), which is a method for selecting variables in a regression model. LAR is closely related to the lasso and provides an efficient algorithm for computing the entire lasso path.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The LAR (Least Angle Regression) algorithm is a linear method for regression that starts with no variables in the model and gradually adds variables until reaching the full least-squares fit. The algorithm standardizes the predictors and finds the predictor most correlated with the current residual. It then moves the coefficients of this predictor towards their least-squares coefficient, while also considering other competitors. This process continues until all predictors have been entered. The LAR algorithm keeps the correlations tied and decreasing, and the coefficients change in a piecewise linear fashion. The LAR coefficient profile evolves as a function of their L1 arc length, which measures the sum of the L1 norms of the changes in coefficients from step to step.\n",
      "\n",
      "The summary describes the progression of absolute correlations in the LAR procedure using a simulated data set with six predictors. It also compares the LAR and Lasso coefficient profiles, showing that they are identical until a certain point where the Lasso coefficient crosses zero.\n",
      "\n",
      "The passage discusses linear methods for regression, specifically the LAR (Least Angle Regression) algorithm and its modification for the lasso problem. The LAR algorithm is efficient and requires the same computation as a single least squares fit. It always takes p steps to reach the full least squares estimates. The lasso path can have more steps, but the two are often similar. The passage also explains the similarity between the LAR algorithm and the lasso criterion, showing that they are equivalent when the input features are standardized.\n",
      "\n",
      "In this section, the authors discuss shrinkage methods such as the least angle regression (LAR) algorithm and the lasso method. They compare these methods to forward stepwise and stagewise regression and show that LAR and lasso perform better than forward stepwise regression. They also introduce the concept of effective degrees of freedom for adaptively fitted models.\n",
      "\n",
      "This passage discusses linear methods for regression, including forward stepwise, LAR, lasso, forward stagewise, and incremental forward stagewise regression. It compares the performance of these methods and discusses the concept of degrees of freedom in regression models. It also highlights the adaptability of LAR and lasso methods in estimating degrees of freedom.\n",
      "\n",
      "The text discusses methods that use derived input directions in regression analysis. These methods produce a small number of linear combinations of the original inputs, which are then used in the regression instead of the original inputs. One method discussed is Principal Components Regression, where the derived input columns are the principal components of the original inputs. This method involves regressing the target variable on these principal components. Principal components regression is similar to ridge regression, as both methods operate using the principal components of the input matrix. However, ridge regression shrinks the coefficients of the principal components, while principal components regression discards the smallest eigenvalue components.\n",
      "\n",
      "The summary discusses two linear methods for regression: ridge regression and partial least squares (PLS). Ridge regression shrinks the regression coefficients of principal components using shrinkage factors, while PLS constructs a set of linear combinations of the inputs for regression. PLS uses both the input variables and the outcome variable to construct these combinations. The process is described in detail in Algorithm 3.3.\n",
      "\n",
      "Partial least squares (PLS) is a method used to solve an optimization problem by constructing directions that have high variance and high correlation with the response variable. It is similar to principal components regression but also considers the correlation with the response. PLS iteratively calculates directions that maximize variance and correlation, and these directions can be used to estimate linear coefficients. PLS behaves similarly to ridge regression and principal components regression. If the input matrix is orthogonal, PLS finds the least squares estimates after one step.\n",
      "\n",
      "The summary discusses linear methods for regression, including ridge regression, partial least squares (PLS), principal components regression (PCR), and best subset regression. It compares the behavior and performance of these methods in different scenarios. The authors conclude that ridge regression is generally preferable for minimizing prediction error, although PLS and PCR show similar behavior. The lasso method falls between ridge regression and best subset regression in terms of properties and behavior.\n",
      "\n",
      "The discussion compares different selection and shrinkage methods for a simple problem with two inputs that have a correlation of ±0.5. The figure shows coefficient profiles for each method, including least squares, ridge, lasso, best subset, PLS, and PCR.\n",
      "\n",
      "This section discusses multiple outcome shrinkage and selection methods in linear regression. It explains that the least squares estimates for each output can be obtained individually. The section also explores the use of ridge regression and other techniques that exploit correlations between different responses. It introduces canonical correlation analysis (CCA) as a data reduction technique and explains how it finds linear combinations of predictors and responses that maximize correlations. The section concludes by mentioning reduced-rank regression as a formalized approach that pools information.\n",
      "\n",
      "The summary discusses multiple outcome shrinkage and selection in a restricted multivariate regression problem. It explains that reduced-rank regression performs a linear regression on the pooled response matrix and maps the coefficients back to the original response space. The summary also mentions the concept of shrinkage of canonical variates between X and Y, as proposed by Breiman and Friedman, and provides the formula for the shrinkage matrix.\n",
      "\n",
      "This passage discusses linear methods for regression, including the use of response shrinkage operators and hybrid shrinkage models. It also mentions the development of algorithms for fitting regularization paths, particularly in relation to L1 regularization. One algorithm discussed is the Incremental Forward Stagewise Regression, which involves updating predictors based on their correlation with the residuals.\n",
      "\n",
      "The text discusses the Lasso and related path algorithms in linear models. It explores the LAR procedure and the incremental version of forward-stagewise regression. It also mentions the inﬁnitesimal forward stagewise regression, which is similar to the lasso path and is important in non-linear, adaptive methods like boosting.\n",
      "\n",
      "The LAR algorithm was initially thought to implement a balanced update for tied predictors, but it was later discovered that the least-squares fit amongst tied predictors can result in coefficients moving in the opposite direction of their correlation. A modification to the LAR algorithm called FS0 implements a non-negative least squares fit that keeps the signs of the coefficients the same as those of the correlations. This modification achieves optimal balancing of update turns for tied variables with maximal correlation. If the LAR profiles are monotone non-increasing or non-decreasing, all three methods (LAR, lasso, and FS0) give identical profiles. If the profiles are not monotone but do not cross the zero axis, LAR and lasso are identical. FS0 is more constrained than lasso and can be viewed as a monotone version of the lasso. It may be useful in situations with many predictors, where its coefficient profiles are smoother and have less variance than those of lasso. FS0 optimizes per unit increase in L1 arc-length traveled along the coefficient path.\n",
      "\n",
      "The passage discusses the least angle regression procedure and its application to other regularized problems. It introduces the concept of piecewise-linear path algorithms and provides conditions for the solution path to be piecewise linear. It also mentions the Dantzig selector, a criterion proposed by Candes and Tao, which is a linear programming problem that produces a different path of solutions compared to the lasso.\n",
      "\n",
      "The discussed method, the Dantzig selector, is similar to the lasso in terms of its properties and ability to recover a sparse coefficient vector. However, the Dantzig selector has unsatisfactory operating properties and can yield erratic coefficient paths. On the other hand, the grouped lasso is a method that shrinks and selects predictors belonging to pre-defined groups together, encouraging sparsity at both the group and individual levels.\n",
      "\n",
      "This section discusses various advancements and generalizations of the Lasso algorithm, including the use of different norms and the inclusion of overlapping groups of predictors. It also explores the ability of the Lasso and related procedures to recover the correct model, with many studies focusing on the identification of predictors with high probability. The results often rely on a condition regarding the model matrix, which ensures that the good variables are not highly correlated with the nuisance variables. While the Lasso estimates of non-zero coefficients are biased towards zero and generally inconsistent, methods such as the relaxed Lasso can help reduce this bias by selecting and fitting an unrestricted linear model to the set of non-zero predictors.\n",
      "\n",
      "This section discusses linear methods for regression and introduces the concept of shrinkage in coefficient estimates. It explains the use of the smoothly clipped absolute deviation (SCAD) penalty as an alternative to the lasso penalty, which allows for less severe shrinkage of larger coefficients. The section also mentions the adaptive lasso, which uses a weighted penalty based on the ordinary least squares estimate. Additionally, it briefly mentions the pathwise coordinate optimization approach as an alternative to the LARS algorithm for computing the lasso solution.\n",
      "\n",
      "The passage discusses the computational considerations for solving the lasso problem. It presents a univariate lasso problem with a partial residual and provides an explicit solution for updating the lasso estimate. The passage also describes an algorithm for efficiently computing the lasso solutions at a grid of values of lambda. It compares the algorithm's speed to the LARS algorithm and mentions its applicability to other models. Additionally, the passage mentions the computational considerations for least squares fitting, including the Cholesky and QR decompositions.\n",
      "\n",
      "This passage discusses various linear methods for regression, including linear regression, ridge regression, and the lasso. It also mentions the least angle regression procedure and the partial least squares method. The passage provides references to further reading on these topics. The exercises in this passage involve proving the equivalence of the F-statistic and the square of the z-score, comparing two approaches for constructing a confidence band in a cubic polynomial regression model, and proving the Gauss-Markov theorem.\n",
      "\n",
      "This section of exercises covers various topics related to regression analysis, including least squares coefficients, ridge regression, lasso, posterior distribution, QR decomposition, stepwise regression, and multivariate linear regression. The exercises provide mathematical proofs and explanations for these concepts.\n",
      "\n",
      "This section discusses various topics related to linear methods for regression, including ridge regression, principal component regression (PCR), partial least squares (PLS), and canonical correlation. It provides exercises and derivations for each topic, exploring concepts such as model constraints, shrinkage of coefficients, orthogonal cases, estimators, and the relationship between algorithms and methods.\n",
      "\n",
      "The exercises in this section involve solving equations and showing relationships between variables in regression problems. The exercises demonstrate that certain quantities and correlations remain equal or decrease monotonically as calculations are performed. The LAR algorithm and forward stepwise regression are also discussed and compared in terms of variable selection criteria.\n",
      "\n",
      "This text discusses linear methods for regression, specifically the Lasso and LAR problems. It explains the Lagrange multiplier form of the Lasso problem and derives the Lagrange dual function and Karush-Kuhn-Tucker optimality conditions. The text also shows that the KKT conditions imply three possible scenarios. It further discusses the relationship between the gradient of the objective function and the correlation between predictors and residuals. Lastly, it states that the lasso solution path is linear as the regularization parameter λ ranges from λ0 to λ1.\n",
      "\n",
      "Exercises 99 discuss the effects of exact collinearity on the fitted lasso coefficients and ridge regression coefficients. It is shown that when an identical copy of a variable is included in the regression, the coefficients remain the same. Additionally, the exercise explores how the elastic-net optimization problem can be transformed into a lasso problem using augmented versions of the variables.\n",
      "\n",
      "The topic is about linear methods for regression, specifically focusing on 100 observations.\n",
      "\n",
      "This chapter discusses linear methods for classification, where the decision boundaries between classes are linear. The chapter explores different ways to find these linear decision boundaries, including fitting linear regression models to class indicator variables. The input space is divided into regions of constant classification, with piecewise hyperplanar decision boundaries.\n",
      "\n",
      "This section discusses linear methods for classification that model the posterior probabilities. The decision boundaries in these methods are linear if either the δk(x) or Pr(G=k|X=x) are linear in x. The logit transformation is a popular model for the posterior probabilities, and the decision boundary is a hyperplane where the log-odds are zero. Two popular methods, linear discriminant analysis and linear logistic regression, result in linear log-odds. Another approach is to explicitly model the boundaries as linear, using separating hyperplanes. The chapter also mentions the possibility of generalization by expanding the variable set to include squares and cross-products, mapping linear decision boundaries to quadratic decision boundaries.\n",
      "\n",
      "This section discusses linear regression of an indicator matrix. The indicator matrix is used to code the response categories, and a linear regression model is fitted to each column of the indicator matrix simultaneously. The fit is given by a coefficient matrix, and a new observation is classified based on the largest component of the fitted output.\n",
      "\n",
      "The summary discusses the use of linear methods for classification. It explores the rationale behind this approach, including viewing regression as an estimate of conditional expectation. The summary also mentions the issue of how well the linear regression model approximates the conditional expectation and whether the estimates of the posterior probabilities are reasonable. It discusses the possibility of violations in the model, but notes that this approach can still work effectively in many cases. The summary also mentions the use of basis expansions to improve the estimates. Additionally, it mentions a simplistic viewpoint of constructing targets for each class and fitting a linear model using least squares. The criterion for fitting the model is the sum-of-squared Euclidean distances, and a new observation is classified by computing its fitted vector and classifying it to the closest target. The summary concludes by noting that this approach is similar to the previous approach of estimating conditional expectation.\n",
      "\n",
      "The given text appears to be a combination of numbers and phrases that do not form a coherent summary or meaningful information.\n",
      "\n",
      "The summary states that there is a problem with the regression approach when there are three or more classes, as some classes can be masked by others. The example given shows how linear regression fails to identify the middle class in a dataset with three classes. The solution proposed is to use polynomial regression with terms up to the degree of K-1, where K is the number of classes.\n",
      "\n",
      "The text discusses linear methods for classification and their limitations. It presents an example where linear regression and linear discriminant analysis have high error rates due to masking. It also mentions the use of linear functions in classification methods and their avoidance of the masking problem. The text briefly introduces linear discriminant analysis and the need to know class posteriors for optimal classification.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The summary of the given information is that linear discriminant analysis (LDA) is a technique used for classification, particularly in cases with multiple classes and high-dimensional data. The data in question is vowel training data with eleven classes in ten dimensions. The LDA model is represented in a two-dimensional plot, showing significant class overlap. The summary also includes a table comparing the performance of various linear techniques on the vowel data, with LDA outperforming linear regression but being surpassed by quadratic discriminant analysis in terms of error rates.\n",
      "\n",
      "The passage discusses linear methods for classification, specifically linear discriminant analysis (LDA). LDA assumes that classes have a common covariance matrix and uses a linear log-odds function to determine the decision boundary between classes. This linear function implies that the decision boundaries are linear in p dimensions, forming hyperplanes. The passage also mentions other techniques such as quadratic discriminant analysis, mixtures of Gaussians, and nonparametric density estimates.\n",
      "\n",
      "Linear Discriminant Analysis (LDA) is a method used for classification. It involves creating decision boundaries based on the parameters of Gaussian distributions. The decision boundaries are not necessarily perpendicular bisectors of the centroids. The parameters of the Gaussian distributions need to be estimated using the training data. LDA can be related to linear regression for binary classification. The LDA rule for classifying to class 2 is based on a specific inequality.\n",
      "\n",
      "This passage discusses linear methods for classification, focusing on the use of least squares and linear discriminant analysis (LDA). It explains how the coeﬃcient vector from least squares is proportional to the LDA direction, and discusses the applicability of LDA beyond Gaussian data. It also mentions the use of quadratic discriminant functions (QDA) when the covariance matrices are not assumed to be equal. The passage includes a comparison between QDA and LDA in fitting quadratic boundaries, and highlights the need to estimate separate covariance matrices for each class in QDA.\n",
      "\n",
      "Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are two simple and widely used techniques for classification tasks. LDA assumes that the data follows a Gaussian distribution and that the covariances are approximately equal, while QDA relaxes these assumptions. Both LDA and QDA can estimate simple decision boundaries such as linear or quadratic, and the estimates provided are stable. The success of LDA and QDA may be attributed to the trade-off between bias and variance, where simple decision boundaries can be estimated with lower variance compared to more complex alternatives.\n",
      "\n",
      "Regularized Discriminant Analysis (RDA) is a compromise between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) that allows for shrinking the separate covariances of QDA towards a common covariance as in LDA. This method is similar to ridge regression. The regularized covariance matrices have a specific form, and the parameter α can be chosen based on the model's performance on validation data or through cross-validation. RDA improves both training and test error, but there is a sharp increase in test error after α=0.9. Similar modifications can be made to shrink the covariance matrix itself. Other regularized versions of LDA are also discussed in Chapter 12 for data arising from digitized analog signals and images.\n",
      "\n",
      "Linear Discriminant Analysis (LDA) is a method used for classification in situations where the features are high-dimensional and correlated. The LDA coefficients can be regularized to be smooth or sparse, leading to better generalization and easier interpretation. Computationally, LDA and Quadratic Discriminant Analysis (QDA) can be simplified by diagonalizing the covariance matrices. LDA can also be implemented by sphering the data with respect to the common covariance estimate and classifying to the closest class centroid in the transformed space. LDA also allows for dimension reduction, as the centroids in the high-dimensional input space can be projected onto a lower-dimensional subspace.\n",
      "\n",
      "This section discusses the use of linear methods for classification, specifically focusing on Linear Discriminant Analysis (LDA). LDA aims to find an optimal subspace for class separation by maximizing the spread of projected centroids. The steps for finding optimal subspaces are outlined, including computing class centroids and covariance matrices. The discriminant variables are obtained through a decomposition process. Fisher's criterion for LDA is also explained, which involves finding a linear combination that maximizes between-class variance relative to within-class variance. The importance of considering covariance in achieving minimal overlap between projected classes is highlighted.\n",
      "\n",
      "This section discusses linear discriminant analysis and presents four projections onto pairs of canonical variates. The figure shows that as the rank of the canonical variates increases, the centroids become less spread out, with the classes appearing to be superimposed in the lower right panel. This indicates confusion between the classes.\n",
      "\n",
      "This section discusses linear methods for classification. It introduces Fisher's problem, which aims to maximize the Rayleigh quotient in order to find the optimal discriminant coordinates. These coordinates help minimize overlap in the projected data and are used to determine linear decision boundaries. Gaussian classification with common covariances can be achieved by sphering the data and classifying based on the closest centroid. The data can be confined to the subspace spanned by the centroids, which can be further decomposed into optimal subspaces for centroid separation. This decomposition is identical to the decomposition due to Fisher.\n",
      "\n",
      "Linear Discriminant Analysis (LDA) is a dimension reduction technique that can be used for classification. It involves limiting the distance-to-centroid calculations to a chosen subspace, resulting in a Gaussian classification rule. The misclassification rate is based on the overlap between the densities of different classes, and adjusting the cut-point can improve the error rate. LDA can be beneficial in reducing the dimensionality of data for classification tasks, as shown in the example of the vowel data. There is a connection between LDA and regression of an indicator response matrix.\n",
      "\n",
      "The summary is about linear methods for classification in a reduced subspace. It mentions the decision boundaries for vowel training data in a two-dimensional subspace and explains that in higher-dimensional subspaces, the decision boundaries are higher-dimensional affine planes rather than lines.\n",
      "\n",
      "This section discusses logistic regression and its relationship to linear discriminant analysis (LDA). LDA involves regression followed by an eigen-decomposition of the Y matrix. Logistic regression, on the other hand, models the posterior probabilities of the classes using linear functions in x. The model ensures that the probabilities sum to one and remain between 0 and 1. The logistic regression model is specified in terms of log-odds transformations and the choice of denominator in the odds-ratios is arbitrary. The probabilities are dependent on the parameter set θ. Logistic regression is commonly used in biostatistical applications for binary responses.\n",
      "\n",
      "The concise summary is about fitting logistic regression models using maximum likelihood and the conditional likelihood of G given X. The log-likelihood is discussed for the two-class case, and the score equations are derived to maximize the log-likelihood. The Newton-Raphson algorithm is used to solve the score equations, requiring the second-derivative or Hessian matrix. A single Newton update is performed to update the parameter estimates.\n",
      "\n",
      "Logistic regression is a statistical model used for data analysis and inference. It involves solving a weighted least squares problem iteratively to estimate the parameters. The algorithm can be expressed as an iteratively reweighted least squares algorithm. It is commonly used to understand the relationship between input variables and the outcome variable.\n",
      "\n",
      "This summary discusses the results of a logistic regression analysis conducted on South African heart disease data. The analysis examines the relationship between various risk factors and the presence or absence of myocardial infarction. The results show that some variables, such as systolic blood pressure and obesity, are not significant predictors of heart disease when considered in isolation but become significant when considered alongside other variables. The summary highlights the importance of interpreting coefficients with caution due to the correlation between predictors.\n",
      "\n",
      "The given text appears to be a combination of numbers and letters arranged in a specific pattern. It is difficult to determine the exact meaning or context of the text without further information.\n",
      "\n",
      "The provided text does not form a coherent or meaningful summary. It appears to be a random arrangement of letters and spaces.\n",
      "\n",
      "The given text does not provide any meaningful information or context to summarize.\n",
      "\n",
      "The given text contains a series of characters, including letters and spaces, arranged in a pattern. The pattern repeats multiple times throughout the text.\n",
      "\n",
      "The given text is a series of \"o\" characters arranged in a pattern. The pattern repeats several times with varying lengths of \"o\" sequences.\n",
      "\n",
      "The given text consists of a series of letters and numbers arranged in a pattern.\n",
      "\n",
      "The summary is not clear as it includes a scatterplot matrix and information about color coding in the plot. Please provide more specific information for a concise summary.\n",
      "\n",
      "This summary discusses the results of stepwise logistic regression analysis conducted on South African heart disease data. The table shows the coefficients, standard errors, and z-scores of the variables included in the model. The analyst then performed model selection by dropping the least significant coefficient until no further terms could be dropped. The strategy of re-fitting each model with one variable removed and conducting an analysis of deviance was also used. The interpretation of a coefficient for the variable tobacco is provided, indicating that an increase of 1kg in lifetime tobacco usage leads to an 8.4% increase in the odds of coronary heart disease. The concept of quadratic approximations and inference in maximum likelihood parameter estimates is briefly mentioned.\n",
      "\n",
      "Logistic regression is a statistical model used for binary classification. It is connected to least squares and can be used to approximate the deviance. Asymptotic likelihood theory shows that the estimated coefficients of logistic regression are consistent and their distribution can be derived using normal theory inference. Model building for logistic regression can be costly, but shortcuts such as the Rao score test and Wald test can be used. Software implementations, such as in R, can take advantage of these connections. Additionally, L1 regularized logistic regression can be used for variable selection and shrinkage. The penalty term in L1 regularization is used to maximize a penalized version of the logistic regression model.\n",
      "\n",
      "The text discusses linear methods for classification, specifically focusing on the L1 regularization path for logistic regression. It explains that the problem can be solved using nonlinear programming methods or a weighted lasso algorithm. The text also mentions the use of quadratic approximations and coordinate descent methods for computing the coefficient profiles. A figure is provided to illustrate the L1 regularization path for the South African heart disease data. The discussion references the R package glmnet for implementing these methods.\n",
      "\n",
      "This section discusses logistic regression and its comparison to linear discriminant analysis (LDA) for classification problems. Logistic regression can efficiently fit coeﬃcient paths for large logistic regression problems and can exploit sparsity in the predictor matrix. LDA assumes Gaussian class densities and a common covariance matrix, while logistic regression makes fewer assumptions. Logistic regression estimates the coeﬃcients by maximizing the conditional likelihood, while LDA maximizes the full log-likelihood based on the joint density.\n",
      "\n",
      "Linear methods for classification can use maximum-likelihood estimates to estimate parameters, with the marginal density playing a role. Ignoring the marginal part of the likelihood can result in a loss of efficiency, but relying on strong model assumptions can allow for the use of unclassified observations. The marginal likelihood acts as a regularizer and can prevent parameter degeneracies. Logistic regression is often considered safer and more robust than LDA due to its reliance on fewer assumptions.\n",
      "\n",
      "This section discusses separating hyperplanes, which are linear decision boundaries that aim to separate data into different classes as accurately as possible. The example given shows two classes of data points in IR2 that can be separated by a linear boundary. The least squares solution, obtained through regression, does not perfectly separate the points. This approach is similar to linear discriminant analysis and logistic regression.\n",
      "\n",
      "This passage discusses linear methods for classification, specifically focusing on the concept of a hyperplane and its properties. It also introduces Rosenblatt's Perceptron Learning Algorithm, which aims to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.\n",
      "\n",
      "The text discusses the concept of separating hyperplanes and a criterion for minimizing misclassifications. It explains the use of stochastic gradient descent in finding a separating hyperplane and highlights some problems with the algorithm. These problems include the dependence on starting values, potentially large number of steps, and difficulty in detecting cycles when the data is not separable.\n",
      "\n",
      "Linear methods for classification involve transforming the original variables using basis functions. Perfect separation between classes may not always be achievable or desirable. One solution is to add additional constraints to the separating hyperplane. The optimal separating hyperplane maximizes the distance to the closest point from either class, leading to better classification performance on test data. This is achieved through a convex optimization problem.\n",
      "\n",
      "The given text discusses the concept of separating hyperplanes and their application in classification problems. It introduces the Lagrange function and the Wolfe dual, which are used to find the optimal solution for separating hyperplanes. The solution involves finding support points that define the boundary of the slab and using them to determine the coefficients of the hyperplane equation. The hyperplane is then used to classify new observations.\n",
      "\n",
      "This section discusses linear methods for classification, focusing on the concept of maximum margin. The optimal separating hyperplane is introduced, which aims to create a large margin between classes for better separation on test data. The use of support points is explained, highlighting their importance in determining the optimal hyperplane. Logistic regression is also mentioned as a solution to the classification problem, with its similarity to the separating hyperplane solution. However, when the data is not separable, an alternative formulation is required.\n",
      "\n",
      "This passage discusses the concept of classifying data using linear discriminant analysis (LDA) and the least squares criterion. It provides equations and explanations for solving the generalized eigenvalue problem and deriving the LDA rule. It also shows that the solution to the least squares criterion is proportional to the LDA coefficient.\n",
      "\n",
      "This section discusses various linear methods for classification. It shows that the results hold for any coding of the two classes and provides the solution for classification. It also compares the classification rule to the LDA rule and explains its differences. Additionally, it discusses the LDA using transformed predictors and the multilogit model. It also addresses logistic regression problems with multiple classes and provides proofs for the convergence of the perceptron learning algorithm. Finally, it introduces a criterion called D*(β,β0).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Exercises 137 discusses a generalization of a criterion where we sum over all observations. It also considers the optimal separating hyperplane problem and describes the criterion in words. Exercise 4.8 explores the constrained maximum likelihood estimators for the parameters in a multivariate Gaussian model. It shows that the Bayes classification rule is equivalent to classifying in the reduced subspace computed by LDA. Exercise 4.9 suggests writing a computer program for quadratic discriminant analysis using separate Gaussian models per class and applying it to vowel data to compute the misclassification error.\n",
      "\n",
      "The topic discussed in this section is linear methods for classification.\n",
      "\n",
      "In this section, the authors discuss the limitations of linear models in representing non-linear relationships in data. They explain that linear models are often used as approximations because they are easy to interpret and can be the only option when dealing with limited data. However, they introduce the concept of augmenting or replacing input variables with additional transformed variables to move beyond linearity. They propose using linear models in this new space of derived input features to better represent the true function.\n",
      "\n",
      "This section discusses basis expansions and regularization in linear models. Basis expansions involve transforming the original input variables into new variables using basis functions. Examples of basis functions include polynomial terms, logarithmic transformations, and indicator functions. These basis functions allow for more flexible representations of the model. However, the number of variables can grow exponentially with the degree of the polynomial. Regularization methods are used to control the complexity of the model by limiting the class of functions. Additivity is one example of a restriction method, where the model is assumed to have a specific form.\n",
      "\n",
      "This section discusses the use of piecewise polynomials and splines in modeling. The size of the model is limited by the number of basis functions used for each component function. There are two main approaches for selecting basis functions: selection methods that adaptively include only significant basis functions, and regularization methods that restrict the coefficients. Piecewise polynomials are obtained by dividing the domain of X into intervals and representing the function with separate polynomials in each interval. Different types of piecewise polynomials, such as piecewise constant and piecewise linear, are shown in Figure 5.1. The use of continuity restrictions in piecewise linear functions can lead to linear constraints on the parameters. A basis that incorporates these constraints can be used to achieve smoother functions. Piecewise-cubic polynomials are also discussed as a way to achieve smoother fits.\n",
      "\n",
      "This section discusses basis expansions and regularization in the context of fitting piecewise constant and piecewise linear functions to data. It includes a visual representation of the fitting process and the use of basis functions.\n",
      "\n",
      "The passage discusses piecewise polynomials and splines, specifically cubic splines. It explains that cubic splines are continuous and have continuous first and second derivatives at the knots. The passage also introduces a basis representation for cubic splines with knots at ξ1 and ξ2. It concludes by mentioning the parameter count for cubic splines.\n",
      "\n",
      "This section discusses basis expansions and regularization in the context of splines. It explains that a spline is a piecewise-polynomial function with continuous derivatives, and cubic splines are often used because they are the lowest-order spline that is not visibly discontinuous. The section also mentions the use of fixed-knot splines, the selection of spline order and knot placement, and the use of different basis functions for representing splines. Additionally, it introduces natural cubic splines and discusses the potential issues with spline behavior near the boundaries.\n",
      "\n",
      "The text discusses pointwise variances for different models, including global linear, global cubic polynomial, cubic spline, and natural cubic spline. The natural cubic spline adds constraints that the function is linear beyond the boundary knots, allowing for more flexibility in the interior region. The tradeoff between variance and bias is illustrated in Figure 5.3. A natural cubic spline with K knots is represented by K basis functions.\n",
      "\n",
      "The summary discusses the use of basis expansions and regularization in fitting logistic regression models to the South African heart disease data. Natural splines are used to model nonlinearities in the functions. The model includes multiple terms, each associated with a vector of coefficients multiplying their respective basis functions. A backward stepwise deletion process is used to select the final model, and the AIC statistic is used to determine which terms to drop. The final model is displayed in a plot, showing the estimated functions for each variable. The pointwise variance function is also calculated. The AIC statistic is compared to the likelihood-ratio test.\n",
      "\n",
      "The summary states that the figure shows fitted natural-spline functions for different terms in the final model selected by the stepwise procedure. The figure also includes pointwise standard-error bands and a rug plot indicating the location of sample values for each variable.\n",
      "\n",
      "The summary discusses the use of basis expansions and regularization in logistic regression models. It presents a table showing the final model after stepwise deletion of natural splines terms, including the likelihood-ratio test statistic and p-value for each term. The summary also mentions the use of splines in reducing flexibility in phoneme recognition, where the goal is to classify spoken phonemes based on log-periodograms measured at different frequencies. It describes the linear logistic regression model fitted to a training sample and the coefficients plotted as a function of frequency.\n",
      "\n",
      "The summary states that the graph shows the log-periodogram for phoneme examples \"aa\" and \"ao\", with the coefficients of a logistic regression fit shown below. The coefficients are restricted to a smooth curve in red and unrestricted in a jagged gray curve.\n",
      "\n",
      "This section discusses basis expansions and regularization in the context of approximating a contrast functional. The coeﬃcients in the approximation capture the differences in log-periodograms between two classes of data. The use of natural cubic splines allows for a smooth variation of the coeﬃcients as a function of frequency. The smoothing not only aids in interpretation but also improves the accuracy of classification. Filtering and feature extraction using basis matrices can be a powerful method for enhancing the performance of learning algorithms. The preprocessing does not have to be linear and can be a general transformation of the features.\n",
      "\n",
      "This section discusses the use of smoothing splines as a method for fitting functions to data. It introduces the problem of selecting knots and proposes a method that avoids this issue by using a maximal set of knots. The complexity of the fit is controlled by regularization, which balances closeness to the data and penalizes curvature in the function. The criterion for finding the optimal function is defined on an infinite-dimensional function space, but it can be shown that there is a finite-dimensional unique minimizer, which is a natural cubic spline with knots at the unique values of the input data. The solution can be written as a sum of basis functions multiplied by coefficients.\n",
      "\n",
      "The text describes the use of basis expansions and regularization in fitting a smoothing spline to data on bone mineral density in adolescents. The smoothing spline is used to model the relationship between age and the relative change in spinal BMD. The fitted spline is obtained using ridge regression and is shown in Figure 5.6. The data are color-coded by gender, and separate curves are fit for males and females. Efficient computational techniques for fitting smoothing splines are also discussed.\n",
      "\n",
      "This section discusses the use of smoothing splines and the choice of the smoothing parameter. It explains that a smoothing spline with a prechosen smoothing parameter is a linear smoother, and the fit is linear in the data. It also compares the smoothing matrix to the hat matrix used in traditional least squares fitting, highlighting similarities and differences. The concept of effective degrees of freedom is introduced as a measure of the complexity of the fit.\n",
      "\n",
      "The summary discusses the concept of degrees of freedom in smoothing splines and the use of regularization. It explains how the penalty matrix is used in the smoothing spline equation and the eigen-decomposition of the penalty matrix. The summary also mentions the effect of different penalties on the smoothness of the fit and the decomposition of the data using eigenvectors.\n",
      "\n",
      "The summary describes a smoothing spline model that is used to fit ozone concentration data against daggot pressure gradient. The model has two fits with different values of the smoothing parameter, resulting in five and eleven effective degrees of freedom. The lower left graph shows the first 25 eigenvalues of the smoothing spline matrices, which are all greater than or equal to zero. The lower right graph displays the third to sixth eigenvectors of the spline smoother matrices, with the damped functions representing the smoothed versions of these functions using the 5-degree of freedom smoother.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"\"The Elements of Statistical Learning\" is a comprehensive book on statistics, data mining, and machine learning. Written by renowned professors, it covers various topics and includes examples and color graphics. The second edition has additional chapters and updates. It is a valuable resource for statisticians and those interested in data mining. The book covers topics such as supervised and unsupervised learning, regression, classification, and high-dimensional problems. It also discusses methods like random forests, ensemble learning, and undirected graphical models. The book's preface acknowledges errors in the first edition and thanks contributors. It is dedicated to the memory of Anna McPhee. The book is suitable for researchers and students in various fields, with exercises provided for practice.\n",
      "\n",
      "The given text discusses various topics related to supervised learning, including k-nearest neighbor fits, least squares fits, statistical decision theory, function approximation, and model selection. It covers concepts such as bias, variance, mean squared error, and the curse of dimensionality. The text also introduces different methods for estimating regression functions, such as linear regression and local regression. It provides examples and comparisons of different methods and discusses the tradeoff between bias and variance. The text emphasizes the importance of choosing the appropriate model complexity and includes exercises and bibliographic notes for further study.\n",
      "\n",
      "This summary discusses various aspects of linear regression models and least squares fitting. It covers topics such as the goal of finding the linear function that minimizes the sum of squared residuals, methods for calculating predicted values and orthogonal projections, dealing with rank deficiencies and correlated inputs, and testing the significance of coefficients using Z-scores and F-statistics. It also explores subset selection techniques, shrinkage methods like ridge regression and the lasso, as well as the use of the singular value decomposition (SVD) and principal components analysis (PCA) in regression analysis. The summary concludes by mentioning the elastic-net penalty and least angle regression (LAR) as additional methods for variable selection.\n",
      "\n",
      "The passage discusses various linear methods for regression, such as the LAR algorithm, ridge regression, partial least squares (PLS), and the lasso method. It compares the performance and behavior of these methods and explores their applications in different scenarios. The passage also discusses computational considerations, advancements, and generalizations of the lasso algorithm. Additionally, it explores linear methods for classification, including linear discriminant analysis and logistic regression. The limitations of linear methods for classification are also mentioned.\n",
      "\n",
      "The given information discusses linear methods for classification, with a focus on linear discriminant analysis (LDA) and logistic regression. LDA is a technique used for classification in cases with multiple classes and high-dimensional data. It assumes a common covariance matrix for the classes and uses a linear log-odds function to determine decision boundaries. Logistic regression models the posterior probabilities of classes using linear functions and is commonly used for binary responses. The information also mentions other techniques such as quadratic discriminant analysis and regularized discriminant analysis, as well as the importance of considering covariance in achieving minimal overlap between classes.\n",
      "\n",
      "This section discusses the use of basis expansions and regularization in linear methods for classification. It explains the limitations of linear models in representing non-linear relationships in data and introduces the concept of augmenting or replacing input variables with additional transformed variables to move beyond linearity. The section also discusses basis expansions, regularization methods, and the use of piecewise polynomials and splines in modeling. It includes visual representations of the fitting process and the use of basis functions. Lastly, the section discusses the use of basis expansions and regularization in logistic regression models and the fitting of smoothing splines to data.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"The Elements of Statistical Learning\" is a comprehensive book covering topics in statistics, data mining, and machine learning. It is a valuable resource for statisticians and those interested in data mining. The book discusses various topics such as supervised and unsupervised learning, regression, classification, and high-dimensional problems. It also covers methods like random forests, ensemble learning, and undirected graphical models. The second edition includes additional chapters and updates. The book is suitable for researchers and students in various fields, with exercises provided for practice. It emphasizes the importance of model selection and choosing appropriate complexity. It also discusses linear regression models, least squares fitting, and methods for estimating regression functions. The book explores linear methods for regression and classification, including the LAR algorithm, ridge regression, logistic regression, and linear discriminant analysis. It discusses basis expansions, regularization, and the use of splines in modeling.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#chain.run(daily_weather[:200])\n",
    "chain.run(sl_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f86301ae-c877-4e2c-8a97-c4eddd82127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]='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' #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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7f6a408-4d59-43e2-a3e0-d883a34f777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-Qoxcd***************************************8IPT. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(\n\u001b[0;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      3\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(sl_data[:\u001b[38;5;241m20\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:538\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    539\u001b[0m         _output_key\n\u001b[0;32m    540\u001b[0m     ]\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    544\u001b[0m         _output_key\n\u001b[0;32m    545\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    137\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\combine_documents\\refine.py:152\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_initial_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 152\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_llm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    153\u001b[0m refine_steps \u001b[38;5;241m=\u001b[39m [res]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    116\u001b[0m         prompts,\n\u001b[0;32m    117\u001b[0m         stop,\n\u001b[0;32m    118\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    412\u001b[0m ]\n\u001b[0;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    399\u001b[0m                 m,\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    403\u001b[0m             )\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    578\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:439\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    434\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    438\u001b[0m }\n\u001b[1;32m--> 439\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    440\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    441\u001b[0m )\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:364\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:362\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    712\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-Qoxcd***************************************8IPT. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9c55089-44b8-4461-abe7-ea94d90a7ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.initial_llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57485106-414c-4bdc-969b-69991a00f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.refine_llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f35486-3058-459d-bd9f-23af13d3a1e5",
   "metadata": {},
   "source": [
    "<strong>But the LLM sometimes fails to execute correctly that refine step, breaking the whole chain. We can create custom prompts to modify the behavior of that chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34f3a9f6-0af6-4f3f-b981-7093151f277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Extract the most relevant themes from the following:\n",
      "\n",
      "Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\n",
      "\n",
      "THEMES:\u001b[0m\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-Qoxcd***************************************8IPT. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 32\u001b[0m\n\u001b[0;32m     22\u001b[0m refine_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(refine_template)\n\u001b[0;32m     24\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(\n\u001b[0;32m     25\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     26\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m chain\u001b[38;5;241m.\u001b[39mrun(sl_data[:\u001b[38;5;241m20\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:538\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    539\u001b[0m         _output_key\n\u001b[0;32m    540\u001b[0m     ]\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    544\u001b[0m         _output_key\n\u001b[0;32m    545\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    137\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\combine_documents\\refine.py:152\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_initial_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 152\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_llm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    153\u001b[0m refine_steps \u001b[38;5;241m=\u001b[39m [res]\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    116\u001b[0m         prompts,\n\u001b[0;32m    117\u001b[0m         stop,\n\u001b[0;32m    118\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    412\u001b[0m ]\n\u001b[0;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    399\u001b[0m                 m,\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    403\u001b[0m             )\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    578\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:439\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    434\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    438\u001b[0m }\n\u001b[1;32m--> 439\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    440\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    441\u001b[0m )\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:364\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:362\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    712\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML\\Lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-Qoxcd***************************************8IPT. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "initial_template = \"\"\"\n",
    "Extract the most relevant themes from the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "THEMES:\"\"\"\n",
    "\n",
    "refine_template = \"\"\"\n",
    "Your job is to extract the most relevant themes\n",
    "We have provided an existing list of themes up to a certain point: {existing_answer}\n",
    "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "Given the new context, refine the original list\n",
    "If the context isn't useful, return the original list and ONLY the original list.\n",
    "Return that list as a comma separated list.\n",
    "\n",
    "LIST:\"\"\"\n",
    "\n",
    "initial_prompt = PromptTemplate.from_template(initial_template)\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750024c-63e7-49cd-ac4a-b0130d45a489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
